{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "981fc0e4",
   "metadata": {},
   "source": [
    "## 下載官方Distilled DeiT-T並處理數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf6bedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengwei/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-15 22:03:47.977368: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-15 22:03:49.236928: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/chengwei/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/transformers/models/deit/feature_extraction_deit.py:28: FutureWarning: The class DeiTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DeiTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, DeiTForImageClassificationWithTeacher\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import os\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained('facebook/deit-tiny-distilled-patch16-224')\n",
    "model = DeiTForImageClassificationWithTeacher.from_pretrained('facebook/deit-tiny-distilled-patch16-224')\n",
    "# img_folder_path = 'to/your/image/path'\n",
    "\n",
    "# label_mapping = {}\n",
    "# with open(os.path.join(img_folder_path, 'file_label_mapping.txt'), 'r') as f:\n",
    "#     for line in f:\n",
    "#         filename, label = line.strip().split(\",\")\n",
    "#         label_mapping[filename] = int(label)\n",
    "\n",
    "# image_tensors = []\n",
    "# true_labels = []\n",
    "# for filename in os.listdir(img_folder_path):\n",
    "#     if filename.endswith('.JPEG'):\n",
    "#         image_path = os.path.join(img_folder_path, filename)\n",
    "#         image = Image.open(image_path).convert('RGB')\n",
    "#         inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "#         image_tensors.append(inputs['pixel_values'])\n",
    "#         true_labels.append(label_mapping[filename])\n",
    "\n",
    "# images_tensor = torch.cat(image_tensors, dim=0)\n",
    "# true_labels = torch.tensor(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f11085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeiTForImageClassificationWithTeacher(\n",
      "  (deit): DeiTModel(\n",
      "    (embeddings): DeiTEmbeddings(\n",
      "      (patch_embeddings): DeiTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): DeiTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DeiTLayer(\n",
      "          (attention): DeiTSdpaAttention(\n",
      "            (attention): DeiTSdpaSelfAttention(\n",
      "              (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): DeiTSelfOutput(\n",
      "              (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DeiTIntermediate(\n",
      "            (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DeiTOutput(\n",
      "            (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (cls_classifier): Linear(in_features=192, out_features=1000, bias=True)\n",
      "  (distillation_classifier): Linear(in_features=192, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f89445",
   "metadata": {},
   "source": [
    "## 建立`model4hls`使得HLS4ML可以識別，並轉移DeiT-T `model`權重至`model4hls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9de863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "class Transformer4HLS(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, activation, norm_first, device):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.norm_first = norm_first\n",
    "        self.device = device\n",
    "        self._init_transformer()\n",
    "\n",
    "    def _init_transformer(self):\n",
    "        norm = nn.LayerNorm(self.d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "                                    nn.TransformerEncoderLayer(d_model=self.d_model, \n",
    "                                                                nhead=self.nhead,\n",
    "                                                                dim_feedforward=self.dim_feedforward,\n",
    "                                                                dropout=self.dropout,\n",
    "                                                                activation=self.activation,\n",
    "                                                                norm_first=self.norm_first,\n",
    "                                                                device=self.device),\n",
    "                                    self.num_encoder_layers,\n",
    "                                    norm=norm\n",
    "                                    )\n",
    "\n",
    "    def forward(self, src):  \n",
    "        output = self.transformer_encoder(src)\n",
    "        return output\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model4hls = Transformer4HLS(d_model=192, \n",
    "                          nhead=3, \n",
    "                          num_encoder_layers=12, \n",
    "                          dim_feedforward=768, \n",
    "                          dropout=0, \n",
    "                          activation='gelu', \n",
    "                          norm_first=True, \n",
    "                          device='cpu')\n",
    "\n",
    "for i in range(12):\n",
    "    wq = model.deit.encoder.layer[i].attention.attention.query.weight\n",
    "    wk = model.deit.encoder.layer[i].attention.attention.key.weight\n",
    "    wv = model.deit.encoder.layer[i].attention.attention.value.weight\n",
    "    bq = model.deit.encoder.layer[i].attention.attention.query.bias\n",
    "    bk = model.deit.encoder.layer[i].attention.attention.key.bias\n",
    "    bv = model.deit.encoder.layer[i].attention.attention.value.bias\n",
    "    #concatenate wq, wk, wv\n",
    "    w_in_proj = torch.cat([wq, wk, wv], dim=0)\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.in_proj_weight.data = w_in_proj\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.in_proj_bias.data = torch.cat([bq, bk, bv], dim=0)\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.out_proj.weight.data = model.deit.encoder.layer[i].attention.output.dense.weight\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.out_proj.bias.data = model.deit.encoder.layer[i].attention.output.dense.bias\n",
    "    model4hls.transformer_encoder.layers[i].linear1.weight.data = model.deit.encoder.layer[i].intermediate.dense.weight\n",
    "    model4hls.transformer_encoder.layers[i].linear1.bias.data = model.deit.encoder.layer[i].intermediate.dense.bias\n",
    "    model4hls.transformer_encoder.layers[i].linear2.weight.data = model.deit.encoder.layer[i].output.dense.weight\n",
    "    model4hls.transformer_encoder.layers[i].linear2.bias.data = model.deit.encoder.layer[i].output.dense.bias\n",
    "    model4hls.transformer_encoder.layers[i].norm1.weight.data = model.deit.encoder.layer[i].layernorm_before.weight\n",
    "    model4hls.transformer_encoder.layers[i].norm1.bias.data = model.deit.encoder.layer[i].layernorm_before.bias\n",
    "    model4hls.transformer_encoder.layers[i].norm2.weight.data = model.deit.encoder.layer[i].layernorm_after.weight\n",
    "    model4hls.transformer_encoder.layers[i].norm2.bias.data = model.deit.encoder.layer[i].layernorm_after.bias\n",
    "model4hls.transformer_encoder.norm.weight.data = model.deit.layernorm.weight\n",
    "model4hls.transformer_encoder.norm.bias.data = model.deit.layernorm.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948bba4",
   "metadata": {},
   "source": [
    "## 生成`transformer_quant_config`量化transformer encoder，並calibrate quantized model觀察quantizer的數值範圍並重新生成calibated `transformer_quant_config`\n",
    "#### Tips : 由於calibration可能會很久(取決於使用多大的calibation dataset)，建議將calibrated `transformer_quant_config`存檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770601c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantizers import *\n",
    "from synchronizer import *\n",
    "import hls4ml\n",
    "import json\n",
    "import copy\n",
    "def load_transformer_quant_config(quant_config_path: str = \"./quant_config.json\",\n",
    "                                  norm_quant_config_path: str = \"./norm_quant_config.json\",\n",
    "                                  num_layers: int = 1) -> dict:\n",
    "    with open(quant_config_path, 'r') as f:\n",
    "        quant_config = json.load(f)\n",
    "    with open(norm_quant_config_path, 'r') as f:\n",
    "        norm_quant_config = json.load(f)\n",
    "    transformer_quant_config = {}\n",
    "    for i in range(num_layers):\n",
    "        transformer_quant_config[i] = copy.deepcopy(quant_config)\n",
    "    transformer_quant_config['norm'] = copy.deepcopy(norm_quant_config)\n",
    "    return transformer_quant_config\n",
    "\n",
    "\n",
    "transformer_quant_config = load_transformer_quant_config(num_layers=12)\n",
    "qmodel = QTransformerEncoder([QTransformerEncoderLayer(192, \n",
    "                                                       3, \n",
    "                                                       768, \n",
    "                                                       activation='gelu', \n",
    "                                                       quant_config=transformer_quant_config[i], \n",
    "                                                       calibration=True, \n",
    "                                                       device='cpu') for i in range(12)], \n",
    "                             12, \n",
    "                             QLayerNorm(192, quant_config=transformer_quant_config['norm'], calibration=True, device='cpu'),\n",
    "                             TorchQuantizer(bitwidth=18, int_bitwidth=5, signed=True, calibration=True),\n",
    "                             dtype=torch.float64)\n",
    "qmodel.transfer_weights(model4hls)\n",
    "qmodel.to(torch.device('cpu'))\n",
    "qmodel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    embbed_out = model.deit.embeddings(images_tensor)\n",
    "    transformer_quant_config = calibrate_transformer(qmodel, transformer_quant_config, embbed_out[0:1,:,:].permute(1, 0, 2).type(torch.float64).to(torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412652b",
   "metadata": {},
   "source": [
    "## 生成`state` for Simulated Annealing(若沒有要透過Simulated Annealing優化，這邊只是作為同步`quant_config`和`hls_config`的方法)並測試sync_quant_config\n",
    "- `state`包含影響BRAM數目的變數`BRAMstate`以及不影響BRAM數目的變數`DSPstate`(或者說影響DSP數目的變數，但目前並沒有 ***TODO : 將DSP相關變數加入Design Search Space***)\n",
    "- `num_layers`為Transformer Block的數量\n",
    "- `weight_bits`主要包含MHSA的兩個linear的weight(或者是Q、K、V的weight以及O的weight)、FFN的兩個linear layer的weight的bit-wdith\n",
    "- `table_input_bits`和`table_output_bits`包含，MHSA的exponential、倒數查表、LayerNorm的variance查表、FFN的GeLU(CDF)查表。\n",
    "  - 2的`table_input_bits`次方即為Look-up table的Entry數量，因此這個數值只會設置約12上下\n",
    "  - `table_output_bits`即為Look-up table的width。由於BRAM的配置18 bits或9 bits的使用效率最高，因此這邊通常只會是這兩個數值或其倍數\n",
    "- `intermediate_bits`包含MHSA中的QKV cache，由於對Deit-tiny來說，QKV所需緩存很大，因此使用UltraRAM實現，而UltraRAM使用72 bits = 24 bits* 3 heads最有效率，並不將此列入BRAM計算(***TODO : KV cache存至HBM***)\n",
    "- `result_bits`包含所有layer的output，使用FIFO實現，由於選取適當的Tile size可減小FIFO深度，所以使用LUTRAM實現並不列入BRAM計算(***TODO : Formulize FIFO深度與Tile size的關係以估計LUTRAM數量***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRAMstate = gen_init_BRAMaware_state(num_layers=12, \n",
    "                                   weight_bits=32, \n",
    "                                   table_input_bits=32, \n",
    "                                   table_output_bits=32, \n",
    "                                   intermediate_bits=32, \n",
    "                                   result_bits=32)\n",
    "DSPstate = gen_init_nonBRAMaware_state(num_layers=12)\n",
    "state = {**BRAMstate, **DSPstate}\n",
    "\n",
    "config = hls4ml.utils.config_from_pytorch_model(model4hls, \n",
    "                                              granularity='name',\n",
    "                                              backend='Vitis',\n",
    "                                              input_shapes=[[1, 198, 192]], \n",
    "                                              default_precision='ap_fixed<18,5,AP_RND_CONV,AP_SAT>', \n",
    "                                              inputs_channel_last=True, \n",
    "                                              transpose_outputs=False)\n",
    "\n",
    "valid = sync_quant_config(transformer_quant_config, config, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb1aba",
   "metadata": {},
   "source": [
    "## 建立quantize model `qmodel` 並載入calibared和sync up後的 `transformer_quant_config`。配置HLS config中的Tile size以最大化BRAM以及硬體使用效率並產生 `hls_model` 和HLS project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea821ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_config in config['LayerName'].keys():\n",
    "    if layer_config.endswith('self_attn'):\n",
    "        config['LayerName'][layer_config]['TilingFactor'] = [1,1,1]\n",
    "    elif layer_config.endswith('ffn'):\n",
    "        config['LayerName'][layer_config]['TilingFactor'] = [1,1,12]\n",
    "hls_model = hls4ml.converters.convert_from_pytorch_model(\n",
    "                                                            model4hls,\n",
    "                                                            [[1, 198, 192]],\n",
    "                                                            output_dir='./hls/deit_tiny_w8_Bdk-1_Bffn-12',\n",
    "                                                            project_name='myproject',\n",
    "                                                            backend='Vitis',\n",
    "                                                            part='xcu55c-fsvh2892-2L-e',\n",
    "                                                            #board='alveo-u55c',\n",
    "                                                            hls_config=config,\n",
    "                                                            io_type='io_tile_stream',\n",
    "                                                        )\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4197496",
   "metadata": {},
   "source": [
    "## 比較`qmodel` 、 `hls_model`和`model4hls`的輸出。理論上，`qmodel` 和 `hls_model`的輸入要一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b22f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embbed_out = model.deit.embeddings(images_tensor[0:1])\n",
    "    encoder_out2 = model4hls(embbed_out)\n",
    "    output = qmodel(embbed_out.permute(1, 0, 2).type(torch.float64))\n",
    "    hls_output = hls_model.predict(embbed_out.numpy())\n",
    "    print(output)\n",
    "    print(encoder_out2)\n",
    "    print(hls_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe45a17",
   "metadata": {},
   "source": [
    "## 預測`hls_model`以及產生的HLS project的BRAM數目，這會`state`的配置與tile size的大小有關"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8544475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from estimators import *\n",
    "def layer_estimater(quant_config):\n",
    "    bram_dict = {}  \n",
    "    for layer_name in quant_config.keys():\n",
    "        bram_dict[layer_name] = {}\n",
    "        for var_name in quant_config[layer_name].keys():\n",
    "            #pprint(quant_config[layer_name][var_name])\n",
    "            bram_dict[layer_name][var_name] = VivadoVariableBRAMEstimator(name=var_name,**quant_config[layer_name][var_name])\n",
    "\n",
    "    num_ram = 0\n",
    "    for layer_name in bram_dict.keys():\n",
    "        for var_name in bram_dict[layer_name].keys():\n",
    "            ram_est = bram_dict[layer_name][var_name]\n",
    "            num_ram += ram_est.get_num_ram()\n",
    "    return num_ram\n",
    "num_ram = layer_estimater(parse_hls_model(hls_model))\n",
    "print(num_ram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_hls4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
