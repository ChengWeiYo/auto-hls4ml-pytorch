{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立一個torch model給hls4ml轉換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一開始不會有model4hls.pth，這個是從baseline model的transformer encoder取出來的weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# import torch\n",
    "# class Transformer4HLS(torch.nn.Module):\n",
    "#       def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, activation, norm_first, device):\n",
    "#           super().__init__()\n",
    "#           self.d_model = d_model\n",
    "#           self.nhead = nhead\n",
    "#           self.num_encoder_layers = num_encoder_layers\n",
    "#           self.dim_feedforward = dim_feedforward\n",
    "#           self.dropout = dropout\n",
    "#           self.activation = activation\n",
    "#           self.norm_first = norm_first\n",
    "#           self.device = device\n",
    "#           self._init_transformer()\n",
    "\n",
    "#       def _init_transformer(self):\n",
    "#           norm = nn.LayerNorm(self.d_model)\n",
    "#           self.transformer_encoder = nn.TransformerEncoder(\n",
    "#                                         nn.TransformerEncoderLayer(d_model=self.d_model, \n",
    "#                                                                    nhead=self.nhead,\n",
    "#                                                                    dim_feedforward=self.dim_feedforward,\n",
    "#                                                                    dropout=self.dropout,\n",
    "#                                                                    activation=self.activation,\n",
    "#                                                                    norm_first=self.norm_first,\n",
    "#                                                                    device=self.device),\n",
    "#                                         self.num_encoder_layers,\n",
    "#                                         norm=norm\n",
    "#                                       )\n",
    "\n",
    "#       def forward(self, src):  \n",
    "#           output = self.transformer_encoder(src)\n",
    "#           return output\n",
    "# model4hls = torch.load('./model4hls.pth', map_location=torch.device('cpu'))\n",
    "# print(model4hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.fx import symbolic_trace\n",
    "\n",
    "# traced_model = symbolic_trace(model4hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for node in traced_model.graph.nodes:\n",
    "#     print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(traced_model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "from warnings import warn\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from textwrap import wrap\n",
    "from contextlib import suppress\n",
    "from statistics import mean, stdev\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import axes_grid1\n",
    "from einops import rearrange, reduce\n",
    "from timm.models import create_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import models\n",
    "from datasets import build_dataset\n",
    "from train import get_args_parser, adjust_config, set_seed, set_run_name, count_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_name': 'soylocal', 'dataset_root_path': '../../data/soylocal', 'df_train': 'train.csv', 'df_trainval': 'train_val.csv', 'df_val': 'val.csv', 'df_test': 'test.csv', 'folder_train': 'soybean200_square', 'folder_val': 'soybean200_square', 'folder_test': 'soybean200_square', 'df_classid_classname': False}\n",
      "{'pretrained': True}\n",
      "{'horizontal_flip': True}\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('DeiT training and evaluation script', parents=[get_args_parser()])\n",
    "parser.add_argument('--compute_attention_average', action='store_true')\n",
    "parser.add_argument('--compute_attention_cka', action='store_true')\n",
    "parser.set_defaults(output_dir='results_inference')\n",
    "args = parser.parse_args(args=[])\n",
    "adjust_config(args)\n",
    "\n",
    "args.model = 'evit_deit_tiny_patch16_224.fb_in1k'\n",
    "args.dataset_name = 'cotton'\n",
    "args.dataset_root_path = '../../data/cotton'\n",
    "args.folder_train = 'cotton_square_new'\n",
    "args.folder_val = 'cotton_square_new'\n",
    "args.folder_test = 'cotton_square_new'\n",
    "args.device = 'cpu'\n",
    "# args.keep_rate = [0.1]\n",
    "# args.finetune = './results_clca/cotton_evit_vit_base_patch16_224.orig_in21k_0.1_30.pth'\n",
    "# args.finetune = './results_clca/cotton_evit_vit_base_patch16_224.orig_in21k_0.1_cla_clc_30.pth'\n",
    "args.finetune = None\n",
    "args.input_size = 16\n",
    "args.model_depth = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(18, 18), interpolation=bicubic, max_size=None, antialias=True)\n",
      "    RandomCrop(size=(16, 16), padding=None)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(18, 18), interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(16, 16))\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "set_seed(args.seed)\n",
    "\n",
    "dataset_train, args.num_classes = build_dataset(is_train=True, args=args)\n",
    "dataset_val, _ = build_dataset(is_train=False, args=args)\n",
    "\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, sampler=sampler_train,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem,\n",
    "    drop_last=False,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset_val, sampler=sampler_val,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "if args.finetune and args.ifa_head and args.clc:\n",
    "    args.setting = 'ft_clca'\n",
    "elif args.finetune and args.ifa_head:\n",
    "    args.setting = 'ft_cla'\n",
    "elif args.finetune:\n",
    "    args.setting = 'ft_bl'\n",
    "else:\n",
    "    args.setting = 'fz_bl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: evit_deit_tiny_patch16_224.fb_in1k\n",
      "[] []\n",
      "EViT(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc_norm): Identity()\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=192, out_features=80, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Creating model: {args.model}\")\n",
    "model = create_model(\n",
    "    args.model,\n",
    "    pretrained=True,\n",
    "    pretrained_cfg=None,\n",
    "    pretrained_cfg_overlay=None,\n",
    "    num_classes=1000,\n",
    "    drop_rate=args.drop,\n",
    "    drop_path_rate=args.drop_path,\n",
    "    drop_block_rate=None,\n",
    "    img_size=args.input_size,\n",
    "    args = args\n",
    ")\n",
    "if args.dataset_name.lower() != \"imagenet\":\n",
    "    model.reset_classifier(args.num_classes)\n",
    "print(model)\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if args.finetune:\n",
    "    checkpoint = torch.load(args.finetune, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters (M):  5.502416\n",
      "Trainable parameters (M):  5.502416\n"
     ]
    }
   ],
   "source": [
    "print('Total parameters (M): ', count_params(model) / (1e6))\n",
    "print('Trainable parameters (M): ', count_params(model, trainable=True) / (1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立`model4hls`使得HLS4ML可以識別，並轉移DeiT-T `model`權重至`model4hls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengwei/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class Transformer4HLS(torch.nn.Module):\n",
    "      def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, activation, norm_first, device):\n",
    "          super().__init__()\n",
    "          self.d_model = d_model\n",
    "          self.nhead = nhead\n",
    "          self.num_encoder_layers = num_encoder_layers\n",
    "          self.dim_feedforward = dim_feedforward\n",
    "          self.dropout = dropout\n",
    "          self.activation = activation\n",
    "          self.norm_first = norm_first\n",
    "          self.device = device\n",
    "          self._init_transformer()\n",
    "\n",
    "      def _init_transformer(self):\n",
    "          norm = nn.LayerNorm(self.d_model)\n",
    "          self.transformer_encoder = nn.TransformerEncoder(\n",
    "                                        nn.TransformerEncoderLayer(d_model=self.d_model, \n",
    "                                                                   nhead=self.nhead,\n",
    "                                                                   dim_feedforward=self.dim_feedforward,\n",
    "                                                                   dropout=self.dropout,\n",
    "                                                                   activation=self.activation,\n",
    "                                                                   norm_first=self.norm_first,\n",
    "                                                                   device=self.device),\n",
    "                                        self.num_encoder_layers,\n",
    "                                        norm=norm\n",
    "                                      )\n",
    "\n",
    "      def forward(self, src):  \n",
    "          output = self.transformer_encoder(src)\n",
    "          return output\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model4hls = Transformer4HLS(d_model=192, \n",
    "                          nhead=3, \n",
    "                          num_encoder_layers=args.model_depth, \n",
    "                          dim_feedforward=768, \n",
    "                          dropout=0, \n",
    "                          activation='gelu', \n",
    "                          norm_first=True, \n",
    "                          device='cpu')\n",
    "\n",
    "model4hls.eval()\n",
    "\n",
    "for i in range(args.model_depth):\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.in_proj_weight    = model.blocks[i].attn.qkv.weight\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.in_proj_bias      = model.blocks[i].attn.qkv.bias\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.out_proj.weight   = model.blocks[i].attn.proj.weight\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.out_proj.bias     = model.blocks[i].attn.proj.bias\n",
    "    model4hls.transformer_encoder.layers[i].linear1.weight              = model.blocks[i].mlp.fc1.weight\n",
    "    model4hls.transformer_encoder.layers[i].linear1.bias                = model.blocks[i].mlp.fc1.bias\n",
    "    model4hls.transformer_encoder.layers[i].linear2.weight              = model.blocks[i].mlp.fc2.weight\n",
    "    model4hls.transformer_encoder.layers[i].linear2.bias                = model.blocks[i].mlp.fc2.bias\n",
    "    model4hls.transformer_encoder.layers[i].norm1.weight                = model.blocks[i].norm1.weight\n",
    "    model4hls.transformer_encoder.layers[i].norm1.bias                  = model.blocks[i].norm1.bias\n",
    "    model4hls.transformer_encoder.layers[i].norm2.weight                = model.blocks[i].norm2.weight\n",
    "    model4hls.transformer_encoder.layers[i].norm2.bias                  = model.blocks[i].norm2.bias\n",
    "model4hls.transformer_encoder.norm.weight   = model.norm.weight\n",
    "model4hls.transformer_encoder.norm.bias     = model.norm.bias\n",
    "\n",
    "torch.save(model4hls, './model4hls_{}.pth'.format(args.input_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比較model和model4hls的輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape of encoders = torch.Size([1, 2, 192])\n",
      "Difference between pytorch model and model4hls = -7.995311170816422e-05\n"
     ]
    }
   ],
   "source": [
    "for idx in range(1):\n",
    "    random_tensor = torch.randn(1, 3, args.input_size, args.input_size)\n",
    "    # print(random_tensor)\n",
    "    with torch.no_grad():\n",
    "        x = model.patch_embed(random_tensor)\n",
    "        x = model._pos_embed(x)\n",
    "        x = model.patch_drop(x)\n",
    "        x = model.norm_pre(x)\n",
    "        print('Input shape of encoders = {}'.format(x.shape))\n",
    "        out = x\n",
    "        out2 = x\n",
    "        # out, left_token, sample_idx, compl = model.blocks[0](x)\n",
    "        # out2 = model4hls.transformer_encoder.layers[0](x.permute(1, 0, 2))\n",
    "        for i, blk in enumerate(model.blocks):\n",
    "            out, left_token, sample_idx, compl = blk(out)\n",
    "        out = model.norm(out)\n",
    "        out2 = model4hls(out2.permute(1, 0, 2))\n",
    "        out2 = out2.permute(1, 0, 2)\n",
    "        # print(out.shape)\n",
    "        # print(out2.shape)\n",
    "        # print(out)\n",
    "        # print(out2)\n",
    "        difference = (out - out2).sum()\n",
    "        print('Difference between pytorch model and model4hls = {}'.format(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成`transformer_quant_config`量化transformer encoder，並calibrate quantized model觀察quantizer的數值範圍並重新生成calibated `transformer_quant_config`\n",
    "#### Tips : 由於calibration可能會很久(取決於使用多大的calibation dataset)，建議將calibrated `transformer_quant_config`存檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantizers import *\n",
    "from synchronizer import *\n",
    "import hls4ml\n",
    "import json\n",
    "import copy\n",
    "from pprint import pprint\n",
    "\n",
    "def load_transformer_quant_config(quant_config_path: str = \"./quant_config.json\",\n",
    "                                  norm_quant_config_path: str = \"./norm_quant_config.json\",\n",
    "                                  num_layers: int = 1) -> dict:\n",
    "    with open(quant_config_path, 'r') as f:\n",
    "        quant_config = json.load(f)\n",
    "    with open(norm_quant_config_path, 'r') as f:\n",
    "        norm_quant_config = json.load(f)\n",
    "    transformer_quant_config = {}\n",
    "    for i in range(num_layers):\n",
    "        transformer_quant_config[i] = copy.deepcopy(quant_config)\n",
    "    transformer_quant_config['norm'] = copy.deepcopy(norm_quant_config)\n",
    "    return transformer_quant_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before calibration:\n",
      "{0: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 1: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 2: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 3: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 4: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 5: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 6: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 7: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 8: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 9: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 10: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                          'output': {'bitwidth': 18,\n",
      "                                     'int_bitwidth': 8,\n",
      "                                     'quantize': False},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "              'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                           'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                           'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                           'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "      'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "      'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False}},\n",
      "      'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False}},\n",
      "      'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                    'exp_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'signed': False},\n",
      "                    'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18,\n",
      "                                          'int_bitwidth': 8,\n",
      "                                          'quantize': False},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                    'inv_input': {'bitwidth': 12,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                    'inv_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 0,\n",
      "                                   'signed': False},\n",
      "                    'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                 'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                 'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                 'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                    'row_sum': {'bitwidth': 18,\n",
      "                                'int_bitwidth': 8,\n",
      "                                'signed': False},\n",
      "                    'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 11: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                          'output': {'bitwidth': 18,\n",
      "                                     'int_bitwidth': 8,\n",
      "                                     'quantize': False},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "              'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                           'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                           'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                           'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "      'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "      'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False}},\n",
      "      'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False}},\n",
      "      'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                    'exp_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'signed': False},\n",
      "                    'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18,\n",
      "                                          'int_bitwidth': 8,\n",
      "                                          'quantize': False},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                    'inv_input': {'bitwidth': 12,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                    'inv_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 0,\n",
      "                                   'signed': False},\n",
      "                    'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                 'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                 'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                 'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                    'row_sum': {'bitwidth': 18,\n",
      "                                'int_bitwidth': 8,\n",
      "                                'signed': False},\n",
      "                    'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 'norm': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "          'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "          'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "          'var_input': {'bitwidth': 12, 'int_bitwidth': 3, 'signed': False},\n",
      "          'var_output': {'bitwidth': 18, 'int_bitwidth': 8, 'signed': False}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QTransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x QTransformerEncoderLayer(\n",
       "      (self_attn): QFlashMultiheadAttention(\n",
       "        (out_proj): QLinear(\n",
       "          in_features=192, out_features=192, bias=True\n",
       "          (weight_qtzr): TorchQuantizer()\n",
       "          (bias_qtzr): TorchQuantizer()\n",
       "          (input_qtzr): TorchQuantizer()\n",
       "          (output_qtzr): TorchQuantizer()\n",
       "        )\n",
       "        (in_proj): QLinear(\n",
       "          in_features=192, out_features=576, bias=True\n",
       "          (weight_qtzr): TorchQuantizer()\n",
       "          (bias_qtzr): TorchQuantizer()\n",
       "          (input_qtzr): TorchQuantizer()\n",
       "          (output_qtzr): TorchQuantizer()\n",
       "        )\n",
       "        (scale_qtzr): TorchQuantizer()\n",
       "        (row_sum_qtzr): TorchQuantizer()\n",
       "        (exp_input_qtzr): TorchQuantizer()\n",
       "        (exp_output_qtzr): TorchQuantizer()\n",
       "        (inv_input_qtzr): TorchQuantizer()\n",
       "        (inv_output_qtzr): TorchQuantizer()\n",
       "        (attn_out_qtzr): TorchQuantizer()\n",
       "      )\n",
       "      (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "      (norm1): QLayerNorm(\n",
       "        (192,), eps=1e-05, elementwise_affine=True\n",
       "        (input_qtzr): TorchQuantizer()\n",
       "        (scale_qtzr): TorchQuantizer()\n",
       "        (bias_qtzr): TorchQuantizer()\n",
       "        (output_qtzr): TorchQuantizer()\n",
       "        (mean_qtzr): TorchQuantizer()\n",
       "        (var_input_qtzr): TorchQuantizer()\n",
       "        (var_output_qtzr): TorchQuantizer()\n",
       "        (dim_qtzr): TorchQuantizer()\n",
       "      )\n",
       "      (norm2): QLayerNorm(\n",
       "        (192,), eps=1e-05, elementwise_affine=True\n",
       "        (input_qtzr): TorchQuantizer()\n",
       "        (scale_qtzr): TorchQuantizer()\n",
       "        (bias_qtzr): TorchQuantizer()\n",
       "        (output_qtzr): TorchQuantizer()\n",
       "        (mean_qtzr): TorchQuantizer()\n",
       "        (var_input_qtzr): TorchQuantizer()\n",
       "        (var_output_qtzr): TorchQuantizer()\n",
       "        (dim_qtzr): TorchQuantizer()\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.0, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (feedforward): QFeedForward(\n",
       "        (in_proj): QLinear(\n",
       "          in_features=192, out_features=768, bias=True\n",
       "          (weight_qtzr): TorchQuantizer()\n",
       "          (bias_qtzr): TorchQuantizer()\n",
       "          (input_qtzr): TorchQuantizer()\n",
       "          (output_qtzr): TorchQuantizer()\n",
       "        )\n",
       "        (cdf_input_qtzr): TorchQuantizer()\n",
       "        (cdf_output_qtzr): TorchQuantizer()\n",
       "        (out_proj): QLinear(\n",
       "          in_features=768, out_features=192, bias=True\n",
       "          (weight_qtzr): TorchQuantizer()\n",
       "          (bias_qtzr): TorchQuantizer()\n",
       "          (input_qtzr): TorchQuantizer()\n",
       "          (output_qtzr): TorchQuantizer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): QLayerNorm(\n",
       "    (192,), eps=1e-05, elementwise_affine=True\n",
       "    (input_qtzr): TorchQuantizer()\n",
       "    (scale_qtzr): TorchQuantizer()\n",
       "    (bias_qtzr): TorchQuantizer()\n",
       "    (output_qtzr): TorchQuantizer()\n",
       "    (mean_qtzr): TorchQuantizer()\n",
       "    (var_input_qtzr): TorchQuantizer()\n",
       "    (var_output_qtzr): TorchQuantizer()\n",
       "    (dim_qtzr): TorchQuantizer()\n",
       "  )\n",
       "  (input_qtzr): TorchQuantizer()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_quant_config = load_transformer_quant_config(num_layers=args.model_depth)\n",
    "print('Before calibration:')\n",
    "pprint(transformer_quant_config)\n",
    "qmodel = QTransformerEncoder([QTransformerEncoderLayer(192, \n",
    "                                                       3, \n",
    "                                                       768, \n",
    "                                                       activation='gelu', \n",
    "                                                       quant_config=transformer_quant_config[i], \n",
    "                                                       calibration=True, \n",
    "                                                       device='cpu') for i in range(args.model_depth)], \n",
    "                             args.model_depth, \n",
    "                             QLayerNorm(192, quant_config=transformer_quant_config['norm'], calibration=True, device='cpu'),\n",
    "                             TorchQuantizer(bitwidth=18, int_bitwidth=5, signed=True, calibration=True),\n",
    "                             dtype=torch.float64)\n",
    "qmodel.transfer_weights(model4hls)\n",
    "qmodel.to(torch.device('cpu'))\n",
    "qmodel.eval()\n",
    "\n",
    "# # 校準\n",
    "# print('Calibrating...')\n",
    "# images, target = next(iter(test_loader))\n",
    "# with torch.no_grad():\n",
    "#     x = model.patch_embed(images[0:1])\n",
    "#     x = model._pos_embed(x)\n",
    "#     x = model.patch_drop(x)\n",
    "#     x = model.norm_pre(x)\n",
    "#     print(x.min())\n",
    "#     print(x.max())\n",
    "#     transformer_quant_config = calibrate_transformer(qmodel, transformer_quant_config, x.permute(1, 0, 2).type(torch.float64).to(torch.device('cpu')))\n",
    "\n",
    "# print('After calibration:')\n",
    "# pprint(transformer_quant_config)\n",
    "# #save transformer_quant_config\n",
    "# torch.save(transformer_quant_config, './transformer_quant_config_{}.pth'.format(args.input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 1}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 0,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 1}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 1,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 1,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 1: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 1}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 1,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 1,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 2: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 1,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 1,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 3: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 0},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 1,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 1,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 4: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 1,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 1,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 5: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 1,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 1,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 6: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 1,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 1,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 7: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': -1}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 1,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 1,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 8: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 1,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 0},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 1,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 9: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 0,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 0,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 1,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 1,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 10: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'input': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                          'output': {'bitwidth': 18,\n",
      "                                     'int_bitwidth': -inf,\n",
      "                                     'quantize': False},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "              'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                           'input': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                           'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                           'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "      'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "      'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 1,\n",
      "                               'signed': False}},\n",
      "      'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 1,\n",
      "                               'signed': False}},\n",
      "      'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                    'exp_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'signed': False},\n",
      "                    'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'input': {'bitwidth': 18,\n",
      "                                          'int_bitwidth': -inf,\n",
      "                                          'quantize': False},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                    'inv_input': {'bitwidth': 12,\n",
      "                                  'int_bitwidth': 1,\n",
      "                                  'signed': False},\n",
      "                    'inv_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 0,\n",
      "                                   'signed': False},\n",
      "                    'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                 'input': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                 'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                 'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                    'row_sum': {'bitwidth': 18,\n",
      "                                'int_bitwidth': 1,\n",
      "                                'signed': False},\n",
      "                    'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 11: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'input': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                          'output': {'bitwidth': 18,\n",
      "                                     'int_bitwidth': -inf,\n",
      "                                     'quantize': False},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "              'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                           'input': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                           'output': {'bitwidth': 18, 'int_bitwidth': 0},\n",
      "                           'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "      'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "      'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 0,\n",
      "                               'signed': False}},\n",
      "      'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 1,\n",
      "                               'signed': False}},\n",
      "      'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                    'exp_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'signed': False},\n",
      "                    'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'input': {'bitwidth': 18,\n",
      "                                          'int_bitwidth': -inf,\n",
      "                                          'quantize': False},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                    'inv_input': {'bitwidth': 12,\n",
      "                                  'int_bitwidth': 1,\n",
      "                                  'signed': False},\n",
      "                    'inv_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 0,\n",
      "                                   'signed': False},\n",
      "                    'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                 'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                 'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                 'weight': {'bitwidth': 18, 'int_bitwidth': 1}},\n",
      "                    'row_sum': {'bitwidth': 18,\n",
      "                                'int_bitwidth': 1,\n",
      "                                'signed': False},\n",
      "                    'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 'norm': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "          'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "          'mean': {'bitwidth': 18, 'int_bitwidth': -3},\n",
      "          'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "          'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "          'var_input': {'bitwidth': 12, 'int_bitwidth': 1, 'signed': False},\n",
      "          'var_output': {'bitwidth': 18, 'int_bitwidth': 1, 'signed': False}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8660/1672244013.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  transformer_quant_config = torch.load('./transformer_quant_config_{}.pth'.format(args.input_size))\n"
     ]
    }
   ],
   "source": [
    "#load transformer_quant_config\n",
    "transformer_quant_config = torch.load('./transformer_quant_config_{}.pth'.format(args.input_size))\n",
    "pprint(transformer_quant_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跑baseline model的準確度(跟model4hls比對)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "# from timm.utils import accuracy\n",
    "# amp_autocast = suppress\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "# header = 'Val:'\n",
    "\n",
    "# # switch to evaluation mode\n",
    "# model.eval()\n",
    "\n",
    "# for images, target in metric_logger.log_every(test_loader, 10, header):\n",
    "#     images = images.to(args.device, non_blocking=True)\n",
    "#     target = target.to(args.device, non_blocking=True)\n",
    "\n",
    "#     # compute output\n",
    "#     with amp_autocast():\n",
    "#         output = model(images)\n",
    "#     if isinstance(output, (tuple, list)):\n",
    "#         output = output[0]\n",
    "\n",
    "#     loss = criterion(output, target)\n",
    "#     acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "#     batch_size = images.shape[0]\n",
    "#     metric_logger.update(loss=loss.item())\n",
    "#     metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n",
    "#     metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n",
    "\n",
    "# # gather the stats from all processes\n",
    "# metric_logger.synchronize_between_processes()\n",
    "# print('* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'\n",
    "#         .format(top1=metric_logger.acc1, top5=metric_logger.acc5, losses=metric_logger.loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成`state` for Simulated Annealing(若沒有要透過Simulated Annealing優化，這邊只是作為同步`quant_config`和`hls_config`的方法)並測試sync_quant_config\n",
    "- `state`包含影響BRAM數目的變數`BRAMstate`以及不影響BRAM數目的變數`DSPstate`(或者說影響DSP數目的變數，但目前並沒有 ***TODO : 將DSP相關變數加入Design Search Space***)\n",
    "- `num_layers`為Transformer Block的數量\n",
    "- `weight_bits`主要包含MHSA的兩個linear的weight(或者是Q、K、V的weight以及O的weight)、FFN的兩個linear layer的weight的bit-wdith\n",
    "- `table_input_bits`和`table_output_bits`包含，MHSA的exponential、倒數查表、LayerNorm的variance查表、FFN的GeLU(CDF)查表。\n",
    "  - 2的`table_input_bits`次方即為Look-up table的Entry數量，因此這個數值只會設置約12上下(設成32可能會讓軟體本身overflow)\n",
    "  - `table_output_bits`即為Look-up table的width。由於BRAM的配置18 bits或9 bits的使用效率最高，因此這邊通常只會是這兩個數值或其倍數\n",
    "- `intermediate_bits`包含MHSA中的QKV cache，由於對Deit-tiny來說，QKV所需緩存很大，因此使用UltraRAM實現，而UltraRAM使用72 bits = 24 bits* 3 heads最有效率，並不將此列入BRAM計算(***TODO : KV cache存至HBM***)\n",
    "- `result_bits`包含所有layer的output，使用FIFO實現，由於選取適當的Tile size可減小FIFO深度，所以使用LUTRAM實現並不列入BRAM計算(***TODO : Formulize FIFO深度與Tile size的關係以估計LUTRAM數量***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "BRAMstate = gen_init_BRAMaware_state(num_layers=args.model_depth, \n",
    "                                #    weight_bits=8, \n",
    "                                   weight_bits=18, \n",
    "                                   table_input_bits=12, \n",
    "                                   table_output_bits=18, \n",
    "                                   intermediate_bits=24, \n",
    "                                #    result_bits=18)\n",
    "                                   result_bits=18)\n",
    "DSPstate = gen_init_nonBRAMaware_state(num_layers=args.model_depth)\n",
    "state = {**BRAMstate, **DSPstate}\n",
    "\n",
    "config = hls4ml.utils.config_from_pytorch_model(model4hls, \n",
    "                                              granularity='name',\n",
    "                                              backend='Vitis',\n",
    "                                              input_shapes=[[1, (args.input_size/16)**2+1, 192]], \n",
    "                                              default_precision='ap_fixed<18,5,AP_RND_CONV,AP_SAT>', \n",
    "                                              inputs_channel_last=True, \n",
    "                                              transpose_outputs=False)\n",
    "\n",
    "valid = sync_quant_config(transformer_quant_config, config, state)\n",
    "print(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立quantize model `qmodel` 並載入calibared和sync up後的 `transformer_quant_config`。配置HLS config中的Tile size以最大化BRAM以及硬體使用效率並產生 `hls_model` 和HLS project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayerName\u001b[39m\u001b[38;5;124m'\u001b[39m][layer_config][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTilingFactor\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m12\u001b[39m]\n\u001b[1;32m     23\u001b[0m hls_model \u001b[38;5;241m=\u001b[39m hls4ml\u001b[38;5;241m.\u001b[39mconverters\u001b[38;5;241m.\u001b[39mconvert_from_pytorch_model(\n\u001b[1;32m     24\u001b[0m                                                             model4hls,\n\u001b[1;32m     25\u001b[0m                                                             [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mint\u001b[39m((args\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m16\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m192\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m                                                             io_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mio_tile_stream\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     33\u001b[0m                                                         )\n\u001b[0;32m---> 34\u001b[0m \u001b[43mhls_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/hls4ml-0.0.0-py3.10.egg/hls4ml/model/graph.py:687\u001b[0m, in \u001b[0;36mModelGraph.compile\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    683\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compile the generated project and link the library into current environment.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m    Users should call this function if they want to use `predict` functionality for simulation.\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile()\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/hls4ml-0.0.0-py3.10.egg/hls4ml/model/graph.py:680\u001b[0m, in \u001b[0;36mModelGraph.write\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write the generated project to disk.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03m    This function converts the model to C++ and writes the generated files in the output\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03m    directory specified in the `config`.\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/hls4ml-0.0.0-py3.10.egg/hls4ml/backends/fpga/fpga_backend.py:194\u001b[0m, in \u001b[0;36mFPGABackend.write\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[1;32m    185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write the generated project to disk.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    This function converts the model to C++ and writes the generated files in the output\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m        model (ModelGraph): Model to write.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_writer_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/hls4ml-0.0.0-py3.10.egg/hls4ml/model/graph.py:449\u001b[0m, in \u001b[0;36mModelGraph.apply_flow\u001b[0;34m(self, flow, reapply)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_applied_flows\u001b[38;5;241m.\u001b[39mappend(applied_flows)\n\u001b[0;32m--> 449\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_sub_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapplied_flows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/hls4ml-0.0.0-py3.10.egg/hls4ml/model/graph.py:462\u001b[0m, in \u001b[0;36mModelGraph._apply_sub_flow\u001b[0;34m(self, flow_name, applied_flows)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_sub_flow(sub_flow, applied_flows)\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(flow\u001b[38;5;241m.\u001b[39moptimizers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m#print optimizer name\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     applied_passes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/hls4ml-0.0.0-py3.10.egg/hls4ml/model/optimizer/optimizer.py:312\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m(model, passes)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt_name, opt \u001b[38;5;129;01min\u001b[39;00m optimizers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(opt, ModelOptimizerPass) \u001b[38;5;129;01mand\u001b[39;00m opt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m applied_passes:\n\u001b[0;32m--> 312\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m res:\n\u001b[1;32m    314\u001b[0m             applied_passes\u001b[38;5;241m.\u001b[39madd(opt_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/hls4ml-0.0.0-py3.10.egg/hls4ml/model/optimizer/optimizer.py:94\u001b[0m, in \u001b[0;36mModelOptimizerPass.transform\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[0;32m---> 94\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval \u001b[38;5;28;01mif\u001b[39;00m retval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/hls4ml-0.0.0-py3.10.egg/hls4ml/backends/fpga/fpga_backend.py:909\u001b[0m, in \u001b[0;36mFPGABackend.write_hls\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;129m@model_optimizer\u001b[39m()\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite_hls\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_hls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/hls4ml-0.0.0-py3.10.egg/hls4ml/writer/vitis_writer.py:31\u001b[0m, in \u001b[0;36mVitisWriter.write_hls\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite_hls\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    Write the HLS project. Calls the steps from VivadoWriter, adapted for Vitis\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_hls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_nnet_utils_overrides(model)\n\u001b[1;32m     33\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_output_dir() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tar.gz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/hls4ml-0.0.0-py3.10.egg/hls4ml/writer/vivado_writer.py:778\u001b[0m, in \u001b[0;36mVivadoWriter.write_hls\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_generated_code(model)\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_yml(model)\n\u001b[0;32m--> 778\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_tar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/hls4ml-0.0.0-py3.10.egg/hls4ml/writer/vivado_writer.py:762\u001b[0m, in \u001b[0;36mVivadoWriter.write_tar\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Write the generated project as a .tar.gz archive\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \n\u001b[1;32m    757\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03m    model (ModelGraph): the hls4ml model.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mopen(model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_output_dir() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tar.gz\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw:gz\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m archive:\n\u001b[0;32m--> 762\u001b[0m     \u001b[43marchive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/tarfile.py:2008\u001b[0m, in \u001b[0;36mTarFile.add\u001b[0;34m(self, name, arcname, recursive, filter)\u001b[0m\n\u001b[1;32m   2006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recursive:\n\u001b[1;32m   2007\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(name)):\n\u001b[0;32m-> 2008\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43marcname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2012\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddfile(tarinfo)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/tarfile.py:2008\u001b[0m, in \u001b[0;36mTarFile.add\u001b[0;34m(self, name, arcname, recursive, filter)\u001b[0m\n\u001b[1;32m   2006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recursive:\n\u001b[1;32m   2007\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(name)):\n\u001b[0;32m-> 2008\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43marcname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2012\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddfile(tarinfo)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/tarfile.py:2008\u001b[0m, in \u001b[0;36mTarFile.add\u001b[0;34m(self, name, arcname, recursive, filter)\u001b[0m\n\u001b[1;32m   2006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recursive:\n\u001b[1;32m   2007\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(name)):\n\u001b[0;32m-> 2008\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43marcname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2012\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddfile(tarinfo)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/tarfile.py:2002\u001b[0m, in \u001b[0;36mTarFile.add\u001b[0;34m(self, name, arcname, recursive, filter)\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misreg():\n\u001b[1;32m   2001\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m bltn_open(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m-> 2002\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tarinfo\u001b[38;5;241m.\u001b[39misdir():\n\u001b[1;32m   2005\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddfile(tarinfo)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/tarfile.py:2030\u001b[0m, in \u001b[0;36mTarFile.addfile\u001b[0;34m(self, tarinfo, fileobj)\u001b[0m\n\u001b[1;32m   2028\u001b[0m \u001b[38;5;66;03m# If there's data to follow, append it.\u001b[39;00m\n\u001b[1;32m   2029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2030\u001b[0m     \u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2031\u001b[0m     blocks, remainder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(tarinfo\u001b[38;5;241m.\u001b[39msize, BLOCKSIZE)\n\u001b[1;32m   2032\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remainder \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/tarfile.py:251\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buf) \u001b[38;5;241m<\u001b[39m bufsize:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 251\u001b[0m     \u001b[43mdst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remainder \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    254\u001b[0m     buf \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mread(remainder)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/gzip.py:289\u001b[0m, in \u001b[0;36mGzipFile.write\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    286\u001b[0m     length \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mnbytes\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfileobj\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m length\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrc \u001b[38;5;241m=\u001b[39m zlib\u001b[38;5;241m.\u001b[39mcrc32(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrc)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qmodel = QTransformerEncoder([QTransformerEncoderLayer(embed_dim=192, \n",
    "                                                       num_heads=3, \n",
    "                                                       hidden_dim=768, \n",
    "                                                       activation='gelu', \n",
    "                                                       quant_config=transformer_quant_config[i], \n",
    "                                                       calibration=False, \n",
    "                                                       device='cpu') for i in range(args.model_depth)], \n",
    "                             args.model_depth, \n",
    "                             QLayerNorm(normalized_shape=192, \n",
    "                                        quant_config=transformer_quant_config['norm'], \n",
    "                                        calibration=False, \n",
    "                                        device='cpu'),\n",
    "                             TorchQuantizer(bitwidth=18, int_bitwidth=5, signed=True, calibration=False),\n",
    "                             dtype=torch.float64)\n",
    "qmodel.transfer_weights(model4hls)\n",
    "qmodel.to(torch.device('cpu'))\n",
    "qmodel.eval()\n",
    "for layer_config in config['LayerName'].keys():\n",
    "    if layer_config.endswith('self_attn'):\n",
    "        config['LayerName'][layer_config]['TilingFactor'] = [1,1,1]\n",
    "    elif layer_config.endswith('ffn'):\n",
    "        config['LayerName'][layer_config]['TilingFactor'] = [1,1,12]\n",
    "hls_model = hls4ml.converters.convert_from_pytorch_model(\n",
    "                                                            model4hls,\n",
    "                                                            [[1, int((args.input_size/16)**2+1), 192]],\n",
    "                                                            output_dir='./hls/deit_tiny_w8_Bdk-1_Bffn-12_{}'.format(args.input_size),\n",
    "                                                            project_name='myproject',\n",
    "                                                            backend='Vitis',\n",
    "                                                            part='xcu55c-fsvh2892-2L-e',\n",
    "                                                            #board='alveo-u55c',\n",
    "                                                            hls_config=config,\n",
    "                                                            io_type='io_tile_stream',\n",
    "                                                        )\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比較`qmodel` 、 `hls_model`和`model4hls`的輸出。理論上，`qmodel` 和 `hls_model`的輸入要一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 101, 192])\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Input size mismatch, got (), expected [192, 106, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# # print(x)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}/tb_data/tb_input_features.dat'.format(args.input_size), x.numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# output = qmodel(x.permute(1, 0, 2).type(torch.float64))\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# output = output.permute(1, 0, 2)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# encoder_out2 = model4hls(x.permute(1, 0, 2))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# encoder_out2 = encoder_out2.permute(1, 0, 2)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m hls_output \u001b[38;5;241m=\u001b[39m \u001b[43mhls_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhls/deit_tiny_w8_Bdk-1_Bffn-12_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/tb_data/tb_output_predictions.dat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39minput_size), encoder_out2\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%.6f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m test_output1 \u001b[38;5;241m=\u001b[39m hls_output \u001b[38;5;241m-\u001b[39m encoder_out2\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/hls4ml-0.0.0-py3.10.egg/hls4ml/model/graph.py:764\u001b[0m, in \u001b[0;36mModelGraph.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    763\u001b[0m     top_function, ctype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_top_function(x)\n\u001b[0;32m--> 764\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_n_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m     n_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_input_variables())\n\u001b[1;32m    766\u001b[0m     n_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_output_variables())\n",
      "File \u001b[0;32m~/anaconda3/envs/auto_hls4ml/lib/python3.10/site-packages/hls4ml-0.0.0-py3.10.egg/hls4ml/model/graph.py:754\u001b[0m, in \u001b[0;36mModelGraph._compute_n_samples\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    752\u001b[0m     n_sample, rem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(x_size, expected_size)\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rem \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput size mismatch, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_size\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_input_variables()[i]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    755\u001b[0m     n_samples\u001b[38;5;241m.\u001b[39mappend(n_sample)\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m([n_samples[i] \u001b[38;5;241m==\u001b[39m n_samples[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(xlist) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)]):\n",
      "\u001b[0;31mException\u001b[0m: Input size mismatch, got (), expected [192, 106, 1]"
     ]
    }
   ],
   "source": [
    "images, target = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    x = model.patch_embed(images[0:1])\n",
    "    x = model._pos_embed(x)\n",
    "    x = model.patch_drop(x)\n",
    "    x = model.norm_pre(x)\n",
    "    print('Double check input shape of encoders = {}'.format(x.shape))\n",
    "    print(x.shape) \n",
    "    # print(x)\n",
    "    np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}/tb_data/tb_input_features.dat'.format(args.input_size), x.numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    output = qmodel(x.permute(1, 0, 2).type(torch.float64))\n",
    "    output = output.permute(1, 0, 2)\n",
    "    encoder_out2 = model4hls(x.permute(1, 0, 2))\n",
    "    encoder_out2 = encoder_out2.permute(1, 0, 2)\n",
    "    hls_output = hls_model.predict(x.numpy())\n",
    "    np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}/tb_data/tb_output_predictions.dat'.format(args.input_size), encoder_out2.numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    test_output1 = hls_output - encoder_out2.flatten().numpy()\n",
    "    test_output1_sum = np.sum(test_output1)\n",
    "    print('Sum of difference between qmodel output and hls_output = {}'.format((output.flatten().numpy() - hls_output).sum()))\n",
    "    # print(test_output1)\n",
    "    print('Sum of difference between pytorch output and hls_output = {}'.format(test_output1_sum))\n",
    "    # print(output)\n",
    "    # print(encoder_out2)\n",
    "    # print(hls_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 101, 192])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測`hls_model`以及產生的HLS project的BRAM數目，這會`state`的配置與tile size的大小有關"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "3754.5\n"
     ]
    }
   ],
   "source": [
    "from estimators import *\n",
    "def layer_estimater(quant_config):\n",
    "    bram_dict = {}  \n",
    "    for layer_name in quant_config.keys():\n",
    "        bram_dict[layer_name] = {}\n",
    "        for var_name in quant_config[layer_name].keys():\n",
    "            #pprint(quant_config[layer_name][var_name])\n",
    "            bram_dict[layer_name][var_name] = VivadoVariableBRAMEstimator(name=var_name,**quant_config[layer_name][var_name])\n",
    "\n",
    "    num_ram = 0\n",
    "    for layer_name in bram_dict.keys():\n",
    "        for var_name in bram_dict[layer_name].keys():\n",
    "            ram_est = bram_dict[layer_name][var_name]\n",
    "            num_ram += ram_est.get_num_ram()\n",
    "    return num_ram\n",
    "num_ram = layer_estimater(parse_hls_model(hls_model))\n",
    "print(num_ram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_hls4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
