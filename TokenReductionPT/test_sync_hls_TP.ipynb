{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立一個torch model給hls4ml轉換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一開始不會有model4hls.pth，這個是從baseline model的transformer encoder取出來的weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1180: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.vit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_tiny_patch16_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1189: UserWarning: Overwriting vit_small_patch16_224 in registry with models.vit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1198: UserWarning: Overwriting vit_small_patch8_224 in registry with models.vit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch8_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1207: UserWarning: Overwriting vit_base_patch16_224 in registry with models.vit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1217: UserWarning: Overwriting vit_base_patch8_224 in registry with models.vit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch8_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1227: UserWarning: Overwriting vit_large_patch16_224 in registry with models.vit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1237: UserWarning: Overwriting vit_large_patch14_224 in registry with models.vit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch14_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1246: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.vit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_huge_patch14_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1255: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.vit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224_miil(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1266: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.vit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_medium_patch16_gap_240(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1278: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.vit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_medium_patch16_gap_256(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1290: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.vit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_gap_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1301: UserWarning: Overwriting vit_huge_patch14_gap_224 in registry with models.vit.vit_huge_patch14_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_huge_patch14_gap_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1312: UserWarning: Overwriting vit_giant_patch16_gap_224 in registry with models.vit.vit_giant_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_giant_patch16_gap_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1324: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.vit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_clip_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1334: UserWarning: Overwriting flexivit_small in registry with models.vit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def flexivit_small(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1343: UserWarning: Overwriting flexivit_base in registry with models.vit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def flexivit_base(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1352: UserWarning: Overwriting flexivit_large in registry with models.vit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def flexivit_large(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1361: UserWarning: Overwriting vit_small_patch14_dinov2 in registry with models.vit.vit_small_patch14_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch14_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1371: UserWarning: Overwriting vit_base_patch14_dinov2 in registry with models.vit.vit_base_patch14_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch14_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1381: UserWarning: Overwriting vit_large_patch14_dinov2 in registry with models.vit.vit_large_patch14_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch14_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1391: UserWarning: Overwriting vit_giant_patch14_dinov2 in registry with models.vit.vit_giant_patch14_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_giant_patch14_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1408: UserWarning: Overwriting vit_small_patch14_reg4_dinov2 in registry with models.vit.vit_small_patch14_reg4_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch14_reg4_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1421: UserWarning: Overwriting vit_base_patch14_reg4_dinov2 in registry with models.vit.vit_base_patch14_reg4_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch14_reg4_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1434: UserWarning: Overwriting vit_large_patch14_reg4_dinov2 in registry with models.vit.vit_large_patch14_reg4_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch14_reg4_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1447: UserWarning: Overwriting vit_giant_patch14_reg4_dinov2 in registry with models.vit.vit_giant_patch14_reg4_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_giant_patch14_reg4_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1464: UserWarning: Overwriting vit_base_patch16_siglip_224 in registry with models.vit.vit_base_patch16_siglip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_siglip_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1474: UserWarning: Overwriting vit_base_patch16_siglip_256 in registry with models.vit.vit_base_patch16_siglip_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_siglip_256(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1484: UserWarning: Overwriting vit_base_patch16_siglip_512 in registry with models.vit.vit_base_patch16_siglip_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_siglip_512(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1494: UserWarning: Overwriting vit_large_patch16_siglip_256 in registry with models.vit.vit_large_patch16_siglip_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_siglip_256(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1515: UserWarning: Overwriting vit_medium_patch16_reg4_gap_256 in registry with models.vit.vit_medium_patch16_reg4_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_medium_patch16_reg4_gap_256(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1537: UserWarning: Overwriting deit_tiny_patch16_224 in registry with models.vit.deit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_tiny_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1547: UserWarning: Overwriting deit_small_patch16_224 in registry with models.vit.deit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_small_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1557: UserWarning: Overwriting deit_base_patch16_224 in registry with models.vit.deit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_base_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1567: UserWarning: Overwriting deit3_small_patch16_224 in registry with models.vit.deit3_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit3_small_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1577: UserWarning: Overwriting deit3_medium_patch16_224 in registry with models.vit.deit3_medium_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit3_medium_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1587: UserWarning: Overwriting deit3_base_patch16_224 in registry with models.vit.deit3_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit3_base_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1597: UserWarning: Overwriting deit3_large_patch16_224 in registry with models.vit.deit3_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit3_large_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1607: UserWarning: Overwriting deit3_huge_patch14_224 in registry with models.vit.deit3_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit3_huge_patch14_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit_h2t.py:1246: UserWarning: Overwriting flexivit_small in registry with models.vit_h2t.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def flexivit_small(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit_h2t.py:1255: UserWarning: Overwriting flexivit_base in registry with models.vit_h2t.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def flexivit_base(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit_h2t.py:1264: UserWarning: Overwriting flexivit_large in registry with models.vit_h2t.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def flexivit_large(pretrained: bool = False, **kwargs):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "from warnings import warn\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from textwrap import wrap\n",
    "from contextlib import suppress\n",
    "from statistics import mean, stdev\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import axes_grid1\n",
    "from einops import rearrange, reduce\n",
    "from timm.models import create_model\n",
    "from timm.utils import accuracy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "\n",
    "import models\n",
    "from datasets import build_dataset\n",
    "from train import get_args_parser, adjust_config, set_seed, set_run_name, count_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_name': 'cub', 'dataset_root_path': '../../data/cub/CUB_200_2011', 'df_train': 'train.csv', 'df_trainval': 'train_val.csv', 'df_val': 'val.csv', 'df_test': 'test_100.csv', 'folder_train': 'images', 'folder_val': 'images', 'folder_test': 'images', 'df_classid_classname': 'classid_classname.csv'}\n",
      "{'pretrained': True}\n",
      "{'horizontal_flip': True}\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('DeiT training and evaluation script', parents=[get_args_parser()])\n",
    "parser.add_argument('--compute_attention_average', action='store_true')\n",
    "parser.add_argument('--compute_attention_cka', action='store_true')\n",
    "parser.set_defaults(output_dir='results_inference')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# args.model = 'topk_deit_tiny_patch16_224.fb_in1k'\n",
    "args.model = 'evit_deit_tiny_patch16_224.fb_in1k'\n",
    "# args.model = 'tome_deit_tiny_patch16_224.fb_in1k'\n",
    "args.cfg = 'configs/cub_test3.4p_ft_weakaugs.yaml'\n",
    "# args.cfg = 'configs/cotton_ft_weakaugs.yaml'\n",
    "args.device = 'cpu'\n",
    "args.keep_rate = [0.7]\n",
    "args.reduction_loc = [3, 6, 9]\n",
    "args.train_trainval = True\n",
    "args.input_size = 224\n",
    "args.model_depth = 12\n",
    "# clca\n",
    "# args.ifa_head = True\n",
    "# args.clc = True\n",
    "# args.num_clr = 1\n",
    "adjust_config(args)\n",
    "# args.finetune = './results_tiny/{}_topk_deit_tiny_patch16_224.fb_in1k_61.pth'.format(args.dataset_name)\n",
    "# args.finetune = './results_tiny/{}_topk_deit_tiny_patch16_224.fb_in1k_{}_61.pth'.format(args.dataset_name, args.keep_rate[0])\n",
    "# args.finetune = './results_tiny/{}_topk_deit_tiny_patch16_224.fb_in1k_{}_cla_clc_1_61.pth'.format(args.dataset_name, args.keep_rate[0])\n",
    "\n",
    "model_name = args.model  # 例如 'evit_deit_tiny_patch16_224.fb_in1k'\n",
    "enable_evit = re.search(r'evit', model_name) or re.search(r'tome', model_name)\n",
    "enable_tome = re.search(r'tome', model_name)\n",
    "# if enable_evit:\n",
    "#     print('model 包含 evit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(256, 256), interpolation=bicubic, max_size=None, antialias=True)\n",
      "    RandomCrop(size=(224, 224), padding=None)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(256, 256), interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "Creating model: evit_deit_tiny_patch16_224.fb_in1k\n",
      "[0.7, 0.7, 0.7] [3, 6, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EViT(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pre_softmax): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pre_softmax): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pre_softmax): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pre_softmax): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pre_softmax): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pre_softmax): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pre_softmax): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pre_softmax): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pre_softmax): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pre_softmax): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pre_softmax): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (pre_softmax): Identity()\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=192, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(args.seed)\n",
    "\n",
    "dataset_train, args.num_classes = build_dataset(is_train=True, args=args)\n",
    "dataset_val, _ = build_dataset(is_train=False, args=args)\n",
    "\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, sampler=sampler_train,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem,\n",
    "    drop_last=False,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset_val, sampler=sampler_val,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "if args.finetune and args.ifa_head and args.clc:\n",
    "    args.setting = 'ft_clca'\n",
    "elif args.finetune and args.ifa_head:\n",
    "    args.setting = 'ft_cla'\n",
    "elif args.finetune:\n",
    "    args.setting = 'ft_bl'\n",
    "else:\n",
    "    args.setting = 'fz_bl'\n",
    "\n",
    "print(f\"Creating model: {args.model}\")\n",
    "model = create_model(\n",
    "    args.model,\n",
    "    pretrained=True,\n",
    "    pretrained_cfg=None,\n",
    "    pretrained_cfg_overlay=None,\n",
    "    num_classes=1000,\n",
    "    drop_rate=args.drop,\n",
    "    drop_path_rate=args.drop_path,\n",
    "    drop_block_rate=None,\n",
    "    img_size=args.input_size,\n",
    "    args = args\n",
    ")\n",
    "if args.dataset_name.lower() != \"imagenet\":\n",
    "    model.reset_classifier(args.num_classes)\n",
    "if args.num_clr:\n",
    "    model.add_clr(args.num_clr)\n",
    "\n",
    "if args.finetune:\n",
    "    checkpoint = torch.load(args.finetune, map_location='cpu')\n",
    "    # model.load_state_dict(checkpoint['model'], strict=True)\n",
    "\n",
    "    checkpoint_model = checkpoint['model']\n",
    "    state_dict = model.state_dict()\n",
    "    for k in ['head.weight', 'head.bias', 'head_dist.weight', 'head_dist.bias']:\n",
    "        if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "            print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "            del checkpoint_model[k]\n",
    "\n",
    "    # interpolate position embedding\n",
    "    pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
    "    embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "    num_patches = model.patch_embed.num_patches\n",
    "    num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
    "    # height (== width) for the checkpoint position embedding\n",
    "    orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
    "    # height (== width) for the new position embedding\n",
    "    new_size = int(num_patches ** 0.5)\n",
    "    # class_token and dist_token are kept unchanged\n",
    "    extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "    # only the position tokens are interpolated\n",
    "    pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "    pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
    "    pos_tokens = torch.nn.functional.interpolate(\n",
    "        pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
    "    pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "    new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "    checkpoint_model['pos_embed'] = new_pos_embed\n",
    "\n",
    "    model.load_state_dict(checkpoint_model, strict=False)\n",
    "\n",
    "model.to(args.device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立`model4hls`使得HLS4ML可以識別，並轉移DeiT-T `model`權重至`model4hls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengwei/miniconda3/envs/auto_hls4ml/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "class Transformer4HLS(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward, dropout, activation, norm_first, device, ifa_head, num_clr=0):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.norm_first = norm_first\n",
    "        self.device = device\n",
    "        self.ifa_head = ifa_head\n",
    "        self.num_clr = num_clr\n",
    "\n",
    "        self._init_transformer()\n",
    "        # if num_clr > 0:\n",
    "        #     self._init_clr()\n",
    "\n",
    "    def _init_transformer(self):\n",
    "        # norm = nn.LayerNorm(self.d_model)\n",
    "        if self.ifa_head:\n",
    "            norm = None\n",
    "        else:\n",
    "            norm = nn.LayerNorm(self.d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "                                    nn.TransformerEncoderLayer(d_model=self.d_model, \n",
    "                                                                nhead=self.nhead,\n",
    "                                                                dim_feedforward=self.dim_feedforward,\n",
    "                                                                dropout=self.dropout,\n",
    "                                                                activation=self.activation,\n",
    "                                                                norm_first=self.norm_first,\n",
    "                                                                device=self.device),\n",
    "                                    self.num_encoder_layers,\n",
    "                                    norm=norm\n",
    "                                    )\n",
    "    # def _init_clr(self):\n",
    "    #     self.clr = nn.Parameter(torch.zeros(1, self.num_clr, self.d_model))\n",
    "\n",
    "    def forward(self, src):  \n",
    "        output = self.transformer_encoder(src)\n",
    "        return output\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model4hls = Transformer4HLS(d_model=192, \n",
    "                          nhead=3, \n",
    "                          num_encoder_layers=args.model_depth, \n",
    "                          dim_feedforward=768, \n",
    "                          dropout=0, \n",
    "                          activation='gelu', \n",
    "                          norm_first=True, \n",
    "                          device='cpu',\n",
    "                          ifa_head=args.ifa_head,\n",
    "                          num_clr=args.num_clr if args.num_clr else 0)\n",
    "\n",
    "model4hls.eval()\n",
    "\n",
    "for i in range(args.model_depth):\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.in_proj_weight    = model.blocks[i].attn.qkv.weight\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.in_proj_bias      = model.blocks[i].attn.qkv.bias\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.out_proj.weight   = model.blocks[i].attn.proj.weight\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.out_proj.bias     = model.blocks[i].attn.proj.bias\n",
    "    model4hls.transformer_encoder.layers[i].linear1.weight              = model.blocks[i].mlp.fc1.weight\n",
    "    model4hls.transformer_encoder.layers[i].linear1.bias                = model.blocks[i].mlp.fc1.bias\n",
    "    model4hls.transformer_encoder.layers[i].linear2.weight              = model.blocks[i].mlp.fc2.weight\n",
    "    model4hls.transformer_encoder.layers[i].linear2.bias                = model.blocks[i].mlp.fc2.bias\n",
    "    model4hls.transformer_encoder.layers[i].norm1.weight                = model.blocks[i].norm1.weight\n",
    "    model4hls.transformer_encoder.layers[i].norm1.bias                  = model.blocks[i].norm1.bias\n",
    "    model4hls.transformer_encoder.layers[i].norm2.weight                = model.blocks[i].norm2.weight\n",
    "    model4hls.transformer_encoder.layers[i].norm2.bias                  = model.blocks[i].norm2.bias\n",
    "\n",
    "if not args.ifa_head:\n",
    "    model4hls.transformer_encoder.norm.weight   = model.norm.weight\n",
    "    model4hls.transformer_encoder.norm.bias     = model.norm.bias\n",
    "\n",
    "# if args.num_clr and hasattr(model, 'clr'):\n",
    "#     print(model4hls.clr.data)\n",
    "#     model4hls.clr.data = model.clr.data.clone()\n",
    "#     print(f\"Transferred CLR tokens: {model.clr.shape} -> {model4hls.clr.shape}\")\n",
    "    \n",
    "    # print(model4hls.clr.data)\n",
    "\n",
    "# torch.save(model4hls, './model4hls_{}.pth'.format(args.input_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比較model和model4hls的輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(1):\n",
    "#     random_tensor = torch.randn(1, 3, args.input_size, args.input_size)\n",
    "#     # 將 random_tensor 移動到與模型相同的設備\n",
    "#     random_tensor = random_tensor.to(args.device)\n",
    "#     model4hls.to(args.device)\n",
    "#     # print(random_tensor)\n",
    "#     with torch.no_grad():\n",
    "#         x = model.patch_embed(random_tensor)\n",
    "#         x = model._pos_embed(x)\n",
    "#         x = model.patch_drop(x)\n",
    "#         x = model.norm_pre(x)\n",
    "#         print('Input shape of encoders = {}'.format(x.shape))\n",
    "#         out = x\n",
    "#         out2 = x\n",
    "#         # out, left_token, sample_idx, compl = model.blocks[0](x)\n",
    "#         # out2 = model4hls.transformer_encoder.layers[0](x.permute(1, 0, 2))\n",
    "#         for i, blk in enumerate(model.blocks):\n",
    "#             # print('Processing block {}'.format(i))\n",
    "#             if enable_evit:\n",
    "#                 out, left_token, sample_idx, compl = blk(out) # for evit\n",
    "#             else:\n",
    "#                 out, left_token, sample_idx = blk(out) # for topk  \n",
    "#         out2 = model4hls(out2.permute(1, 0, 2))\n",
    "#         out2 = out2.permute(1, 0, 2)\n",
    "#         if not args.ifa_head:\n",
    "#             out = model.norm(out)\n",
    "#         print(out.shape)\n",
    "#         print(out2.shape)\n",
    "#         print(out[0][0][0:3])\n",
    "#         print(out2[0][0][0:3])\n",
    "        # difference = (out - out2).abs().max()\n",
    "        \n",
    "        # print('Difference between pytorch model and model4hls = {}'.format(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成`transformer_quant_config`量化transformer encoder，並calibrate quantized model觀察quantizer的數值範圍並重新生成calibated `transformer_quant_config`\n",
    "#### Tips : 由於calibration可能會很久(取決於使用多大的calibation dataset)，建議將calibrated `transformer_quant_config`存檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 04:29:35.771627: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-31 04:29:35.802946: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from quantizers import *\n",
    "from synchronizer import *\n",
    "from quantizers_old import calibrate_transformer as old_calibrate_transformer\n",
    "import hls4ml\n",
    "import json\n",
    "import copy\n",
    "from pprint import pprint\n",
    "\n",
    "def add_topk_cfg_if_needed(qcfg_layer: dict):\n",
    "    if 'topk' not in qcfg_layer:\n",
    "        qcfg_layer['topk'] = {\n",
    "            'input':  {'bitwidth': 18, 'int_bitwidth': 8},\n",
    "        }\n",
    "\n",
    "def add_clc_push_cfg_if_needed(qcfg_layer: dict):\n",
    "    if 'clc_push' not in qcfg_layer:\n",
    "        qcfg_layer['clc_push'] = {\n",
    "            'input':  {'bitwidth': 18, 'int_bitwidth': 8},\n",
    "        }\n",
    "\n",
    "def add_clc_recover_cfg_if_needed(qcfg_layer: dict):\n",
    "    if 'clc_recover' not in qcfg_layer:\n",
    "        qcfg_layer['clc_recover'] = {\n",
    "            'input':  {'bitwidth': 18, 'int_bitwidth': 8},\n",
    "        }\n",
    "\n",
    "def inject_reduction_and_clc_to_quant_cfg(transformer_quant_config: dict,\n",
    "                                          num_layers: int,\n",
    "                                          reduction_loc: list[int],\n",
    "                                          keep_rates: list[float],\n",
    "                                          use_clc: bool,\n",
    "                                          clc_recover_at_last: bool):\n",
    "    if keep_rates:\n",
    "            if len(keep_rates) == 1 and len(reduction_loc) > 1:\n",
    "                keep_rates = keep_rates * len(reduction_loc)\n",
    "            elif len(keep_rates) == len(reduction_loc):\n",
    "                keep_rates = keep_rates\n",
    "            else:\n",
    "                keep_rates = (keep_rates * ((len(reduction_loc) // len(keep_rates)) + 1))[:len(reduction_loc)]\n",
    "    print(f\"Final keep rates for reduction layers: {keep_rates}\")\n",
    "\n",
    "    for loc in reduction_loc:\n",
    "        if loc < num_layers:\n",
    "            print(f\"Injecting TopK quant config at layer {loc} with keep_rate {keep_rates[reduction_loc.index(loc)] if keep_rates else 'N/A'}\")\n",
    "            # if keep_rates[reduction_loc.index(loc)] < 1.0:\n",
    "            add_topk_cfg_if_needed(transformer_quant_config[loc])\n",
    "\n",
    "    # 2) CLC：在「push 發生到 recover 之前的層」加 clc_push，\n",
    "    #         並在 recover 的層加 clc_recover\n",
    "    if use_clc:\n",
    "        if clc_recover_at_last:\n",
    "            recovery_layers = reduction_loc + [num_layers - 2] if num_layers >= 2 else reduction_loc\n",
    "        else:\n",
    "            recovery_layers = reduction_loc\n",
    "            \n",
    "        last_recover = max(recovery_layers)\n",
    "        for i in range(last_recover):  # < last_recover 的層都可能 push\n",
    "            if i < num_layers:\n",
    "                add_clc_push_cfg_if_needed(transformer_quant_config[i])\n",
    "        for i in recovery_layers:  # recover 的層\n",
    "            if i < num_layers:\n",
    "                add_clc_recover_cfg_if_needed(transformer_quant_config[i])\n",
    "\n",
    "def load_transformer_quant_config(quant_config_path: str = \"./quant_config.json\",\n",
    "                                  norm_quant_config_path: str = \"./norm_quant_config.json\",\n",
    "                                  num_layers: int = 1) -> dict:\n",
    "    with open(quant_config_path, 'r') as f:\n",
    "        quant_config = json.load(f)\n",
    "    with open(norm_quant_config_path, 'r') as f:\n",
    "        norm_quant_config = json.load(f)\n",
    "    transformer_quant_config = {}\n",
    "    for i in range(num_layers):\n",
    "        transformer_quant_config[i] = copy.deepcopy(quant_config)\n",
    "    transformer_quant_config['norm'] = copy.deepcopy(norm_quant_config)\n",
    "    return transformer_quant_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final keep rates for reduction layers: [0.7, 0.7, 0.7]\n",
      "Injecting TopK quant config at layer 3 with keep_rate 0.7\n",
      "Injecting TopK quant config at layer 6 with keep_rate 0.7\n",
      "Injecting TopK quant config at layer 9 with keep_rate 0.7\n",
      "Before calibration:\n",
      "{0: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 1: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 2: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 3: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "     'topk': {'input': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      " 4: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 5: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 6: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "     'topk': {'input': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      " 7: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 8: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 9: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 8,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': 8,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 8,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "     'topk': {'input': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      " 10: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                          'output': {'bitwidth': 18,\n",
      "                                     'int_bitwidth': 8,\n",
      "                                     'quantize': False},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "              'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                           'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                           'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                           'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "      'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "      'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False}},\n",
      "      'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False}},\n",
      "      'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                    'exp_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'signed': False},\n",
      "                    'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18,\n",
      "                                          'int_bitwidth': 8,\n",
      "                                          'quantize': False},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                    'inv_input': {'bitwidth': 12,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                    'inv_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 0,\n",
      "                                   'signed': False},\n",
      "                    'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                 'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                 'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                 'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                    'row_sum': {'bitwidth': 18,\n",
      "                                'int_bitwidth': 8,\n",
      "                                'signed': False},\n",
      "                    'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 11: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                          'input': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': 8,\n",
      "                                    'quantize': False},\n",
      "                          'output': {'bitwidth': 18,\n",
      "                                     'int_bitwidth': 8,\n",
      "                                     'quantize': False},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "              'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                           'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                           'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                           'weight': {'bitwidth': 18, 'int_bitwidth': 8}}},\n",
      "      'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "      'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False}},\n",
      "      'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 8,\n",
      "                               'signed': False}},\n",
      "      'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                    'exp_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'signed': False},\n",
      "                    'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'input': {'bitwidth': 18,\n",
      "                                          'int_bitwidth': 8,\n",
      "                                          'quantize': False},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                    'inv_input': {'bitwidth': 12,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                    'inv_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 0,\n",
      "                                   'signed': False},\n",
      "                    'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                 'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                 'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                 'weight': {'bitwidth': 18, 'int_bitwidth': 8}},\n",
      "                    'row_sum': {'bitwidth': 18,\n",
      "                                'int_bitwidth': 8,\n",
      "                                'signed': False},\n",
      "                    'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 'norm': {'bias': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "          'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "          'mean': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "          'output': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "          'scale': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "          'var_input': {'bitwidth': 12, 'int_bitwidth': 3, 'signed': False},\n",
      "          'var_output': {'bitwidth': 18, 'int_bitwidth': 8, 'signed': False}}}\n",
      "Enable CLC: False\n",
      "Number of CLR tokens: 0\n",
      "Reduction locations: [3, 6, 9]\n",
      "Recovery layers: []\n",
      "Keep rates: [0.7, 0.7, 0.7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengwei/miniconda3/envs/auto_hls4ml/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Calibrating:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-11.060524940490723, grad_fn=<MinBackward1>)\n",
      "tensor(13.810079574584961, grad_fn=<MaxBackward1>)\n",
      "Calibration data stats:\n",
      "  mean: 0.013068894855678082, std: 1.2197645902633667\n",
      "input:  tensor([ 0.043638907372952, -0.071037769317627, -1.415930986404419],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "tensor([182,  65, 195,   0, 186, 111,  55,  13, 120, 187, 189, 181,  64,  93,\n",
      "          1, 123,  79,  92, 119, 108, 130, 112, 105,  29, 194, 135, 136,  27,\n",
      "         41,   5, 167, 102, 106, 121,  97, 137, 188, 126, 122,   2, 133,  83,\n",
      "        117,  94, 107,  90,  80, 185,  91,  89, 150, 176,  78, 191,  69,   6,\n",
      "        159, 125, 144,  14, 129, 168, 177,  77, 172, 139,  82, 140, 134, 101,\n",
      "         84, 143,  63, 190, 183, 154,  51, 192,  12, 132, 109,  34, 162,  95,\n",
      "        153,  70,  30,  66, 160, 147, 128, 115,   4, 104,  67,   3, 118, 149,\n",
      "        171,  60, 184,  87, 173,  35, 103,  54,  76, 110, 158, 114, 175,  56,\n",
      "        148,  42,  53,  58,  28,  48, 142,  26,  47, 193,  96,  46, 157, 127,\n",
      "        152,  50,  52, 161, 163, 145,  88, 151,  98,   8,  11, 164])\n",
      "tensor([ 77,  43,  74, 131,  53,  35, 109,  54,  62, 124, 110,  65,  34,  42,\n",
      "        128, 112, 137,  80, 129,  36, 130, 136,  67, 123,  32, 101,  92, 111,\n",
      "         76,   2,  64, 102,  95,  29,  90,  63, 120, 108,  15,  41, 113,  61,\n",
      "         97,  44,  51, 127,  99,  89, 103,  91, 132,  21,  19,  66, 125,  75,\n",
      "         84,  10,  85,  78,  73, 133,  70, 126, 134,  52,  79, 107,  81,  22,\n",
      "         96,  88, 100, 114,  40,  57,  98, 119,  86,  33,   0,  93,  68,  16,\n",
      "         58, 115,   8,  50,  56,  14,  25,  83,  55, 135,  87,  48, 118,  13])\n",
      "tensor([21, 28, 25, 27, 26, 35, 46,  9, 34, 19, 58, 20, 16, 57, 42, 15, 37, 53,\n",
      "        43, 36, 54, 64, 38, 47, 17, 44, 45, 33, 14, 22, 59, 23, 39,  6, 65, 18,\n",
      "        24, 56, 67, 55, 52, 32, 49,  7, 48, 13, 60, 11,  1, 62,  8, 75, 68, 10,\n",
      "        88, 30, 66,  0, 73,  3, 74, 93, 78, 87, 97, 76, 84, 91, 61, 41])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calibrating: 100%|██████████| 1/1 [01:21<00:00, 81.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "transformer_quant_config = load_transformer_quant_config(num_layers=args.model_depth)\n",
    "\n",
    "inject_reduction_and_clc_to_quant_cfg(\n",
    "    transformer_quant_config,\n",
    "    num_layers=args.model_depth,\n",
    "    reduction_loc=args.reduction_loc,\n",
    "    keep_rates=(args.keep_rate if isinstance(args.keep_rate, list) else [args.keep_rate]),\n",
    "    use_clc=args.clc,\n",
    "    clc_recover_at_last=args.clc_recover_at_last\n",
    ")\n",
    "print('Before calibration:')\n",
    "pprint(transformer_quant_config)\n",
    "\n",
    "qmodel = QTransformerEncoder([QTransformerEncoderLayer(192, \n",
    "                                                       3, \n",
    "                                                       768, \n",
    "                                                       activation='gelu', \n",
    "                                                       quant_config=transformer_quant_config[i], \n",
    "                                                       calibration=True, \n",
    "                                                       device=args.device,\n",
    "                                                       enable_topk=i in args.reduction_loc,\n",
    "                                                       enable_evit=enable_evit,\n",
    "                                                       enable_clc=args.clc,\n",
    "                                                       num_clr=args.num_clr) for i in range(args.model_depth)], \n",
    "                             args.model_depth, \n",
    "                             QLayerNorm(192, quant_config=transformer_quant_config['norm'], calibration=True, device=args.device),\n",
    "                             TorchQuantizer(bitwidth=18, int_bitwidth=5, signed=True, calibration=True),\n",
    "                             args,\n",
    "                             dtype=torch.float64)\n",
    "qmodel.transfer_weights(model4hls)\n",
    "qmodel.to(args.device)\n",
    "qmodel.eval()\n",
    "\n",
    "# 假設你要用多個 batch 做校準\n",
    "num_calib_batches = 1  # 例如只取前 1 個 batch 校準\n",
    "args.batch_size = 512\n",
    "calib_iter = iter(train_loader)\n",
    "\n",
    "for i in tqdm(range(num_calib_batches), desc=\"Calibrating\"):\n",
    "    images, target = next(calib_iter)\n",
    "    images = images.to(args.device)\n",
    "    # ...前處理...\n",
    "    x = model.patch_embed(images)\n",
    "    x = model._pos_embed(x)\n",
    "    x = model.patch_drop(x)\n",
    "    x = model.norm_pre(x)\n",
    "    print(x.min())\n",
    "    print(x.max())\n",
    "    # 執行一次校準\n",
    "    # 檢查calibration data和test data的統計差異\n",
    "    print(\"Calibration data stats:\")\n",
    "    print(f\"  mean: {x.mean()}, std: {x.std()}\")\n",
    "    transformer_quant_config = calibrate_transformer(qmodel, transformer_quant_config, x.permute(1, 0, 2).type(torch.float64))\n",
    "\n",
    "# print('After calibration:')\n",
    "# pprint(transformer_quant_config)\n",
    "#save transformer_quant_config\n",
    "# torch.save(transformer_quant_config, './transformer_quant_config_{}_{}_{}_{}_{}.pth'.format(args.input_size, args.finetune.split('/')[-1].replace('.pth', ''), args.dataset_name, num_calib_batches, args.batch_size))\n",
    "# torch.save(transformer_quant_config, './transformer_quant_config_{}_test.pth'.format(args.input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 1}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 1}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 7,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 9},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 7,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 1: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -2},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 1}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 6,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 9},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 6,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 2: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -2},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 5,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 5,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 3: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 0},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 6,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 6,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "     'topk': {'input': {'bitwidth': 18, 'int_bitwidth': 5}}},\n",
      " 4: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 5,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 7},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 5,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 5: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 6,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 7},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 6,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 6: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 6,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 7},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 6,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "     'topk': {'input': {'bitwidth': 18, 'int_bitwidth': 5}}},\n",
      " 7: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': -1}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 5,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 5,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 8: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 6,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 0},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 7},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 6,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 9: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 6,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 18, 'int_bitwidth': 7},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 6,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "     'topk': {'input': {'bitwidth': 18, 'int_bitwidth': 6}}},\n",
      " 10: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'input': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                          'output': {'bitwidth': 18,\n",
      "                                     'int_bitwidth': -inf,\n",
      "                                     'quantize': False},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "              'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                           'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                           'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                           'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "      'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "      'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 2,\n",
      "                               'signed': False}},\n",
      "      'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 2,\n",
      "                               'signed': False}},\n",
      "      'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                    'exp_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'signed': False},\n",
      "                    'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'input': {'bitwidth': 18,\n",
      "                                          'int_bitwidth': -inf,\n",
      "                                          'quantize': False},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                    'inv_input': {'bitwidth': 12,\n",
      "                                  'int_bitwidth': 5,\n",
      "                                  'signed': False},\n",
      "                    'inv_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 0,\n",
      "                                   'signed': False},\n",
      "                    'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                 'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                                 'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                 'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                    'row_sum': {'bitwidth': 18,\n",
      "                                'int_bitwidth': 5,\n",
      "                                'signed': False},\n",
      "                    'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 11: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'input': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                          'output': {'bitwidth': 18,\n",
      "                                     'int_bitwidth': -inf,\n",
      "                                     'quantize': False},\n",
      "                          'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "              'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                           'input': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                           'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                           'weight': {'bitwidth': 18, 'int_bitwidth': 1}}},\n",
      "      'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "      'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 2,\n",
      "                               'signed': False}},\n",
      "      'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 2,\n",
      "                               'signed': False}},\n",
      "      'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                    'exp_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'signed': False},\n",
      "                    'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'input': {'bitwidth': 18,\n",
      "                                          'int_bitwidth': -inf,\n",
      "                                          'quantize': False},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                                'weight': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "                    'inv_input': {'bitwidth': 12,\n",
      "                                  'int_bitwidth': 5,\n",
      "                                  'signed': False},\n",
      "                    'inv_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 0,\n",
      "                                   'signed': False},\n",
      "                    'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                 'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                                 'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                                 'weight': {'bitwidth': 18, 'int_bitwidth': 1}},\n",
      "                    'row_sum': {'bitwidth': 18,\n",
      "                                'int_bitwidth': 5,\n",
      "                                'signed': False},\n",
      "                    'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 'norm': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "          'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "          'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "          'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "          'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "          'var_input': {'bitwidth': 12, 'int_bitwidth': 3, 'signed': False},\n",
      "          'var_output': {'bitwidth': 18, 'int_bitwidth': 2, 'signed': False}}}\n"
     ]
    }
   ],
   "source": [
    "#load transformer_quant_config\n",
    "# transformer_quant_config = torch.load('./transformer_quant_config_{}_{}_{}.pth'.format(args.input_size, args.finetune.split('/')[-1].replace('.pth', ''), args.dataset_name))\n",
    "# transformer_quant_config = torch.load('./transformer_quant_config_{}_{}_{}_batch32.pth'.format(args.input_size, args.finetune.split('/')[-1].replace('.pth', ''), args.dataset_name))\n",
    "# transformer_quant_config = torch.load('./transformer_quant_config_{}_{}_{}_{}_{}.pth'.format(args.input_size, args.finetune.split('/')[-1].replace('.pth', ''), args.dataset_name, num_calib_batches, args.batch_size))\n",
    "# transformer_quant_config = torch.load('./transformer_quant_config_{}_test.pth'.format(args.input_size))\n",
    "# transformer_quant_config = torch.load('./transformer_quant_config_224_cub_topk_deit_tiny_patch16_224.fb_in1k_0.25_61_cub_10_256.pth')\n",
    "pprint(transformer_quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inject_reduction_and_clc_to_quant_cfg(\n",
    "#     transformer_quant_config,\n",
    "#     num_layers=args.model_depth,\n",
    "#     reduction_loc=args.reduction_loc,\n",
    "#     keep_rates=(args.keep_rate if isinstance(args.keep_rate, list) else [args.keep_rate]),\n",
    "#     use_clc=args.clc,\n",
    "#     clc_recover_at_last=args.clc_recover_at_last\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成`state` for Simulated Annealing(若沒有要透過Simulated Annealing優化，這邊只是作為同步`quant_config`和`hls_config`的方法)並測試sync_quant_config\n",
    "- `state`包含影響BRAM數目的變數`BRAMstate`以及不影響BRAM數目的變數`DSPstate`(或者說影響DSP數目的變數，但目前並沒有 ***TODO : 將DSP相關變數加入Design Search Space***)\n",
    "- `num_layers`為Transformer Block的數量\n",
    "- `weight_bits`主要包含MHSA的兩個linear的weight(或者是Q、K、V的weight以及O的weight)、FFN的兩個linear layer的weight的bit-wdith\n",
    "- `table_input_bits`和`table_output_bits`包含，MHSA的exponential、倒數查表、LayerNorm的variance查表、FFN的GeLU(CDF)查表。\n",
    "  - 2的`table_input_bits`次方即為Look-up table的Entry數量，因此這個數值只會設置約12上下(設成32可能會讓軟體本身overflow)\n",
    "  - `table_output_bits`即為Look-up table的width。由於BRAM的配置18 bits或9 bits的使用效率最高，因此這邊通常只會是這兩個數值或其倍數\n",
    "- `intermediate_bits`包含MHSA中的QKV cache，由於對Deit-tiny來說，QKV所需緩存很大，因此使用UltraRAM實現，而UltraRAM使用72 bits = 24 bits* 3 heads最有效率，並不將此列入BRAM計算(***TODO : KV cache存至HBM***)\n",
    "- `result_bits`包含所有layer的output，使用FIFO實現，由於選取適當的Tile size可減小FIFO深度，所以使用LUTRAM實現並不列入BRAM計算(***TODO : Formulize FIFO深度與Tile size的關係以估計LUTRAM數量***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import synchronizer\n",
    "\n",
    "importlib.reload(synchronizer)\n",
    "from synchronizer import *\n",
    "\n",
    "BRAMstate = gen_init_BRAMaware_state(num_layers=args.model_depth, \n",
    "                                   weight_bits=8, \n",
    "                                   table_input_bits=12, \n",
    "                                   table_output_bits=18, \n",
    "                                   intermediate_bits=24,\n",
    "                                   result_bits=18,\n",
    "                                   include_norm=not args.ifa_head)\n",
    "# BRAMstate = gen_init_BRAMaware_state(num_layers=args.model_depth, \n",
    "#                                    weight_bits=8, \n",
    "#                                    table_input_bits=12, \n",
    "#                                    table_output_bits=8, \n",
    "#                                    intermediate_bits=24,\n",
    "#                                    result_bits=8,\n",
    "#                                    include_norm=not args.ifa_head)\n",
    "# BRAMstate = gen_init_BRAMaware_state(num_layers=args.model_depth, \n",
    "#                                      weight_bits=32, \n",
    "#                                      table_input_bits=12, \n",
    "#                                      table_output_bits=32, \n",
    "#                                      intermediate_bits=32,\n",
    "#                                      result_bits=32,\n",
    "#                                      include_norm=not args.ifa_head)\n",
    "DSPstate = gen_init_nonBRAMaware_state(num_layers=args.model_depth, include_norm=not args.ifa_head)\n",
    "REDstate  = gen_reduction_state(transformer_quant_config, result_bits=18)\n",
    "# state = {**BRAMstate, **DSPstate}\n",
    "state = {**BRAMstate, **DSPstate, **REDstate}\n",
    "\n",
    "model4hls.to(device='cpu')\n",
    "if not args.ifa_head:\n",
    "    input_shapes = [[1, int((args.input_size/16)**2+1), 192]]  # [batch_size, num_tokens, embed_dim]\n",
    "else:\n",
    "    input_shapes = [[1, int((args.input_size/16)**2+1+args.num_clr), 192]]  # [batch_size, num_tokens, embed_dim]\n",
    "config = hls4ml.utils.config_from_pytorch_model(model4hls, \n",
    "                                              granularity='name',\n",
    "                                              backend='Vitis',\n",
    "                                              input_shapes=input_shapes, \n",
    "                                              default_precision='ap_fixed<18,5,AP_RND_CONV,AP_SAT>', \n",
    "                                              inputs_channel_last=True, \n",
    "                                              transpose_outputs=False)\n",
    "valid = sync_quant_config(transformer_quant_config, config, state)\n",
    "print(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 24,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 8, 'int_bitwidth': 1}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'input': {'bitwidth': 24, 'int_bitwidth': 5},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'weight': {'bitwidth': 8, 'int_bitwidth': 0}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 3,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 8, 'int_bitwidth': 1}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 7,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'input': {'bitwidth': 24, 'int_bitwidth': 9},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 7,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 1: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 24,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'weight': {'bitwidth': 8, 'int_bitwidth': 0}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -2},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 8, 'int_bitwidth': 1}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 6,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'input': {'bitwidth': 24, 'int_bitwidth': 9},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 6,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 2: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 24,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 24, 'int_bitwidth': 3},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'weight': {'bitwidth': 8, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -2},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 5,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 24, 'int_bitwidth': 6},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 8, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 5,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 3: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 24,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 0},\n",
      "                          'input': {'bitwidth': 24, 'int_bitwidth': 3},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'weight': {'bitwidth': 8, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 6,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 24, 'int_bitwidth': 8},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 6,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "     'topk': {'input': {'bitwidth': 18, 'int_bitwidth': 5}}},\n",
      " 4: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 24,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 24, 'int_bitwidth': 3},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'weight': {'bitwidth': 8, 'int_bitwidth': 0}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 5,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 24, 'int_bitwidth': 7},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 8, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 5,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 5: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 24,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'weight': {'bitwidth': 8, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 0,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 6,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 24, 'int_bitwidth': 7},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 8, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 6,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 6: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 24,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'weight': {'bitwidth': 8, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 6,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 24, 'int_bitwidth': 7},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 6,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "     'topk': {'input': {'bitwidth': 18, 'int_bitwidth': 5}}},\n",
      " 7: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 24,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 8, 'int_bitwidth': -1}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                          'input': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                          'weight': {'bitwidth': 8, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 1,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 5,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 24, 'int_bitwidth': 6},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 8, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 5,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 8: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 24,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'input': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'weight': {'bitwidth': 8, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 2,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 6,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 0},\n",
      "                                'input': {'bitwidth': 24, 'int_bitwidth': 7},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 8, 'int_bitwidth': -1}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 6,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 9: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                         'input': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': -inf,\n",
      "                                   'quantize': False},\n",
      "                         'output': {'bitwidth': 24,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                         'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "             'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'input': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                          'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                          'weight': {'bitwidth': 8, 'int_bitwidth': 1}}},\n",
      "     'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "     'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "               'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "               'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "               'output': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "               'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "               'var_input': {'bitwidth': 12,\n",
      "                             'int_bitwidth': 2,\n",
      "                             'signed': False},\n",
      "               'var_output': {'bitwidth': 18,\n",
      "                              'int_bitwidth': 1,\n",
      "                              'signed': False}},\n",
      "     'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                   'exp_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 8,\n",
      "                                  'signed': False},\n",
      "                   'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                               'input': {'bitwidth': 18,\n",
      "                                         'int_bitwidth': -inf,\n",
      "                                         'quantize': False},\n",
      "                               'output': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                               'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                   'inv_input': {'bitwidth': 12,\n",
      "                                 'int_bitwidth': 6,\n",
      "                                 'signed': False},\n",
      "                   'inv_output': {'bitwidth': 18,\n",
      "                                  'int_bitwidth': 0,\n",
      "                                  'signed': False},\n",
      "                   'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                'input': {'bitwidth': 24, 'int_bitwidth': 7},\n",
      "                                'output': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                   'row_sum': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 6,\n",
      "                               'signed': False},\n",
      "                   'scale': {'bitwidth': 18, 'int_bitwidth': 0}},\n",
      "     'topk': {'input': {'bitwidth': 18, 'int_bitwidth': 6}}},\n",
      " 10: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'input': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                          'output': {'bitwidth': 24,\n",
      "                                     'int_bitwidth': -inf,\n",
      "                                     'quantize': False},\n",
      "                          'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "              'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                           'input': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                           'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                           'weight': {'bitwidth': 8, 'int_bitwidth': 1}}},\n",
      "      'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "      'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 2,\n",
      "                               'signed': False}},\n",
      "      'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 2,\n",
      "                               'signed': False}},\n",
      "      'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                    'exp_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'signed': False},\n",
      "                    'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'input': {'bitwidth': 18,\n",
      "                                          'int_bitwidth': -inf,\n",
      "                                          'quantize': False},\n",
      "                                'output': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                                'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                    'inv_input': {'bitwidth': 12,\n",
      "                                  'int_bitwidth': 5,\n",
      "                                  'signed': False},\n",
      "                    'inv_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 0,\n",
      "                                   'signed': False},\n",
      "                    'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                                 'input': {'bitwidth': 24, 'int_bitwidth': 6},\n",
      "                                 'output': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                 'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                    'row_sum': {'bitwidth': 18,\n",
      "                                'int_bitwidth': 5,\n",
      "                                'signed': False},\n",
      "                    'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 11: {'ffn': {'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                          'input': {'bitwidth': 18,\n",
      "                                    'int_bitwidth': -inf,\n",
      "                                    'quantize': False},\n",
      "                          'output': {'bitwidth': 24,\n",
      "                                     'int_bitwidth': -inf,\n",
      "                                     'quantize': False},\n",
      "                          'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "              'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                           'input': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                           'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                           'weight': {'bitwidth': 8, 'int_bitwidth': 1}}},\n",
      "      'input': {'bitwidth': 18, 'int_bitwidth': 8},\n",
      "      'norm1': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 2,\n",
      "                               'signed': False}},\n",
      "      'norm2': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "                'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "                'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "                'scale': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "                'var_input': {'bitwidth': 12,\n",
      "                              'int_bitwidth': 3,\n",
      "                              'signed': False},\n",
      "                'var_output': {'bitwidth': 18,\n",
      "                               'int_bitwidth': 2,\n",
      "                               'signed': False}},\n",
      "      'self_attn': {'exp_input': {'bitwidth': 12, 'int_bitwidth': 4},\n",
      "                    'exp_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 8,\n",
      "                                   'signed': False},\n",
      "                    'in_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "                                'input': {'bitwidth': 18,\n",
      "                                          'int_bitwidth': -inf,\n",
      "                                          'quantize': False},\n",
      "                                'output': {'bitwidth': 24, 'int_bitwidth': 4},\n",
      "                                'weight': {'bitwidth': 8, 'int_bitwidth': 0}},\n",
      "                    'inv_input': {'bitwidth': 12,\n",
      "                                  'int_bitwidth': 5,\n",
      "                                  'signed': False},\n",
      "                    'inv_output': {'bitwidth': 18,\n",
      "                                   'int_bitwidth': 0,\n",
      "                                   'signed': False},\n",
      "                    'out_proj': {'bias': {'bitwidth': 18, 'int_bitwidth': 1},\n",
      "                                 'input': {'bitwidth': 24, 'int_bitwidth': 6},\n",
      "                                 'output': {'bitwidth': 18, 'int_bitwidth': 4},\n",
      "                                 'weight': {'bitwidth': 8, 'int_bitwidth': 1}},\n",
      "                    'row_sum': {'bitwidth': 18,\n",
      "                                'int_bitwidth': 5,\n",
      "                                'signed': False},\n",
      "                    'scale': {'bitwidth': 18, 'int_bitwidth': 0}}},\n",
      " 'norm': {'bias': {'bitwidth': 18, 'int_bitwidth': 2},\n",
      "          'input': {'bitwidth': 18, 'int_bitwidth': 6},\n",
      "          'mean': {'bitwidth': 18, 'int_bitwidth': -1},\n",
      "          'output': {'bitwidth': 18, 'int_bitwidth': 5},\n",
      "          'scale': {'bitwidth': 18, 'int_bitwidth': 3},\n",
      "          'var_input': {'bitwidth': 12, 'int_bitwidth': 3, 'signed': False},\n",
      "          'var_output': {'bitwidth': 18, 'int_bitwidth': 2, 'signed': False}}}\n"
     ]
    }
   ],
   "source": [
    "pprint(transformer_quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LayerName': {'layers_0_add1': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_0_add2': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_0_ffn': {'CdfTableRange': 4,\n",
      "                                'CdfTableSize': 4096,\n",
      "                                'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                              'cdf_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                              'hidden': 'ap_fixed<24,5,AP_RND_CONV>',\n",
      "                                              'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                              'in_proj_weight': 'ap_fixed<8,1,AP_RND_CONV>',\n",
      "                                              'out_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                              'out_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                              'result': 'ap_fixed<18,4,AP_RND_CONV>'},\n",
      "                                'ReuseFactor': 1,\n",
      "                                'TilingFactor': [1, 1, 1],\n",
      "                                'Trace': False},\n",
      "               'layers_0_norm1': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,5,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,1,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 8,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_0_norm2': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,5,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 4,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_0_self_attn': {'ExpTableRange': 8,\n",
      "                                      'ExpTableSize': 4096,\n",
      "                                      'InvTableRange': 128,\n",
      "                                      'InvTableSize': 4096,\n",
      "                                      'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                    'exp_table': 'ufixed<18,8,RND_CONV,SAT,0>',\n",
      "                                                    'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'in_proj_out': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                                    'in_proj_weight': 'ap_fixed<8,1,AP_RND_CONV>',\n",
      "                                                    'inv_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                                    'out_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'out_proj_in': 'ap_fixed<24,9,AP_RND_CONV>',\n",
      "                                                    'out_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                    'result': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'row_sum': 'ap_ufixed<18,7,AP_RND_CONV>',\n",
      "                                                    'scale': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>'},\n",
      "                                      'ReuseFactor': 1,\n",
      "                                      'TilingFactor': [1, 1, 1],\n",
      "                                      'Trace': False},\n",
      "               'layers_10_add1': {'Precision': {'result': 'ap_fixed<18,6,AP_RND_CONV>'},\n",
      "                                  'Trace': False},\n",
      "               'layers_10_add2': {'Precision': {'result': 'ap_fixed<18,6,AP_RND_CONV>'},\n",
      "                                  'Trace': False},\n",
      "               'layers_10_ffn': {'CdfTableRange': 4,\n",
      "                                 'CdfTableSize': 4096,\n",
      "                                 'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                               'cdf_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                               'hidden': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                               'in_proj_bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                               'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                               'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                               'out_proj_weight': 'ap_fixed<8,1,AP_RND_CONV>',\n",
      "                                               'result': 'ap_fixed<18,4,AP_RND_CONV>'},\n",
      "                                 'ReuseFactor': 1,\n",
      "                                 'TilingFactor': [1, 1, 1],\n",
      "                                 'Trace': False},\n",
      "               'layers_10_norm1': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                 'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                 'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                 'result': 'ap_fixed<18,4,AP_RND_CONV>',\n",
      "                                                 'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                 'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                 'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                 'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                   'ReuseFactor': 1,\n",
      "                                   'TilingFactor': [1, 1, 1],\n",
      "                                   'Trace': False,\n",
      "                                   'VarTableRange': 8,\n",
      "                                   'VarTableSize': 4096},\n",
      "               'layers_10_norm2': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                 'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                 'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                 'result': 'ap_fixed<18,6,AP_RND_CONV>',\n",
      "                                                 'scale': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                 'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                 'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                 'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                   'ReuseFactor': 1,\n",
      "                                   'TilingFactor': [1, 1, 1],\n",
      "                                   'Trace': False,\n",
      "                                   'VarTableRange': 8,\n",
      "                                   'VarTableSize': 4096},\n",
      "               'layers_10_self_attn': {'ExpTableRange': 8,\n",
      "                                       'ExpTableSize': 4096,\n",
      "                                       'InvTableRange': 32,\n",
      "                                       'InvTableSize': 4096,\n",
      "                                       'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                     'exp_table': 'ufixed<18,8,RND_CONV,SAT,0>',\n",
      "                                                     'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                     'in_proj_out': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                                     'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                     'inv_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                                     'out_proj_bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                     'out_proj_in': 'ap_fixed<24,6,AP_RND_CONV>',\n",
      "                                                     'out_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                     'result': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                     'row_sum': 'ap_ufixed<18,5,AP_RND_CONV>',\n",
      "                                                     'scale': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>'},\n",
      "                                       'ReuseFactor': 1,\n",
      "                                       'TilingFactor': [1, 1, 1],\n",
      "                                       'Trace': False},\n",
      "               'layers_11_add1': {'Precision': {'result': 'ap_fixed<18,6,AP_RND_CONV>'},\n",
      "                                  'Trace': False},\n",
      "               'layers_11_add2': {'Precision': {'result': 'ap_fixed<18,6,AP_RND_CONV>'},\n",
      "                                  'Trace': False},\n",
      "               'layers_11_ffn': {'CdfTableRange': 4,\n",
      "                                 'CdfTableSize': 4096,\n",
      "                                 'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                               'cdf_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                               'hidden': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                               'in_proj_bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                               'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                               'out_proj_bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                               'out_proj_weight': 'ap_fixed<8,1,AP_RND_CONV>',\n",
      "                                               'result': 'ap_fixed<18,4,AP_RND_CONV>'},\n",
      "                                 'ReuseFactor': 1,\n",
      "                                 'TilingFactor': [1, 1, 1],\n",
      "                                 'Trace': False},\n",
      "               'layers_11_norm1': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                 'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                 'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                 'result': 'ap_fixed<18,4,AP_RND_CONV>',\n",
      "                                                 'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                 'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                 'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                 'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                   'ReuseFactor': 1,\n",
      "                                   'TilingFactor': [1, 1, 1],\n",
      "                                   'Trace': False,\n",
      "                                   'VarTableRange': 8,\n",
      "                                   'VarTableSize': 4096},\n",
      "               'layers_11_norm2': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                 'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                 'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                 'result': 'ap_fixed<18,5,AP_RND_CONV>',\n",
      "                                                 'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                 'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                 'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                 'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                   'ReuseFactor': 1,\n",
      "                                   'TilingFactor': [1, 1, 1],\n",
      "                                   'Trace': False,\n",
      "                                   'VarTableRange': 8,\n",
      "                                   'VarTableSize': 4096},\n",
      "               'layers_11_self_attn': {'ExpTableRange': 8,\n",
      "                                       'ExpTableSize': 4096,\n",
      "                                       'InvTableRange': 32,\n",
      "                                       'InvTableSize': 4096,\n",
      "                                       'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                     'exp_table': 'ufixed<18,8,RND_CONV,SAT,0>',\n",
      "                                                     'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                     'in_proj_out': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                                     'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                     'inv_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                                     'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                                     'out_proj_in': 'ap_fixed<24,6,AP_RND_CONV>',\n",
      "                                                     'out_proj_weight': 'ap_fixed<8,1,AP_RND_CONV>',\n",
      "                                                     'result': 'ap_fixed<18,4,AP_RND_CONV>',\n",
      "                                                     'row_sum': 'ap_ufixed<18,5,AP_RND_CONV>',\n",
      "                                                     'scale': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>'},\n",
      "                                       'ReuseFactor': 1,\n",
      "                                       'TilingFactor': [1, 1, 1],\n",
      "                                       'Trace': False},\n",
      "               'layers_1_add1': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_1_add2': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_1_ffn': {'CdfTableRange': 4,\n",
      "                                'CdfTableSize': 4096,\n",
      "                                'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                              'cdf_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                              'hidden': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                              'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                              'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                              'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                              'out_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                              'result': 'ap_fixed<18,3,AP_RND_CONV>'},\n",
      "                                'ReuseFactor': 1,\n",
      "                                'TilingFactor': [1, 1, 1],\n",
      "                                'Trace': False},\n",
      "               'layers_1_norm1': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-2,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,4,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,1,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 4,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_1_norm2': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,5,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 2,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_1_self_attn': {'ExpTableRange': 8,\n",
      "                                      'ExpTableSize': 4096,\n",
      "                                      'InvTableRange': 64,\n",
      "                                      'InvTableSize': 4096,\n",
      "                                      'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                    'exp_table': 'ufixed<18,8,RND_CONV,SAT,0>',\n",
      "                                                    'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'in_proj_out': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                                    'in_proj_weight': 'ap_fixed<8,1,AP_RND_CONV>',\n",
      "                                                    'inv_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                                    'out_proj_bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                    'out_proj_in': 'ap_fixed<24,9,AP_RND_CONV>',\n",
      "                                                    'out_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                    'result': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'row_sum': 'ap_ufixed<18,6,AP_RND_CONV>',\n",
      "                                                    'scale': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>'},\n",
      "                                      'ReuseFactor': 1,\n",
      "                                      'TilingFactor': [1, 1, 1],\n",
      "                                      'Trace': False},\n",
      "               'layers_2_add1': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_2_add2': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_2_ffn': {'CdfTableRange': 4,\n",
      "                                'CdfTableSize': 4096,\n",
      "                                'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                              'cdf_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                              'hidden': 'ap_fixed<24,3,AP_RND_CONV>',\n",
      "                                              'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                              'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                              'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                              'out_proj_weight': 'ap_fixed<8,1,AP_RND_CONV>',\n",
      "                                              'result': 'ap_fixed<18,2,AP_RND_CONV>'},\n",
      "                                'ReuseFactor': 1,\n",
      "                                'TilingFactor': [1, 1, 1],\n",
      "                                'Trace': False},\n",
      "               'layers_2_norm1': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-2,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,5,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 1,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_2_norm2': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,4,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 2,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_2_self_attn': {'ExpTableRange': 8,\n",
      "                                      'ExpTableSize': 4096,\n",
      "                                      'InvTableRange': 32,\n",
      "                                      'InvTableSize': 4096,\n",
      "                                      'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                    'exp_table': 'ufixed<18,8,RND_CONV,SAT,0>',\n",
      "                                                    'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'in_proj_out': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                                    'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                    'inv_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                                    'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                                    'out_proj_in': 'ap_fixed<24,6,AP_RND_CONV>',\n",
      "                                                    'out_proj_weight': 'ap_fixed<8,-1,AP_RND_CONV>',\n",
      "                                                    'result': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                    'row_sum': 'ap_ufixed<18,5,AP_RND_CONV>',\n",
      "                                                    'scale': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>'},\n",
      "                                      'ReuseFactor': 1,\n",
      "                                      'TilingFactor': [1, 1, 1],\n",
      "                                      'Trace': False},\n",
      "               'layers_3_add1': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_3_add2': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_3_ffn': {'CdfTableRange': 4,\n",
      "                                'CdfTableSize': 4096,\n",
      "                                'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                              'cdf_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                              'hidden': 'ap_fixed<24,3,AP_RND_CONV>',\n",
      "                                              'in_proj_bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                              'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                              'out_proj_bias': 'ap_fixed<18,0,AP_RND_CONV>',\n",
      "                                              'out_proj_weight': 'ap_fixed<8,1,AP_RND_CONV>',\n",
      "                                              'result': 'ap_fixed<18,2,AP_RND_CONV>'},\n",
      "                                'ReuseFactor': 1,\n",
      "                                'TilingFactor': [1, 1, 1],\n",
      "                                'Trace': False},\n",
      "               'layers_3_norm1': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,4,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 1,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_3_norm2': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,5,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,3,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 1,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_3_self_attn': {'ExpTableRange': 8,\n",
      "                                      'ExpTableSize': 4096,\n",
      "                                      'InvTableRange': 64,\n",
      "                                      'InvTableSize': 4096,\n",
      "                                      'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                    'exp_table': 'ufixed<18,8,RND_CONV,SAT,0>',\n",
      "                                                    'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'in_proj_out': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                                    'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                    'inv_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                                    'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                                    'out_proj_in': 'ap_fixed<24,8,AP_RND_CONV>',\n",
      "                                                    'out_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                    'result': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                    'row_sum': 'ap_ufixed<18,6,AP_RND_CONV>',\n",
      "                                                    'scale': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>'},\n",
      "                                      'ReuseFactor': 1,\n",
      "                                      'TilingFactor': [1, 1, 1],\n",
      "                                      'Trace': False},\n",
      "               'layers_3_topk': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'}},\n",
      "               'layers_4_add1': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_4_add2': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_4_ffn': {'CdfTableRange': 4,\n",
      "                                'CdfTableSize': 4096,\n",
      "                                'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                              'cdf_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                              'hidden': 'ap_fixed<24,3,AP_RND_CONV>',\n",
      "                                              'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                              'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                              'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                              'out_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                              'result': 'ap_fixed<18,2,AP_RND_CONV>'},\n",
      "                                'ReuseFactor': 1,\n",
      "                                'TilingFactor': [1, 1, 1],\n",
      "                                'Trace': False},\n",
      "               'layers_4_norm1': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,4,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,3,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 1,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_4_norm2': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,5,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 1,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_4_self_attn': {'ExpTableRange': 8,\n",
      "                                      'ExpTableSize': 4096,\n",
      "                                      'InvTableRange': 32,\n",
      "                                      'InvTableSize': 4096,\n",
      "                                      'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                    'exp_table': 'ufixed<18,8,RND_CONV,SAT,0>',\n",
      "                                                    'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'in_proj_out': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                                    'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                    'inv_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                                    'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                                    'out_proj_in': 'ap_fixed<24,7,AP_RND_CONV>',\n",
      "                                                    'out_proj_weight': 'ap_fixed<8,-1,AP_RND_CONV>',\n",
      "                                                    'result': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                    'row_sum': 'ap_ufixed<18,5,AP_RND_CONV>',\n",
      "                                                    'scale': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>'},\n",
      "                                      'ReuseFactor': 1,\n",
      "                                      'TilingFactor': [1, 1, 1],\n",
      "                                      'Trace': False},\n",
      "               'layers_5_add1': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_5_add2': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_5_ffn': {'CdfTableRange': 4,\n",
      "                                'CdfTableSize': 4096,\n",
      "                                'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                              'cdf_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                              'hidden': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                              'in_proj_bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                              'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                              'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                              'out_proj_weight': 'ap_fixed<8,1,AP_RND_CONV>',\n",
      "                                              'result': 'ap_fixed<18,3,AP_RND_CONV>'},\n",
      "                                'ReuseFactor': 1,\n",
      "                                'TilingFactor': [1, 1, 1],\n",
      "                                'Trace': False},\n",
      "               'layers_5_norm1': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,4,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 1,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_5_norm2': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,5,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 1,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_5_self_attn': {'ExpTableRange': 8,\n",
      "                                      'ExpTableSize': 4096,\n",
      "                                      'InvTableRange': 64,\n",
      "                                      'InvTableSize': 4096,\n",
      "                                      'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                    'exp_table': 'ufixed<18,8,RND_CONV,SAT,0>',\n",
      "                                                    'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'in_proj_out': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                                    'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                    'inv_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                                    'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                                    'out_proj_in': 'ap_fixed<24,7,AP_RND_CONV>',\n",
      "                                                    'out_proj_weight': 'ap_fixed<8,-1,AP_RND_CONV>',\n",
      "                                                    'result': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                    'row_sum': 'ap_ufixed<18,6,AP_RND_CONV>',\n",
      "                                                    'scale': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>'},\n",
      "                                      'ReuseFactor': 1,\n",
      "                                      'TilingFactor': [1, 1, 1],\n",
      "                                      'Trace': False},\n",
      "               'layers_6_add1': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_6_add2': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_6_ffn': {'CdfTableRange': 4,\n",
      "                                'CdfTableSize': 4096,\n",
      "                                'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                              'cdf_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                              'hidden': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                              'in_proj_bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                              'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                              'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                              'out_proj_weight': 'ap_fixed<8,1,AP_RND_CONV>',\n",
      "                                              'result': 'ap_fixed<18,4,AP_RND_CONV>'},\n",
      "                                'ReuseFactor': 1,\n",
      "                                'TilingFactor': [1, 1, 1],\n",
      "                                'Trace': False},\n",
      "               'layers_6_norm1': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,4,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 2,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_6_norm2': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,5,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 2,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_6_self_attn': {'ExpTableRange': 8,\n",
      "                                      'ExpTableSize': 4096,\n",
      "                                      'InvTableRange': 64,\n",
      "                                      'InvTableSize': 4096,\n",
      "                                      'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                    'exp_table': 'ufixed<18,8,RND_CONV,SAT,0>',\n",
      "                                                    'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'in_proj_out': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                                    'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                    'inv_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                                    'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                                    'out_proj_in': 'ap_fixed<24,7,AP_RND_CONV>',\n",
      "                                                    'out_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                    'result': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'row_sum': 'ap_ufixed<18,6,AP_RND_CONV>',\n",
      "                                                    'scale': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>'},\n",
      "                                      'ReuseFactor': 1,\n",
      "                                      'TilingFactor': [1, 1, 1],\n",
      "                                      'Trace': False},\n",
      "               'layers_6_topk': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'}},\n",
      "               'layers_7_add1': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_7_add2': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_7_ffn': {'CdfTableRange': 4,\n",
      "                                'CdfTableSize': 4096,\n",
      "                                'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                              'cdf_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                              'hidden': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                              'in_proj_bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                              'in_proj_weight': 'ap_fixed<8,-1,AP_RND_CONV>',\n",
      "                                              'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                              'out_proj_weight': 'ap_fixed<8,1,AP_RND_CONV>',\n",
      "                                              'result': 'ap_fixed<18,3,AP_RND_CONV>'},\n",
      "                                'ReuseFactor': 1,\n",
      "                                'TilingFactor': [1, 1, 1],\n",
      "                                'Trace': False},\n",
      "               'layers_7_norm1': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,4,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 2,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_7_norm2': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,5,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 2,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_7_self_attn': {'ExpTableRange': 8,\n",
      "                                      'ExpTableSize': 4096,\n",
      "                                      'InvTableRange': 32,\n",
      "                                      'InvTableSize': 4096,\n",
      "                                      'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                    'exp_table': 'ufixed<18,8,RND_CONV,SAT,0>',\n",
      "                                                    'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'in_proj_out': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                                    'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                    'inv_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                                    'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                                    'out_proj_in': 'ap_fixed<24,6,AP_RND_CONV>',\n",
      "                                                    'out_proj_weight': 'ap_fixed<8,-1,AP_RND_CONV>',\n",
      "                                                    'result': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                    'row_sum': 'ap_ufixed<18,5,AP_RND_CONV>',\n",
      "                                                    'scale': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>'},\n",
      "                                      'ReuseFactor': 1,\n",
      "                                      'TilingFactor': [1, 1, 1],\n",
      "                                      'Trace': False},\n",
      "               'layers_8_add1': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_8_add2': {'Precision': {'result': 'ap_fixed<18,6,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_8_ffn': {'CdfTableRange': 4,\n",
      "                                'CdfTableSize': 4096,\n",
      "                                'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                              'cdf_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                              'hidden': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                              'in_proj_bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                              'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                              'out_proj_bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                              'out_proj_weight': 'ap_fixed<8,1,AP_RND_CONV>',\n",
      "                                              'result': 'ap_fixed<18,4,AP_RND_CONV>'},\n",
      "                                'ReuseFactor': 1,\n",
      "                                'TilingFactor': [1, 1, 1],\n",
      "                                'Trace': False},\n",
      "               'layers_8_norm1': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,4,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 4,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_8_norm2': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,6,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 4,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_8_self_attn': {'ExpTableRange': 8,\n",
      "                                      'ExpTableSize': 4096,\n",
      "                                      'InvTableRange': 64,\n",
      "                                      'InvTableSize': 4096,\n",
      "                                      'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                    'exp_table': 'ufixed<18,8,RND_CONV,SAT,0>',\n",
      "                                                    'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'in_proj_out': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                                    'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                    'inv_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                                    'out_proj_bias': 'ap_fixed<18,0,AP_RND_CONV>',\n",
      "                                                    'out_proj_in': 'ap_fixed<24,7,AP_RND_CONV>',\n",
      "                                                    'out_proj_weight': 'ap_fixed<8,-1,AP_RND_CONV>',\n",
      "                                                    'result': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                    'row_sum': 'ap_ufixed<18,6,AP_RND_CONV>',\n",
      "                                                    'scale': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>'},\n",
      "                                      'ReuseFactor': 1,\n",
      "                                      'TilingFactor': [1, 1, 1],\n",
      "                                      'Trace': False},\n",
      "               'layers_9_add1': {'Precision': {'result': 'ap_fixed<18,6,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_9_add2': {'Precision': {'result': 'ap_fixed<18,6,AP_RND_CONV>'},\n",
      "                                 'Trace': False},\n",
      "               'layers_9_ffn': {'CdfTableRange': 4,\n",
      "                                'CdfTableSize': 4096,\n",
      "                                'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                              'cdf_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                              'hidden': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                              'in_proj_bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                              'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                              'out_proj_bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                              'out_proj_weight': 'ap_fixed<8,1,AP_RND_CONV>',\n",
      "                                              'result': 'ap_fixed<18,4,AP_RND_CONV>'},\n",
      "                                'ReuseFactor': 1,\n",
      "                                'TilingFactor': [1, 1, 1],\n",
      "                                'Trace': False},\n",
      "               'layers_9_norm1': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,4,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,1,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 4,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_9_norm2': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                                'result': 'ap_fixed<18,6,AP_RND_CONV>',\n",
      "                                                'scale': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                'var_table': 'ufixed<18,1,RND_CONV,SAT,0>'},\n",
      "                                  'ReuseFactor': 1,\n",
      "                                  'TilingFactor': [1, 1, 1],\n",
      "                                  'Trace': False,\n",
      "                                  'VarTableRange': 4,\n",
      "                                  'VarTableSize': 4096},\n",
      "               'layers_9_self_attn': {'ExpTableRange': 8,\n",
      "                                      'ExpTableSize': 4096,\n",
      "                                      'InvTableRange': 64,\n",
      "                                      'InvTableSize': 4096,\n",
      "                                      'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                                    'exp_table': 'ufixed<18,8,RND_CONV,SAT,0>',\n",
      "                                                    'in_proj_bias': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                                    'in_proj_out': 'ap_fixed<24,4,AP_RND_CONV>',\n",
      "                                                    'in_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                    'inv_table': 'ufixed<18,0,RND_CONV,SAT,0>',\n",
      "                                                    'out_proj_bias': 'ap_fixed<18,1,AP_RND_CONV>',\n",
      "                                                    'out_proj_in': 'ap_fixed<24,7,AP_RND_CONV>',\n",
      "                                                    'out_proj_weight': 'ap_fixed<8,0,AP_RND_CONV>',\n",
      "                                                    'result': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                                    'row_sum': 'ap_ufixed<18,6,AP_RND_CONV>',\n",
      "                                                    'scale': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>'},\n",
      "                                      'ReuseFactor': 1,\n",
      "                                      'TilingFactor': [1, 1, 1],\n",
      "                                      'Trace': False},\n",
      "               'layers_9_topk': {'Precision': {'result': 'ap_fixed<18,6,AP_RND_CONV>'}},\n",
      "               'norm': {'Precision': {'accum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                      'bias': 'ap_fixed<18,2,AP_RND_CONV>',\n",
      "                                      'mean': 'ap_fixed<18,-1,AP_RND_CONV>',\n",
      "                                      'result': 'ap_fixed<18,5,AP_RND_CONV>',\n",
      "                                      'scale': 'ap_fixed<18,3,AP_RND_CONV>',\n",
      "                                      'sum': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                      'sum_sqr': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "                                      'var_table': 'ufixed<18,2,RND_CONV,SAT,0>'},\n",
      "                        'ReuseFactor': 1,\n",
      "                        'TilingFactor': [1, 1, 1],\n",
      "                        'Trace': False,\n",
      "                        'VarTableRange': 8,\n",
      "                        'VarTableSize': 4096},\n",
      "               'src': {'Precision': {'result': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>'},\n",
      "                       'Trace': False}},\n",
      " 'Model': {'InputsChannelLast': True,\n",
      "           'Precision': 'ap_fixed<18,5,AP_RND_CONV,AP_SAT>',\n",
      "           'ReuseFactor': 1,\n",
      "           'Strategy': 'Latency',\n",
      "           'TilingFactor': [1, 1, 1],\n",
      "           'TraceOutput': False,\n",
      "           'TransposeOutputs': False}}\n"
     ]
    }
   ],
   "source": [
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdgfdfhfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立quantize model `qmodel` 並載入calibared和sync up後的 `transformer_quant_config`。配置HLS config中的Tile size以最大化BRAM以及硬體使用效率並產生 `hls_model` 和HLS project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enable CLC: False\n",
      "Number of CLR tokens: 0\n",
      "Reduction locations: [3, 6, 9]\n",
      "Recovery layers: []\n",
      "Keep rates: [0.7, 0.7, 0.7]\n",
      "Writing HLS project\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# args.keep_rate = [0.25]\n",
    "# args.reduction_loc = [3, 6, 9]\n",
    "\n",
    "qmodel = QTransformerEncoder([QTransformerEncoderLayer(embed_dim=192, \n",
    "                                                       num_heads=3, \n",
    "                                                       hidden_dim=768, \n",
    "                                                       activation='gelu', \n",
    "                                                       quant_config=transformer_quant_config[i], \n",
    "                                                       calibration=False, \n",
    "                                                       device='cpu',\n",
    "                                                       enable_topk=i in args.reduction_loc,\n",
    "                                                       enable_evit=enable_evit,\n",
    "                                                       enable_clc=args.clc,\n",
    "                                                       num_clr=args.num_clr) for i in range(args.model_depth)], \n",
    "                             args.model_depth, \n",
    "                             QLayerNorm(normalized_shape=192, quant_config=transformer_quant_config['norm'], calibration=False, device='cpu', ifa_head=args.ifa_head),\n",
    "                             TorchQuantizer(bitwidth=18, int_bitwidth=5, signed=True, calibration=False),\n",
    "                            #  TorchQuantizer(bitwidth=32, int_bitwidth=5, signed=True, calibration=False),\n",
    "                             args,\n",
    "                             dtype=torch.float64)\n",
    "qmodel.transfer_weights(model4hls)\n",
    "qmodel.to(torch.device('cpu'))\n",
    "qmodel.eval()\n",
    "\n",
    "for layer_config in config['LayerName'].keys():\n",
    "    if layer_config.endswith('self_attn'):\n",
    "        config['LayerName'][layer_config]['TilingFactor'] = [1,1,1]\n",
    "    elif layer_config.endswith('ffn'):\n",
    "        config['LayerName'][layer_config]['TilingFactor'] = [1,1,12]\n",
    "\n",
    "if enable_tome:\n",
    "    method = 'tome'\n",
    "elif enable_evit:\n",
    "    method = 'evit'\n",
    "else:\n",
    "    method = 'topk'\n",
    "config['Model']['Reduction'] = {\n",
    "    'reduction_loc': args.reduction_loc,    # 0-based\n",
    "    'keep_rate': args.keep_rate,              # or [0.5,0.5,0.5]\n",
    "    'use_clc': args.clc,\n",
    "    'method': method\n",
    "}\n",
    "# print('before convert:', config['Model'].get('Reduction'))\n",
    "if not args.ifa_head:\n",
    "    input_shapes = [[1, int((args.input_size/16)**2+1), 192]]  # [batch_size, num_tokens, embed_dim]\n",
    "else:\n",
    "    input_shapes = [[1, int((args.input_size/16)**2+1+args.num_clr), 192]]  # [batch_size, num_tokens, embed_dim]\n",
    "# print(input_shapes[0])\n",
    "\n",
    "hls_model = hls4ml.converters.convert_from_pytorch_model(\n",
    "                                                            model4hls,\n",
    "                                                            input_shapes,\n",
    "                                                            # output_dir='./hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_test'.format(args.input_size),\n",
    "                                                            # output_dir='./hls/deit_tiny_w8_Bdk-1_Bffn-12_{}'.format(args.input_size),\n",
    "                                                            # output_dir='./hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_{}_trained'.format(args.input_size, args.dataset_name),\n",
    "                                                            # output_dir='./hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_{}_trained_topk-{}'.format(args.input_size, args.dataset_name, args.keep_rate[0]),\n",
    "                                                            output_dir='./hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_{}_kr-{}'.format(args.input_size, method, args.keep_rate[0]),\n",
    "                                                            # output_dir='./hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_{}_kr-{}_{}_{}'.format(args.input_size, method, args.keep_rate[0], args.keep_rate[1], args.keep_rate[2]),\n",
    "                                                            # output_dir='./hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_{}_kr-{}_clc'.format(args.input_size, method, args.keep_rate[0]),\n",
    "                                                            project_name='myproject',\n",
    "                                                            backend='Vitis',\n",
    "                                                            part='xcu55c-fsvh2892-2L-e',\n",
    "                                                            #board='alveo-u55c',\n",
    "                                                            hls_config=config,\n",
    "                                                            io_type='io_tile_stream',\n",
    "                                                        )\n",
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer4HLS(\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0, inplace=False)\n",
      "        (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0, inplace=False)\n",
      "        (dropout2): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model4hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src\n",
      "transformer_encoder\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "from torch.fx import symbolic_trace\n",
    "\n",
    "traced_model = symbolic_trace(model4hls)\n",
    "\n",
    "for node in traced_model.graph.nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./hls/deit_tiny_w8_Bdk-1_Bffn-12_224_evit_kr-[0.7]\n"
     ]
    }
   ],
   "source": [
    "print('./hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_{}_kr-{}'.format(args.input_size, method, args.keep_rate))\n",
    "# print('./hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_{}_kr-{}_clc'.format(args.input_size, method, args.keep_rate[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fgshfhshf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比較`qmodel` 、 `hls_model`和`model4hls`的輸出。理論上，`qmodel` 和 `hls_model`的輸入要一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enable CLC: False\n",
      "Number of CLR tokens: 0\n",
      "Reduction locations: [3, 6, 9]\n",
      "Recovery layers: []\n",
      "Keep rates: [0.7, 0.7, 0.7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QTransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x QTransformerEncoderLayer(\n",
       "      (self_attn): QFlashMultiheadAttention(\n",
       "        (out_proj): QLinear(\n",
       "          in_features=192, out_features=192, bias=True\n",
       "          (weight_qtzr): TorchQuantizer()\n",
       "          (bias_qtzr): TorchQuantizer()\n",
       "          (input_qtzr): TorchQuantizer()\n",
       "          (output_qtzr): TorchQuantizer()\n",
       "        )\n",
       "        (in_proj): QLinear(\n",
       "          in_features=192, out_features=576, bias=True\n",
       "          (weight_qtzr): TorchQuantizer()\n",
       "          (bias_qtzr): TorchQuantizer()\n",
       "          (input_qtzr): TorchQuantizer()\n",
       "          (output_qtzr): TorchQuantizer()\n",
       "        )\n",
       "        (scale_qtzr): TorchQuantizer()\n",
       "        (row_sum_qtzr): TorchQuantizer()\n",
       "        (exp_input_qtzr): TorchQuantizer()\n",
       "        (exp_output_qtzr): TorchQuantizer()\n",
       "        (inv_input_qtzr): TorchQuantizer()\n",
       "        (inv_output_qtzr): TorchQuantizer()\n",
       "        (attn_out_qtzr): TorchQuantizer()\n",
       "      )\n",
       "      (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "      (norm1): QLayerNorm(\n",
       "        (192,), eps=1e-05, elementwise_affine=True\n",
       "        (input_qtzr): TorchQuantizer()\n",
       "        (scale_qtzr): TorchQuantizer()\n",
       "        (bias_qtzr): TorchQuantizer()\n",
       "        (output_qtzr): TorchQuantizer()\n",
       "        (mean_qtzr): TorchQuantizer()\n",
       "        (var_input_qtzr): TorchQuantizer()\n",
       "        (var_output_qtzr): TorchQuantizer()\n",
       "        (dim_qtzr): TorchQuantizer()\n",
       "      )\n",
       "      (norm2): QLayerNorm(\n",
       "        (192,), eps=1e-05, elementwise_affine=True\n",
       "        (input_qtzr): TorchQuantizer()\n",
       "        (scale_qtzr): TorchQuantizer()\n",
       "        (bias_qtzr): TorchQuantizer()\n",
       "        (output_qtzr): TorchQuantizer()\n",
       "        (mean_qtzr): TorchQuantizer()\n",
       "        (var_input_qtzr): TorchQuantizer()\n",
       "        (var_output_qtzr): TorchQuantizer()\n",
       "        (dim_qtzr): TorchQuantizer()\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.0, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (cache_qtzr): TorchQuantizer()\n",
       "      (feedforward): QFeedForward(\n",
       "        (in_proj): QLinear(\n",
       "          in_features=192, out_features=768, bias=True\n",
       "          (weight_qtzr): TorchQuantizer()\n",
       "          (bias_qtzr): TorchQuantizer()\n",
       "          (input_qtzr): TorchQuantizer()\n",
       "          (output_qtzr): TorchQuantizer()\n",
       "        )\n",
       "        (cdf_input_qtzr): TorchQuantizer()\n",
       "        (cdf_output_qtzr): TorchQuantizer()\n",
       "        (out_proj): QLinear(\n",
       "          in_features=768, out_features=192, bias=True\n",
       "          (weight_qtzr): TorchQuantizer()\n",
       "          (bias_qtzr): TorchQuantizer()\n",
       "          (input_qtzr): TorchQuantizer()\n",
       "          (output_qtzr): TorchQuantizer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): QLayerNorm(\n",
       "    (192,), eps=1e-05, elementwise_affine=True\n",
       "    (input_qtzr): TorchQuantizer()\n",
       "    (scale_qtzr): TorchQuantizer()\n",
       "    (bias_qtzr): TorchQuantizer()\n",
       "    (output_qtzr): TorchQuantizer()\n",
       "    (mean_qtzr): TorchQuantizer()\n",
       "    (var_input_qtzr): TorchQuantizer()\n",
       "    (var_output_qtzr): TorchQuantizer()\n",
       "    (dim_qtzr): TorchQuantizer()\n",
       "  )\n",
       "  (input_qtzr): TorchQuantizer()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import quantizers\n",
    "\n",
    "# 重新載入 quantizers 模組\n",
    "importlib.reload(quantizers)\n",
    "from quantizers import *\n",
    "model4hls = model4hls.to(torch.device('cuda'))\n",
    "qmodel = QTransformerEncoder([QTransformerEncoderLayer(embed_dim=192, \n",
    "                                                       num_heads=3, \n",
    "                                                       hidden_dim=768, \n",
    "                                                       activation='gelu', \n",
    "                                                       quant_config=transformer_quant_config[i], \n",
    "                                                       calibration=False, \n",
    "                                                       device='cuda',\n",
    "                                                       enable_topk=i in args.reduction_loc,\n",
    "                                                       enable_evit=enable_evit,\n",
    "                                                       enable_clc=args.clc,\n",
    "                                                       num_clr=args.num_clr) for i in range(args.model_depth)], \n",
    "                             args.model_depth, \n",
    "                             QLayerNorm(normalized_shape=192, \n",
    "                                        quant_config=transformer_quant_config['norm'], \n",
    "                                        calibration=False, \n",
    "                                        device='cuda', ifa_head=args.ifa_head),\n",
    "                             TorchQuantizer(bitwidth=18, int_bitwidth=5, signed=True, calibration=False),\n",
    "                             dtype=torch.float64,\n",
    "                             args=args)\n",
    "qmodel.transfer_weights(model4hls)\n",
    "qmodel.to(torch.device('cuda'))\n",
    "qmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double check input shape of encoders = torch.Size([1, 197, 192])\n",
      "tensor([ 0.043638907372952, -0.071037769317627, -1.415930986404419],\n",
      "       device='cuda:0')\n",
      "tensor([-0.572836697101593, -0.362583488225937, -0.766521215438843],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 72, 192])\n",
      "-----------------------------------------\n",
      "input:  tensor([ 0.043579101562500, -0.071044921875000, -1.415893554687500],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "tensor([103, 150, 188, 195,  17,  49,  13,  45,  46,  48, 182, 136, 187,  86,\n",
      "         31,  44,  72,   0, 184, 183,  50, 104, 189, 126,  78,  92, 186, 100,\n",
      "        118,  75,  19,  18, 122, 131,  91,  35, 105,  61,  77,  58,  33,   5,\n",
      "         34,  32,  60, 117, 185,  98,  63,  62, 140,  89, 133,  47, 171,   6,\n",
      "         30,  93,  27, 121, 154, 174, 159, 160,  59, 167,   4, 106, 119, 181,\n",
      "        128, 190, 172, 135,  70,   7, 168, 158,  76, 120, 173, 107,  83, 146,\n",
      "        157, 170, 145,  64,  56, 114,  20, 111,  12,  74,  55,   2, 108,  11,\n",
      "        144, 194,  90, 142, 112, 132,  21, 169,  88, 123,   1,  14, 164,   3,\n",
      "         10,  71, 137, 129,  84, 143, 156,  41, 115,  87,  22,  57,  85, 139,\n",
      "         69,  97,  79, 102,   9,  73, 191, 127, 125,   8, 151,  36],\n",
      "       device='cuda:0')\n",
      "tensor([132,  43,  66, 108,  73,  55, 116,  42,  35,  31,  33,  32,  74, 127,\n",
      "         45,  99,  88,  18,  22,  84,  24, 109, 128,  15,  19,   5,  83,  54,\n",
      "         98,  36,  23,  34,  27,  87,  25,  77,  64,  44,  26,  53,  67,  85,\n",
      "         56,  75,   4,  30,   6, 119,  41,  65,  68, 107,  16, 100,  17,  39,\n",
      "         95, 137,  76,  46,  86,  78,  52, 131, 133, 130,  96, 115, 122,  13,\n",
      "         60,  71, 111,  51, 121,  63,  21, 120,  40, 124,  12, 103,  72, 114,\n",
      "        123,  49,  61, 129,  50, 134, 136, 106, 126,  11,  92, 104,  62,   0],\n",
      "       device='cuda:0')\n",
      "tensor([56, 55, 62, 57, 69, 63, 30, 68, 20, 89, 22, 15, 48, 49, 53,  8, 19, 75,\n",
      "        14, 32, 29, 38, 27, 88, 82, 36, 28, 66,  9, 92, 93, 25, 64, 40, 77, 59,\n",
      "        45, 21, 16, 41, 17, 46, 44,  7, 85, 52, 35, 10, 23, 61, 78, 71, 91, 51,\n",
      "        43, 73, 90, 83, 31, 72, 37, 18, 84, 60, 54, 39,  2, 96, 13, 24],\n",
      "       device='cuda:0')\n",
      "tensor([-0.774414062500000,  0.019653320312500, -0.800170898437500],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "torch.Size([1, 72, 192])\n",
      "[-0.55737305 -0.24267578 -1.0650635 ]\n",
      "(13824,)\n",
      "Sum of difference between qmodel output and pytorch output = 22195.17733219135\n",
      "Max difference between qmodel output and pytorch output = 10.044634819030762\n",
      "Sum of difference between qmodel output and hls_output = 17536.27392578125\n"
     ]
    }
   ],
   "source": [
    "if enable_evit:\n",
    "    method = 'evit'\n",
    "else:\n",
    "    method = 'topk'\n",
    "\n",
    "images, target = next(iter(test_loader))\n",
    "args.device = 'cuda'\n",
    "images = images.to(args.device)\n",
    "target = target.to(args.device)\n",
    "model = model.to(args.device)\n",
    "with torch.no_grad():\n",
    "    x = model.patch_embed(images[0:1])\n",
    "    x = model._pos_embed(x)\n",
    "    x = model.patch_drop(x)\n",
    "    x = model.norm_pre(x)\n",
    "    # print(\"Test data stats:\")  \n",
    "    # print(f\"  mean: {x.mean()}, std: {x.std()}\")\n",
    "    print('Double check input shape of encoders = {}'.format(x.shape))\n",
    "    # print(x.shape) \n",
    "    print(x[0][0][0:3])\n",
    "    out = x\n",
    "\n",
    "    cls_list = [] if model.ifa_head is not False else None\n",
    "    cross_layer_cache = [] if model.clc else None\n",
    "    if hasattr(model, 'clr') and not model.clc_pool_clr:\n",
    "        curr_num_pool = out.shape[1] - model.num_clr\n",
    "    else:\n",
    "        curr_num_pool = out.shape[1]\n",
    "    for i, blk in enumerate(model.blocks):\n",
    "        # print('Processing block {}'.format(i))\n",
    "        if enable_evit:\n",
    "            out, left_token, sample_idx, compl = blk(out) # for evit\n",
    "        else:\n",
    "            out, left_token, sample_idx = blk(out) # for topk\n",
    "        # if sample_idx is not None:\n",
    "        #     print('Pytorch sample_idx: ', sample_idx[sample_idx != -1])\n",
    "        # cross layer post reduction aggregation\n",
    "        if model.clc and (i in model.recovery_layers):\n",
    "            curr_num_pool = out.shape[1]\n",
    "            prev_feats = torch.cat(cross_layer_cache, dim=1)\n",
    "            out = torch.cat([out, prev_feats], dim=1)\n",
    "            cross_layer_cache = []\n",
    "\n",
    "        # add relevant tokens act as carriers to cross layer caches\n",
    "        if model.clc and (i < model.recovery_layers[-1]):\n",
    "            cross_layer_carriers = []\n",
    "\n",
    "            if model.clc_include_gap:\n",
    "                if model.clc_pool_cls:\n",
    "                    feats_to_pool = out[:, :curr_num_pool]\n",
    "                else:\n",
    "                    feats_to_pool = out[:, 1:curr_num_pool]\n",
    "                    \n",
    "                gap = reduce(feats_to_pool, 'b s d -> b 1 d', 'mean')\n",
    "                cross_layer_carriers.append(gap)\n",
    "\n",
    "            if hasattr(model, 'clr'):\n",
    "                cross_layer_carriers.append(out[:, -model.num_clr:])\n",
    "\n",
    "            cross_layer_carriers = torch.cat(cross_layer_carriers, dim=1)\n",
    "            cross_layer_cache.append(cross_layer_carriers)\n",
    "\n",
    "        # add cls token / pooled tokens to the lists\n",
    "        if cls_list is not None and (i in model.reduction_loc or i == len(model.blocks) - 1):\n",
    "            cls_list.append(out[:, 0])\n",
    "\n",
    "        # print('Layer 1 output: ', output[0][0][0:3])\n",
    "            \n",
    "    if not args.ifa_head:\n",
    "        out = model.norm(out)\n",
    "    pytorch_out = out\n",
    "    print(pytorch_out[0][0][0:3])\n",
    "    print(pytorch_out.shape)\n",
    "    print('-----------------------------------------')\n",
    "    # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_test/tb_data/tb_input_features.dat'.format(args.input_size), x.cpu().numpy().flatten().reshape(1, -1), fmt=\"%.12f\", delimiter=\" \")\n",
    "    # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}/tb_data/tb_input_features.dat'.format(args.input_size), x.numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_pruning_kr-{}/tb_data/tb_input_features.dat'.format(args.input_size, args.keep_rate[0]), x.numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_{}_kr-{}/tb_data/tb_input_features.dat'.format(args.input_size, method, args.keep_rate[0]), x.cpu().numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "\n",
    "    q_output = qmodel(x.permute(1, 0, 2).type(torch.float64))\n",
    "    q_output = q_output.permute(1, 0, 2)\n",
    "    print(q_output[0][0][0:3])\n",
    "    print(q_output.shape)\n",
    "\n",
    "    # encoder_out2 = model4hls(x.permute(1, 0, 2))\n",
    "    # encoder_out2 = encoder_out2.permute(1, 0, 2)\n",
    "    # print(encoder_out2)\n",
    "    # print(encoder_out2.shape)\n",
    "\n",
    "    hls_output = hls_model.predict(x.cpu().numpy())\n",
    "    print(hls_output[0:3])\n",
    "    print(hls_output.shape)\n",
    "\n",
    "    # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_test/tb_data/tb_output_predictions.dat'.format(args.input_size), out.cpu().numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}/tb_data/tb_output_predictions.dat'.format(args.input_size), encoder_out2.numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_{}_kr-{}/tb_data/tb_output_predictions.dat'.format(args.input_size, method, args.keep_rate[0]), pytorch_out.cpu().numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    # print(hls_output.shape)\n",
    "    # test_output1 = hls_output - encoder_out2.cpu().flatten().numpy()\n",
    "    print('Sum of difference between qmodel output and pytorch output = {}'.format(np.abs(q_output.cpu().flatten().numpy() - pytorch_out.cpu().flatten().numpy()).sum()))\n",
    "    print('Max difference between qmodel output and pytorch output = {}'.format(np.abs(q_output.cpu().flatten().numpy() - pytorch_out.cpu().flatten().numpy()).max()))\n",
    "    # print('Sum of difference between pytorch output and hls_output = {}'.format((out.cpu().flatten().numpy() - hls_output).sum()))\n",
    "    print('Sum of difference between qmodel output and hls_output = {}'.format(np.abs((q_output.cpu().flatten().numpy() - hls_output)).sum()))\n",
    "    # test_output1_sum = np.sum(test_output1)\n",
    "    # print('Sum of difference between pytorch output and hls_output = {}'.format(test_output1_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 相關性分析 ===\n",
      "Pearson 相關係數: 0.430170\n",
      "餘弦相似度: 0.430696\n",
      "均方誤差 (MSE): 4.276929e+00\n",
      "平均絕對誤差 (MAE): 1.605554e+00\n",
      "最大絕對誤差: 1.004463e+01\n",
      "平均相對誤差: 5.196208\n"
     ]
    }
   ],
   "source": [
    "# print(np.sum(np.abs(output.cpu().flatten().numpy() - encoder_out2.cpu().flatten().numpy())))\n",
    "\n",
    "# 轉換為 numpy\n",
    "output_np = q_output.cpu().numpy().flatten()\n",
    "# output_np = pytorch_out.cpu().numpy().flatten()\n",
    "encoder_out2_np = pytorch_out.cpu().numpy().flatten()\n",
    "# encoder_out2_np = hls_output\n",
    "\n",
    "# 計算各種相關性指標\n",
    "print(\"\\n=== 相關性分析 ===\")\n",
    "\n",
    "# 1. Pearson 相關係數\n",
    "correlation_matrix = np.corrcoef(output_np, encoder_out2_np)\n",
    "pearson_corr = correlation_matrix[0, 1]\n",
    "print(f\"Pearson 相關係數: {pearson_corr:.6f}\")\n",
    "\n",
    "# 2. 餘弦相似度\n",
    "cos_sim = np.dot(output_np, encoder_out2_np) / (np.linalg.norm(output_np) * np.linalg.norm(encoder_out2_np))\n",
    "print(f\"餘弦相似度: {cos_sim:.6f}\")\n",
    "\n",
    "# 3. 均方誤差 (MSE)\n",
    "mse = np.mean((output_np - encoder_out2_np)**2)\n",
    "print(f\"均方誤差 (MSE): {mse:.6e}\")\n",
    "\n",
    "# 4. 平均絕對誤差 (MAE)\n",
    "mae = np.mean(np.abs(output_np - encoder_out2_np))\n",
    "print(f\"平均絕對誤差 (MAE): {mae:.6e}\")\n",
    "\n",
    "# 5. 最大絕對誤差\n",
    "max_error = np.max(np.abs(output_np - encoder_out2_np))\n",
    "print(f\"最大絕對誤差: {max_error:.6e}\")\n",
    "\n",
    "# 6. 相對誤差 (如果沒有零值)\n",
    "if not np.any(encoder_out2_np == 0):\n",
    "    relative_error = np.mean(np.abs((output_np - encoder_out2_np) / encoder_out2_np))\n",
    "    print(f\"平均相對誤差: {relative_error:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gdsdsfdsf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgdsdsfdsf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gdsdsfdsf' is not defined"
     ]
    }
   ],
   "source": [
    "gdsdsfdsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, target = next(iter(test_loader))\n",
    "images = images.cuda()\n",
    "with torch.no_grad():\n",
    "    x = model(images[0:1])\n",
    "    # x = model.patch_embed(images[0:1])\n",
    "    # x = model._pos_embed(x)\n",
    "    # x = model.patch_drop(x)\n",
    "    # x = model.norm_pre(x)\n",
    "\n",
    "    # if model.viz_mode:\n",
    "    #         decisions = {}\n",
    "    #         features = {}\n",
    "\n",
    "    # cls_list = [] if model.ifa_head is not False else None\n",
    "    # cross_layer_cache = [] if model.clc else None\n",
    "    # if hasattr(model, 'clr') and not model.clc_pool_clr:\n",
    "    #     curr_num_pool = x.shape[1] - model.num_clr\n",
    "    # else:\n",
    "    #     curr_num_pool = x.shape[1]\n",
    "\n",
    "    # print('Double check input shape of encoders = {}'.format(x.shape))\n",
    "    # print(x.shape)  \n",
    "    # print(x)     \n",
    "    # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_test/tb_data/tb_input_features.dat'.format(args.input_size), x.cpu().numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")  \n",
    "    # # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_topk_kr-{}_clca/tb_data/tb_input_features.dat'.format(args.input_size, args.keep_rate[0]), x.numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")  \n",
    "    # for i, blk in enumerate(model.blocks):\n",
    "    #     # out, left_token, sample_idx, compl = blk(x) # for evit\n",
    "    #     x, left_token, sample_idx = blk(x) # for topk \n",
    "    #     print(i, x.shape)\n",
    "    # out = model.norm(x)\n",
    "    # # out = model(images[0:1])\n",
    "    # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_test/tb_data/tb_output_predictions.dat'.format(args.input_size), out.cpu().numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_topk_kr-{}_clca/tb_data/tb_output_predictions.dat'.format(args.input_size, args.keep_rate[0]), out.numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    # out = out[:, 0]  # class token\n",
    "    # out = model.fc_norm(out)\n",
    "    # out = model.head_drop(out)\n",
    "    # out = model.head(out)\n",
    "    # print(out.shape)\n",
    "    # print(out)\n",
    "    # csim_out = np.loadtxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_pruning/tb_data/csim_results.log'.format(args.input_size, args.keep_rate[0]))\n",
    "    # csim_out = torch.from_numpy(csim_out).float()\n",
    "    # csim_out = csim_out.view(1, 26, 192) # input size 224, keep rate 0.5, final tokens 26\n",
    "    # csim_out = csim_out[:, 0]  # class token\n",
    "    # csim_out = model.fc_norm(csim_out)\n",
    "    # csim_out = model.head_drop(csim_out)\n",
    "    # csim_out = model.head(csim_out)\n",
    "    # print(csim_out.shape)\n",
    "    # print(csim_out)\n",
    "    # np.savetxt('pytorch_out.txt', out.cpu().numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    # np.savetxt('csim_out.txt', csim_out.cpu().numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "all_outputs = []\n",
    "\n",
    "# images, target = next(iter(test_loader))\n",
    "for batch_idx, (images, target) in enumerate(test_loader):\n",
    "    with torch.no_grad():\n",
    "        for img_idx in range(images.shape[0]):  # 處理batch中的每個圖片\n",
    "            print('Processing image index: {}'.format(img_idx))\n",
    "            # 獲取patch embedding的輸出\n",
    "            x = model.patch_embed(images[img_idx:img_idx+1])\n",
    "            x = model._pos_embed(x)\n",
    "            x = model.patch_drop(x)\n",
    "            x = model.norm_pre(x)\n",
    "\n",
    "            # print('Double check input shape of encoders = {}'.format(x.shape))\n",
    "            # print(x.shape)  \n",
    "            # print(x)  \n",
    "            all_inputs.append(x.numpy().flatten())   \n",
    "            for i, blk in enumerate(model.blocks):\n",
    "                # out, left_token, sample_idx, compl = blk(x) # for evit\n",
    "                x, left_token, sample_idx = blk(x) # for topk \n",
    "            out = model.norm(x)\n",
    "            # out = model(images[0:1])\n",
    "            all_outputs.append(out.numpy().flatten())\n",
    "            # out = out[:, 0]  # class token\n",
    "            # out = model.fc_norm(out)\n",
    "            # out = model.head_drop(out)\n",
    "            # out = model.head(out)\n",
    "            # print(out.shape)\n",
    "            # print(out)\n",
    "            # csim_out = np.loadtxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_topk_kr-{}/tb_data/csim_results.log'.format(args.input_size, args.keep_rate[0]))\n",
    "            # csim_out = torch.from_numpy(csim_out).float()\n",
    "            # csim_out = csim_out.view(1, 26, 192) # input size 224, keep rate 0.5, final tokens 26\n",
    "            # csim_out = csim_out[:, 0]  # class token\n",
    "            # csim_out = model.fc_norm(csim_out)\n",
    "            # csim_out = model.head_drop(csim_out)\n",
    "            # csim_out = model.head(csim_out)\n",
    "            # print(csim_out.shape)\n",
    "            # print(csim_out)\n",
    "            # np.savetxt('pytorch_out.txt', out.cpu().numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "            # np.savetxt('csim_out.txt', csim_out.cpu().numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "\n",
    "all_inputs = np.array(all_inputs)\n",
    "all_outputs = np.array(all_outputs)\n",
    "\n",
    "np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_topk_kr-{}/tb_data/tb_input_features_all_{}.dat'.format(args.input_size, args.keep_rate[0], args.dataset_name), \n",
    "           all_inputs, fmt=\"%.6f\", delimiter=\" \")\n",
    "np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_topk_kr-{}/tb_data/tb_output_predictions_all_{}.dat'.format(args.input_size, args.keep_rate[0], args.dataset_name), \n",
    "           all_outputs, fmt=\"%.6f\", delimiter=\" \")\n",
    "\n",
    "# np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}/tb_data/tb_input_features_all_{}.dat'.format(args.input_size, args.dataset_name), \n",
    "#            all_inputs, fmt=\"%.6f\", delimiter=\" \")\n",
    "# np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}/tb_data/tb_output_predictions_all_{}.dat'.format(args.input_size, args.dataset_name), \n",
    "#            all_outputs, fmt=\"%.6f\", delimiter=\" \")\n",
    "\n",
    "print(f\"成功保存 {len(all_inputs)} 個樣本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "all_inputs = []\n",
    "all_outputs = []\n",
    "all_predictions = []  # 儲存最終預測結果\n",
    "all_targets = []      # 儲存真實標籤\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# images, target = next(iter(test_loader))\n",
    "for batch_idx, (images, target) in enumerate(test_loader):\n",
    "    images = images.to(args.device)\n",
    "    target = target.to(args.device)\n",
    "    with torch.no_grad():\n",
    "        for img_idx in range(images.shape[0]):  # 處理batch中的每個圖片\n",
    "            # print('Processing image index: {}'.format(img_idx))\n",
    "            current_target = target[img_idx].item()  # 獲取當前圖片的真實標籤\n",
    "            \n",
    "            # 獲取patch embedding的輸出\n",
    "            x = model.patch_embed(images[img_idx:img_idx+1])\n",
    "            x = model._pos_embed(x)\n",
    "            x = model.patch_drop(x)\n",
    "            x = model.norm_pre(x)\n",
    "            \n",
    "            ########################################################\n",
    "            # using pytorch model\n",
    "            # 儲存輸入特徵\n",
    "            # all_inputs.append(x.numpy().flatten())   \n",
    "            cls_list = [] if model.ifa_head is not False else None\n",
    "            cross_layer_cache = [] if model.clc else None\n",
    "            if hasattr(model, 'clr') and not model.clc_pool_clr:\n",
    "                curr_num_pool = x.shape[1] - model.num_clr\n",
    "            else:\n",
    "                curr_num_pool = x.shape[1]\n",
    "            # 通過所有 transformer blocks\n",
    "            for i, blk in enumerate(model.blocks):\n",
    "                x, left_token, sample_idx = blk(x) # for topk \n",
    "\n",
    "                # cross layer post reduction aggregation\n",
    "                if model.clc and (i in model.recovery_layers):\n",
    "                    curr_num_pool = x.shape[1]\n",
    "                    prev_feats = torch.cat(cross_layer_cache, dim=1)\n",
    "                    x = torch.cat([x, prev_feats], dim=1)\n",
    "                    cross_layer_cache = []\n",
    "\n",
    "                # add relevant tokens act as carriers to cross layer caches\n",
    "                if model.clc and (i < model.recovery_layers[-1]):\n",
    "                    cross_layer_carriers = []\n",
    "\n",
    "                    if model.clc_include_gap:\n",
    "                        if model.clc_pool_cls:\n",
    "                            feats_to_pool = x[:, :curr_num_pool]\n",
    "                        else:\n",
    "                            feats_to_pool = x[:, 1:curr_num_pool]\n",
    "                            \n",
    "                        gap = reduce(feats_to_pool, 'b s d -> b 1 d', 'mean')\n",
    "                        cross_layer_carriers.append(gap)\n",
    "\n",
    "                    if hasattr(model, 'clr'):\n",
    "                        cross_layer_carriers.append(x[:, -model.num_clr:])\n",
    "\n",
    "                    cross_layer_carriers = torch.cat(cross_layer_carriers, dim=1)\n",
    "                    cross_layer_cache.append(cross_layer_carriers)\n",
    "\n",
    "                # add cls token / pooled tokens to the lists\n",
    "                if cls_list is not None and (i in model.reduction_loc or i == len(model.blocks) - 1):\n",
    "                    cls_list.append(x[:, 0])\n",
    "            \n",
    "            # 正規化\n",
    "            out = model.norm(x)\n",
    "            # all_outputs.append(out.numpy().flatten())\n",
    "            ########################################################\n",
    "            # using q model\n",
    "            # out = model4hls(x.permute(1, 0, 2))\n",
    "            # out = qmodel(x.permute(1, 0, 2).type(torch.float64))\n",
    "            # out = out.permute(1, 0, 2)\n",
    "            # all_outputs.append(out)\n",
    "            ########################################################\n",
    "            # using hls model\n",
    "            # out = hls_model.predict(x.numpy())\n",
    "            # all_outputs.append(out)\n",
    "\n",
    "            # # 確定最終的 token 數量\n",
    "            # if args.keep_rate == []:\n",
    "            #     expected_final_tokens = 197\n",
    "            # else:\n",
    "            #     expected_final_tokens = math.ceil(args.keep_rate[0] * math.ceil(args.keep_rate[0] * math.ceil(args.keep_rate[0] * (197 - 1)))) + 1  # 根據您的設定調整\n",
    "            # expected_features = 192  # 根據模型的特徵數量\n",
    "\n",
    "            # # 重新整形 HLS 結果\n",
    "            # out = out.reshape(-1, expected_final_tokens, expected_features)\n",
    "            # out = torch.from_numpy(out).float()  # 確保是 torch tensor\n",
    "            ########################################################\n",
    "\n",
    "            if model.ifa_head:\n",
    "                # 如果是 IFA head，需要收集不同層的 cls token\n",
    "                inter_cls = torch.stack(cls_list, dim=-1)\n",
    "                logits = model.ifa_head(inter_cls)\n",
    "            else:\n",
    "                # 標準分類流程\n",
    "                if model.attn_pool is not None:\n",
    "                    cls_token = model.attn_pool(out)\n",
    "                elif model.global_pool == 'avg':\n",
    "                    cls_token = out[:, model.num_prefix_tokens:].mean(dim=1)\n",
    "                elif model.global_pool:\n",
    "                    cls_token = out[:, 0]  # class token\n",
    "                else:\n",
    "                    cls_token = out[:, 0]  # 預設使用 class token\n",
    "                \n",
    "                # 確保 cls_token 是 float32 類型，與模型權重匹配\n",
    "                if cls_token.dtype == torch.float64:\n",
    "                    cls_token = cls_token.float()  # 轉換為 float32\n",
    "                \n",
    "                cls_token = model.fc_norm(cls_token)\n",
    "                cls_token = model.head_drop(cls_token)\n",
    "                logits = model.head(cls_token)\n",
    "            \n",
    "            # 獲取預測結果\n",
    "            predicted = torch.argmax(logits, dim=1).item()\n",
    "            \n",
    "            # 儲存預測和真實標籤\n",
    "            all_predictions.append(predicted)\n",
    "            all_targets.append(current_target)\n",
    "            \n",
    "            # 計算準確率\n",
    "            if predicted == current_target:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "            # 每處理10張圖片就印一次準確率\n",
    "            if total % 10 == 0:\n",
    "                current_accuracy = 100 * correct / total\n",
    "                print(f'處理了 {total} 張圖片，目前準確率: {current_accuracy:.2f}%')\n",
    "\n",
    "# 計算最終準確率\n",
    "final_accuracy = 100 * correct / total\n",
    "print(f'\\n最終結果:')\n",
    "print(f'總共處理: {total} 張圖片')\n",
    "print(f'正確預測: {correct} 張')\n",
    "print(f'準確率: {final_accuracy:.2f}%')\n",
    "\n",
    "# 儲存結果\n",
    "all_inputs = np.array(all_inputs)\n",
    "all_outputs = np.array(all_outputs)\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_targets = np.array(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_inputs = []\n",
    "qmodel_all_outputs = []\n",
    "qmodel_all_predictions = []  # 儲存最終預測結果\n",
    "all_targets = []      # 儲存真實標籤\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "args.device = 'cuda'\n",
    "args.batch_size = 512\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset_val, sampler=sampler_val,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "# images, targets = next(iter(test_loader))\n",
    "for batch_idx, (images, targets) in tqdm(enumerate(test_loader), desc=\"Batch\", total=len(test_loader)):\n",
    "    images = images.to(args.device)\n",
    "    targets = targets.to(args.device)\n",
    "    batch_size = images.shape[0]\n",
    "    if batch_idx == 0:\n",
    "        print(f'Batch size: {batch_size}, Image shape: {images.shape}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print('Processing batch index: {}'.format(batch_idx))\n",
    "\n",
    "        # 獲取patch embedding的輸出\n",
    "        x = model.patch_embed(images)\n",
    "        x = model._pos_embed(x)\n",
    "        x = model.patch_drop(x)\n",
    "        x = model.norm_pre(x)\n",
    "        \n",
    "        ########################################################\n",
    "        # using pytorch model\n",
    "        # 儲存輸入特徵\n",
    "        # all_inputs.append(x.numpy().flatten())   \n",
    "        \n",
    "        # 通過所有 transformer blocks\n",
    "        # for i, blk in enumerate(model.blocks):\n",
    "        #     x, left_token, sample_idx = blk(x) # for topk \n",
    "        \n",
    "        # # 正規化\n",
    "        # out = model.norm(x)\n",
    "        # all_outputs.append(out.numpy().flatten())\n",
    "        ########################################################\n",
    "        # using q model\n",
    "        # out = model4hls(x.permute(1, 0, 2))\n",
    "        out = qmodel(x.permute(1, 0, 2).type(torch.float64))\n",
    "        out = out.permute(1, 0, 2)\n",
    "        qmodel_all_outputs.append(out)\n",
    "        ########################################################\n",
    "        # using hls model\n",
    "        # out = hls_model.predict(x.numpy())\n",
    "        # all_outputs.append(out)\n",
    "\n",
    "        # # 確定最終的 token 數量\n",
    "        # if args.keep_rate == []:\n",
    "        #     expected_final_tokens = 197\n",
    "        # else:\n",
    "        #     expected_final_tokens = math.ceil(args.keep_rate[0] * math.ceil(args.keep_rate[0] * math.ceil(args.keep_rate[0] * (197 - 1)))) + 1  # 根據您的設定調整\n",
    "        # expected_features = 192  # 根據模型的特徵數量\n",
    "\n",
    "        # # 重新整形 HLS 結果\n",
    "        # out = out.reshape(-1, expected_final_tokens, expected_features)\n",
    "        # out = torch.from_numpy(out).float()  # 確保是 torch tensor\n",
    "        ########################################################\n",
    "\n",
    "        # if model.ifa_head is not False:\n",
    "        #     # IFA head 流程\n",
    "        #     inter_cls = torch.stack(cls_list, dim=-1)\n",
    "        #     logits = model.ifa_head(inter_cls)\n",
    "        # else:\n",
    "        # 標準分類流程\n",
    "        # 如果模型沒有 norm，手動添加 LayerNorm\n",
    "        # temp_norm = nn.LayerNorm(out.shape[-1]).to(out.device).double()\n",
    "        # out = temp_norm(out)\n",
    "        \n",
    "        if model.attn_pool is not None:\n",
    "            cls_token = model.attn_pool(out)\n",
    "        elif model.global_pool == 'avg':\n",
    "            cls_token = out[:, model.num_prefix_tokens:].mean(dim=1)\n",
    "        elif model.global_pool:\n",
    "            cls_token = out[:, 0]  # class token\n",
    "        else:\n",
    "            cls_token = out[:, 0]  # 預設使用 class token\n",
    "        \n",
    "        # 確保 cls_token 是 float32 類型\n",
    "        if cls_token.dtype == torch.float64:\n",
    "            cls_token = cls_token.float()\n",
    "        \n",
    "        cls_token = model.fc_norm(cls_token)\n",
    "        cls_token = model.head_drop(cls_token)\n",
    "        logits = model.head(cls_token)\n",
    "        \n",
    "        # 獲取預測結果\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # 儲存預測和真實標籤\n",
    "        qmodel_all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        # 計算準確率\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += batch_size\n",
    "        \n",
    "        # 每處理10張圖片就印一次準確率\n",
    "        if total % 10 == 0:\n",
    "            current_accuracy = 100 * correct / total\n",
    "            print(f'處理了 {total} 張圖片，目前準確率: {current_accuracy:.2f}%')\n",
    "\n",
    "# 計算最終準確率\n",
    "final_accuracy = 100 * correct / total\n",
    "print(f'\\n最終結果:')\n",
    "print(f'總共處理: {total} 張圖片')\n",
    "print(f'正確預測: {correct} 張')\n",
    "print(f'準確率: {final_accuracy:.2f}%')\n",
    "\n",
    "# 儲存結果\n",
    "all_inputs = np.array(all_inputs)\n",
    "all_outputs = np.array(all_outputs)\n",
    "qmodel_all_predictions = np.array(qmodel_all_predictions)\n",
    "all_targets = np.array(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = min(len(qmodel_all_predictions), len(all_targets))\n",
    "# 預測一致性分析\n",
    "if len(all_predictions) == len(qmodel_all_predictions):\n",
    "    prediction_match = np.sum(all_predictions[:min_samples] == qmodel_all_predictions)\n",
    "    prediction_consistency = 100 * prediction_match / min_samples\n",
    "    print(f\"預測一致性: {prediction_consistency:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdgfhdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全面測試所有數據，不只是前10個\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_logits = []\n",
    "all_confidence_scores = []\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(\"=== 全面測試所有數據 ===\")\n",
    "\n",
    "for batch_idx, (images, target) in enumerate(test_loader):\n",
    "    print(f\"批次 {batch_idx}: 標籤 {target.tolist()}\")\n",
    "    images = images.to(args.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_idx in range(images.shape[0]):\n",
    "            current_target = target[img_idx].item()\n",
    "            \n",
    "            # 你的特徵提取程式碼\n",
    "            x = model.patch_embed(images[img_idx:img_idx+1])\n",
    "            x = model._pos_embed(x)\n",
    "            x = model.patch_drop(x)\n",
    "            x = model.norm_pre(x)\n",
    "            \n",
    "            for i, blk in enumerate(model.blocks):\n",
    "                x, left_token, sample_idx = blk(x)\n",
    "            \n",
    "            out = model.norm(x)\n",
    "            cls_token = out[:, 0]\n",
    "            \n",
    "            if hasattr(model, 'fc_norm'):\n",
    "                cls_token = model.fc_norm(cls_token)\n",
    "            if hasattr(model, 'head_drop'):\n",
    "                cls_token = model.head_drop(cls_token)\n",
    "            if hasattr(model, 'head'):\n",
    "                logits = model.head(cls_token)\n",
    "            \n",
    "            # 獲取預測和信心分數\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted = torch.argmax(logits, dim=1).item()\n",
    "            confidence = probabilities[0, predicted].item()\n",
    "            max_logit = logits[0, predicted].item()\n",
    "            \n",
    "            # 儲存結果\n",
    "            all_predictions.append(predicted)\n",
    "            all_targets.append(current_target)\n",
    "            all_logits.append(logits.cpu().numpy().flatten())\n",
    "            all_confidence_scores.append(confidence)\n",
    "            \n",
    "            # 檢查是否正確\n",
    "            is_correct = predicted == current_target\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            else:\n",
    "                print(f\"*** 錯誤預測 ***\")\n",
    "                print(f\"圖片 {total}: 真實={current_target}, 預測={predicted}\")\n",
    "                print(f\"Logits: {logits.flatten()[:10]}...\")\n",
    "                print(f\"信心分數: {confidence:.4f}\")\n",
    "            \n",
    "            total += 1\n",
    "\n",
    "print(f\"\\n=== 最終結果 ===\")\n",
    "print(f\"總共處理: {total} 張圖片\")\n",
    "print(f\"正確預測: {correct} 張\")\n",
    "print(f\"準確率: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# 轉換為 numpy 陣列\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_targets = np.array(all_targets)\n",
    "all_confidence_scores = np.array(all_confidence_scores)\n",
    "\n",
    "print(f\"\\n=== 詳細分析 ===\")\n",
    "print(f\"預測標籤範圍: {np.min(all_predictions)} ~ {np.max(all_predictions)}\")\n",
    "print(f\"真實標籤範圍: {np.min(all_targets)} ~ {np.max(all_targets)}\")\n",
    "print(f\"信心分數範圍: {np.min(all_confidence_scores):.4f} ~ {np.max(all_confidence_scores):.4f}\")\n",
    "print(f\"平均信心分數: {np.mean(all_confidence_scores):.4f}\")\n",
    "\n",
    "# 檢查每個類別的準確率\n",
    "print(f\"\\n=== 每個類別的準確率 ===\")\n",
    "unique_classes = np.unique(all_targets)\n",
    "for cls in unique_classes:\n",
    "    cls_mask = all_targets == cls\n",
    "    cls_correct = np.sum((all_predictions == all_targets) & cls_mask)\n",
    "    cls_total = np.sum(cls_mask)\n",
    "    cls_accuracy = 100 * cls_correct / cls_total if cls_total > 0 else 0\n",
    "    \n",
    "    # 該類別的信心分數\n",
    "    cls_confidence = np.mean(all_confidence_scores[cls_mask])\n",
    "    \n",
    "    print(f\"類別 {cls:2d}: {cls_correct}/{cls_total} = {cls_accuracy:6.2f}%, 平均信心: {cls_confidence:.4f}\")\n",
    "\n",
    "# 檢查混淆情況\n",
    "print(f\"\\n=== 混淆分析 ===\")\n",
    "wrong_predictions = all_predictions != all_targets\n",
    "if np.any(wrong_predictions):\n",
    "    print(\"錯誤預測的詳細情況:\")\n",
    "    for i, (pred, true) in enumerate(zip(all_predictions[wrong_predictions], all_targets[wrong_predictions])):\n",
    "        confidence = all_confidence_scores[wrong_predictions][i]\n",
    "        print(f\"  樣本: 真實={true}, 預測={pred}, 信心={confidence:.4f}\")\n",
    "else:\n",
    "    print(\"沒有錯誤預測！\")\n",
    "\n",
    "# 檢查信心分數最低的預測\n",
    "print(f\"\\n=== 信心分數最低的10個預測 ===\")\n",
    "low_confidence_indices = np.argsort(all_confidence_scores)[:10]\n",
    "for i, idx in enumerate(low_confidence_indices):\n",
    "    pred = all_predictions[idx]\n",
    "    true = all_targets[idx]\n",
    "    conf = all_confidence_scores[idx]\n",
    "    correct_mark = \"✓\" if pred == true else \"✗\"\n",
    "    print(f\"{i+1:2d}. 樣本{idx:2d}: 真實={true:2d}, 預測={pred:2d}, 信心={conf:.4f} {correct_mark}\")\n",
    "\n",
    "# 最後確認：使用完整模型測試\n",
    "print(f\"\\n=== 完整模型驗證 ===\")\n",
    "model.eval()\n",
    "correct_full = 0\n",
    "total_full = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, target in test_loader:\n",
    "        images = images.to(args.device)\n",
    "        target = target.to(args.device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_full += target.size(0)\n",
    "        correct_full += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"完整模型準確率: {100 * correct_full / total_full:.2f}%\")\n",
    "print(f\"手動實現準確率: {100 * correct / total:.2f}%\")\n",
    "print(f\"結果一致: {correct_full == correct and total_full == total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 HLS C simulation 的結果\n",
    "try:\n",
    "    # csim_results = np.loadtxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_topk_kr-{}/tb_data/csim_results_all_{}.log'.format(args.input_size, args.keep_rate[0], args.dataset_name))\n",
    "    csim_results = np.loadtxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}/tb_data/csim_results_all_{}.log'.format(args.input_size, args.dataset_name))\n",
    "    print(f\"\\nHLS結果形狀: {csim_results.shape}\")\n",
    "    \n",
    "    # 確定最終的 token 數量\n",
    "    expected_final_tokens = 197\n",
    "    # expected_final_tokens = math.ceil(args.keep_rate[0] * math.ceil(args.keep_rate[0] * math.ceil(args.keep_rate[0] * (197 - 1)))) + 1  # 根據您的設定調整\n",
    "    expected_features = 192     # DeiT-tiny 的特徵維度\n",
    "    \n",
    "    # 重新整形 HLS 結果\n",
    "    num_samples = csim_results.shape[0]\n",
    "    csim_results = csim_results.reshape(num_samples, expected_final_tokens, expected_features)\n",
    "    print(f\"重新整形後的 HLS 結果: {csim_results.shape}\")\n",
    "    \n",
    "    # 轉換為 PyTorch tensor\n",
    "    csim_tensor = torch.from_numpy(csim_results).float()\n",
    "    \n",
    "    # 提取 class token (第一個 token)\n",
    "    cls_tokens = csim_tensor[:, 0, :]  # shape: (num_samples, 192)\n",
    "    print(f\"Class tokens 形狀: {cls_tokens.shape}\")\n",
    "    \n",
    "    # 通過分類頭計算 HLS 預測\n",
    "    hls_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(cls_tokens.shape[0]):\n",
    "            cls_token = cls_tokens[i:i+1]  # shape: (1, 192)\n",
    "            \n",
    "            # 根據模型架構選擇正確的分類頭\n",
    "            if hasattr(model, 'fc_norm'):\n",
    "                cls_token = model.fc_norm(cls_token)\n",
    "            \n",
    "            if hasattr(model, 'head_drop'):\n",
    "                cls_token = model.head_drop(cls_token)\n",
    "            \n",
    "            # 分類頭\n",
    "            if hasattr(model, 'head'):\n",
    "                logits = model.head(cls_token)\n",
    "            elif hasattr(model, 'fc'):\n",
    "                logits = model.fc(cls_token)\n",
    "            else:\n",
    "                logits = cls_token\n",
    "            \n",
    "            predicted = torch.argmax(logits, dim=1).item()\n",
    "            hls_predictions.append(predicted)\n",
    "    \n",
    "    hls_predictions = np.array(hls_predictions)\n",
    "    \n",
    "    # 確保數量匹配\n",
    "    min_samples = min(len(hls_predictions), len(all_targets))\n",
    "    hls_predictions = hls_predictions[:min_samples]\n",
    "    targets_for_hls = all_targets[:min_samples]\n",
    "    \n",
    "    # 計算 HLS 準確率\n",
    "    hls_correct = np.sum(hls_predictions == targets_for_hls)\n",
    "    hls_total = len(targets_for_hls)\n",
    "    hls_accuracy = 100 * hls_correct / hls_total\n",
    "    \n",
    "    print(f\"\\n=== HLS C Simulation 準確率結果 ===\")\n",
    "    print(f\"總共處理: {hls_total} 個樣本\")\n",
    "    print(f\"正確預測: {hls_correct} 個\")\n",
    "    print(f\"HLS準確率: {hls_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"\\n=== PyTorch vs HLS 比較 ===\")\n",
    "    print(f\"PyTorch 準確率: {final_accuracy:.2f}%\")\n",
    "    print(f\"HLS 準確率: {hls_accuracy:.2f}%\")\n",
    "    print(f\"準確率差異: {abs(final_accuracy - hls_accuracy):.2f}%\")\n",
    "    \n",
    "    # 預測一致性分析\n",
    "    if len(all_predictions) == len(hls_predictions):\n",
    "        prediction_match = np.sum(all_predictions[:min_samples] == hls_predictions)\n",
    "        prediction_consistency = 100 * prediction_match / min_samples\n",
    "        print(f\"預測一致性: {prediction_consistency:.2f}%\")\n",
    "    \n",
    "    # 儲存 HLS 預測結果\n",
    "    # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_topk_kr-{}/tb_data/hls_predictions.dat'.format(args.input_size, args.keep_rate[0]), \n",
    "    #            hls_predictions, fmt=\"%d\", delimiter=\" \")\n",
    "    \n",
    "    # 儲存準確率報告\n",
    "    # with open('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_topk_kr-{}/tb_data/hls_accuracy_report.txt'.format(args.input_size, args.keep_rate[0]), 'w') as f:\n",
    "    #     f.write(f'HLS C Simulation 準確率報告\\n')\n",
    "    #     f.write(f'========================\\n')\n",
    "    #     f.write(f'PyTorch 準確率: {final_accuracy:.4f}%\\n')\n",
    "    #     f.write(f'HLS 準確率: {hls_accuracy:.4f}%\\n')\n",
    "    #     f.write(f'準確率差異: {abs(final_accuracy - hls_accuracy):.4f}%\\n')\n",
    "    #     f.write(f'預測一致性: {prediction_consistency:.4f}%\\n')\n",
    "    \n",
    "    # print(f\"\\nHLS 準確率報告已保存\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"找不到 csim_results_all.log 檔案，請先執行 HLS C simulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, target = next(iter(test_loader))\n",
    "args.device = 'cuda'\n",
    "images = images.to(args.device)\n",
    "target = target.to(args.device)\n",
    "model = model.to(args.device)\n",
    "with torch.no_grad():\n",
    "    x = model.patch_embed(images[0:1])\n",
    "    x = model._pos_embed(x)\n",
    "    x = model.patch_drop(x)\n",
    "    x = model.norm_pre(x)\n",
    "    print('Double check input shape of encoders = {}'.format(x.shape))\n",
    "    print(x.shape) \n",
    "    # print(x)\n",
    "    # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}/tb_data/tb_input_features.dat'.format(args.input_size), x.numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_pruning_kr-{}/tb_data/tb_input_features.dat'.format(args.input_size, args.keep_rate[0]), x.numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    output = qmodel(x.permute(1, 0, 2).type(torch.float64))\n",
    "    output = output.permute(1, 0, 2)\n",
    "    encoder_out2 = model4hls(x.permute(1, 0, 2))\n",
    "    encoder_out2 = encoder_out2.permute(1, 0, 2)\n",
    "    # hls_output = hls_model.predict(x.numpy())\n",
    "    # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}/tb_data/tb_output_predictions.dat'.format(args.input_size), encoder_out2.numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    # np.savetxt('hls/deit_tiny_w8_Bdk-1_Bffn-12_{}_pruning_kr-{}/tb_data/tb_output_predictions.dat'.format(args.input_size, args.keep_rate[0]), encoder_out2.numpy().flatten().reshape(1, -1), fmt=\"%.6f\", delimiter=\" \")\n",
    "    print(output)\n",
    "    print(encoder_out2)\n",
    "    # print(hls_output)\n",
    "    # test_output1 = hls_output - encoder_out2.flatten().numpy()\n",
    "    # print('Sum of difference between qmodel output and hls_output = {}'.format((output.flatten().numpy() - hls_output).sum()))\n",
    "    # test_output1_sum = np.sum(test_output1)\n",
    "    # print('Sum of difference between pytorch output and hls_output = {}'.format(test_output1_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.to(torch.device('cpu'))\n",
    "encoder_out2 = encoder_out2.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(np.abs(output.flatten().numpy() - encoder_out2.flatten().numpy())))\n",
    "\n",
    "# 轉換為 numpy\n",
    "output_np = output.numpy().flatten()\n",
    "encoder_out2_np = encoder_out2.numpy().flatten()\n",
    "\n",
    "# 計算各種相關性指標\n",
    "print(\"\\n=== 相關性分析 ===\")\n",
    "\n",
    "# 1. Pearson 相關係數\n",
    "correlation_matrix = np.corrcoef(output_np, encoder_out2_np)\n",
    "pearson_corr = correlation_matrix[0, 1]\n",
    "print(f\"Pearson 相關係數: {pearson_corr:.6f}\")\n",
    "\n",
    "# 2. 餘弦相似度\n",
    "cos_sim = np.dot(output_np, encoder_out2_np) / (np.linalg.norm(output_np) * np.linalg.norm(encoder_out2_np))\n",
    "print(f\"餘弦相似度: {cos_sim:.6f}\")\n",
    "\n",
    "# 3. 均方誤差 (MSE)\n",
    "mse = np.mean((output_np - encoder_out2_np)**2)\n",
    "print(f\"均方誤差 (MSE): {mse:.6e}\")\n",
    "\n",
    "# 4. 平均絕對誤差 (MAE)\n",
    "mae = np.mean(np.abs(output_np - encoder_out2_np))\n",
    "print(f\"平均絕對誤差 (MAE): {mae:.6e}\")\n",
    "\n",
    "# 5. 最大絕對誤差\n",
    "max_error = np.max(np.abs(output_np - encoder_out2_np))\n",
    "print(f\"最大絕對誤差: {max_error:.6e}\")\n",
    "\n",
    "# 6. 相對誤差 (如果沒有零值)\n",
    "if not np.any(encoder_out2_np == 0):\n",
    "    relative_error = np.mean(np.abs((output_np - encoder_out2_np) / encoder_out2_np))\n",
    "    print(f\"平均相對誤差: {relative_error:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_out2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測`hls_model`以及產生的HLS project的BRAM數目，這會`state`的配置與tile size的大小有關"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estimators import *\n",
    "def layer_estimater(quant_config):\n",
    "    bram_dict = {}  \n",
    "    for layer_name in quant_config.keys():\n",
    "        bram_dict[layer_name] = {}\n",
    "        for var_name in quant_config[layer_name].keys():\n",
    "            #pprint(quant_config[layer_name][var_name])\n",
    "            bram_dict[layer_name][var_name] = VivadoVariableBRAMEstimator(name=var_name,**quant_config[layer_name][var_name])\n",
    "\n",
    "    num_ram = 0\n",
    "    for layer_name in bram_dict.keys():\n",
    "        for var_name in bram_dict[layer_name].keys():\n",
    "            ram_est = bram_dict[layer_name][var_name]\n",
    "            num_ram += ram_est.get_num_ram()\n",
    "    return num_ram\n",
    "num_ram = layer_estimater(parse_hls_model(hls_model))\n",
    "print(num_ram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_hls4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
