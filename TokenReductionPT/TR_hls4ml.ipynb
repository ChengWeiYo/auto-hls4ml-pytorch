{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e81789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1180: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.vit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_tiny_patch16_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1189: UserWarning: Overwriting vit_small_patch16_224 in registry with models.vit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1198: UserWarning: Overwriting vit_small_patch8_224 in registry with models.vit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch8_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1207: UserWarning: Overwriting vit_base_patch16_224 in registry with models.vit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1217: UserWarning: Overwriting vit_base_patch8_224 in registry with models.vit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch8_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1227: UserWarning: Overwriting vit_large_patch16_224 in registry with models.vit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1237: UserWarning: Overwriting vit_large_patch14_224 in registry with models.vit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch14_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1246: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.vit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_huge_patch14_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1255: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.vit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224_miil(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1266: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.vit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_medium_patch16_gap_240(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1278: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.vit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_medium_patch16_gap_256(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1290: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.vit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_gap_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1301: UserWarning: Overwriting vit_huge_patch14_gap_224 in registry with models.vit.vit_huge_patch14_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_huge_patch14_gap_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1312: UserWarning: Overwriting vit_giant_patch16_gap_224 in registry with models.vit.vit_giant_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_giant_patch16_gap_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1324: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.vit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_clip_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1334: UserWarning: Overwriting flexivit_small in registry with models.vit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def flexivit_small(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1343: UserWarning: Overwriting flexivit_base in registry with models.vit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def flexivit_base(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1352: UserWarning: Overwriting flexivit_large in registry with models.vit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def flexivit_large(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1361: UserWarning: Overwriting vit_small_patch14_dinov2 in registry with models.vit.vit_small_patch14_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch14_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1371: UserWarning: Overwriting vit_base_patch14_dinov2 in registry with models.vit.vit_base_patch14_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch14_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1381: UserWarning: Overwriting vit_large_patch14_dinov2 in registry with models.vit.vit_large_patch14_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch14_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1391: UserWarning: Overwriting vit_giant_patch14_dinov2 in registry with models.vit.vit_giant_patch14_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_giant_patch14_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1408: UserWarning: Overwriting vit_small_patch14_reg4_dinov2 in registry with models.vit.vit_small_patch14_reg4_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch14_reg4_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1421: UserWarning: Overwriting vit_base_patch14_reg4_dinov2 in registry with models.vit.vit_base_patch14_reg4_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch14_reg4_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1434: UserWarning: Overwriting vit_large_patch14_reg4_dinov2 in registry with models.vit.vit_large_patch14_reg4_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch14_reg4_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1447: UserWarning: Overwriting vit_giant_patch14_reg4_dinov2 in registry with models.vit.vit_giant_patch14_reg4_dinov2. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_giant_patch14_reg4_dinov2(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1464: UserWarning: Overwriting vit_base_patch16_siglip_224 in registry with models.vit.vit_base_patch16_siglip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_siglip_224(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1474: UserWarning: Overwriting vit_base_patch16_siglip_256 in registry with models.vit.vit_base_patch16_siglip_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_siglip_256(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1484: UserWarning: Overwriting vit_base_patch16_siglip_512 in registry with models.vit.vit_base_patch16_siglip_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_siglip_512(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1494: UserWarning: Overwriting vit_large_patch16_siglip_256 in registry with models.vit.vit_large_patch16_siglip_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_siglip_256(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1515: UserWarning: Overwriting vit_medium_patch16_reg4_gap_256 in registry with models.vit.vit_medium_patch16_reg4_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_medium_patch16_reg4_gap_256(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1537: UserWarning: Overwriting deit_tiny_patch16_224 in registry with models.vit.deit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_tiny_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1547: UserWarning: Overwriting deit_small_patch16_224 in registry with models.vit.deit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_small_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1557: UserWarning: Overwriting deit_base_patch16_224 in registry with models.vit.deit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit_base_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1567: UserWarning: Overwriting deit3_small_patch16_224 in registry with models.vit.deit3_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit3_small_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1577: UserWarning: Overwriting deit3_medium_patch16_224 in registry with models.vit.deit3_medium_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit3_medium_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1587: UserWarning: Overwriting deit3_base_patch16_224 in registry with models.vit.deit3_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit3_base_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1597: UserWarning: Overwriting deit3_large_patch16_224 in registry with models.vit.deit3_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit3_large_patch16_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit.py:1607: UserWarning: Overwriting deit3_huge_patch14_224 in registry with models.vit.deit3_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def deit3_huge_patch14_224(pretrained=False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit_h2t.py:1246: UserWarning: Overwriting flexivit_small in registry with models.vit_h2t.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def flexivit_small(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit_h2t.py:1255: UserWarning: Overwriting flexivit_base in registry with models.vit_h2t.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def flexivit_base(pretrained: bool = False, **kwargs):\n",
      "/home/chengwei/auto-hls4ml-pytorch/TokenReductionPT/models/vit_h2t.py:1264: UserWarning: Overwriting flexivit_large in registry with models.vit_h2t.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def flexivit_large(pretrained: bool = False, **kwargs):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "from warnings import warn\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from textwrap import wrap\n",
    "from contextlib import suppress\n",
    "from statistics import mean, stdev\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import axes_grid1\n",
    "from einops import rearrange, reduce\n",
    "from timm.models import create_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "import models\n",
    "from datasets import build_dataset\n",
    "from train import get_args_parser, adjust_config, set_seed, set_run_name, count_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db60af07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_name': 'cub', 'dataset_root_path': '../../data/cub/CUB_200_2011', 'df_train': 'train.csv', 'df_trainval': 'train_val.csv', 'df_val': 'val.csv', 'df_test': 'test_100.csv', 'folder_train': 'images', 'folder_val': 'images', 'folder_test': 'images', 'df_classid_classname': 'classid_classname.csv'}\n",
      "{'pretrained': True}\n",
      "{'horizontal_flip': True}\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser('DeiT training and evaluation script', parents=[get_args_parser()])\n",
    "parser.add_argument('--compute_attention_average', action='store_true')\n",
    "parser.add_argument('--compute_attention_cka', action='store_true')\n",
    "parser.set_defaults(output_dir='results_inference')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "args.model = 'topk_deit_tiny_patch16_224.fb_in1k'\n",
    "args.cfg = 'configs/cub_test3.4p_ft_weakaugs.yaml'\n",
    "# args.cfg = 'configs/cotton_ft_weakaugs.yaml'\n",
    "args.device = 'cpu'\n",
    "args.keep_rate = [0.5]\n",
    "# args.reduction_loc = [3, 6, 9]\n",
    "args.train_trainval = True\n",
    "args.input_size = 224\n",
    "args.model_depth = 12\n",
    "# clca\n",
    "# args.ifa_head = True\n",
    "# args.clc = True\n",
    "# args.num_clr = 1\n",
    "adjust_config(args)\n",
    "# args.finetune = './results_tiny/{}_topk_deit_tiny_patch16_224.fb_in1k_61.pth'.format(args.dataset_name)\n",
    "args.finetune = './results_tiny/{}_topk_deit_tiny_patch16_224.fb_in1k_{}_61.pth'.format(args.dataset_name, args.keep_rate[0])\n",
    "# args.finetune = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea9739d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(256, 256), interpolation=bicubic, max_size=None, antialias=True)\n",
      "    RandomCrop(size=(224, 224), padding=None)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(256, 256), interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "Creating model: topk_deit_tiny_patch16_224.fb_in1k\n",
      "[] []\n",
      "TopK(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (pre_softmax): Identity()\n",
      "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "  (fc_norm): Identity()\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=192, out_features=200, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5706/4107818818.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.finetune, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "set_seed(args.seed)\n",
    "\n",
    "dataset_train, args.num_classes = build_dataset(is_train=True, args=args)\n",
    "dataset_val, _ = build_dataset(is_train=False, args=args)\n",
    "\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, sampler=sampler_train,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem,\n",
    "    drop_last=False,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset_val, sampler=sampler_val,\n",
    "    batch_size=args.batch_size,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_mem,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "if args.finetune and args.ifa_head and args.clc:\n",
    "    args.setting = 'ft_clca'\n",
    "elif args.finetune and args.ifa_head:\n",
    "    args.setting = 'ft_cla'\n",
    "elif args.finetune:\n",
    "    args.setting = 'ft_bl'\n",
    "else:\n",
    "    args.setting = 'fz_bl'\n",
    "\n",
    "print(f\"Creating model: {args.model}\")\n",
    "model = create_model(\n",
    "    args.model,\n",
    "    pretrained=True,\n",
    "    pretrained_cfg=None,\n",
    "    pretrained_cfg_overlay=None,\n",
    "    num_classes=1000,\n",
    "    drop_rate=args.drop,\n",
    "    drop_path_rate=args.drop_path,\n",
    "    drop_block_rate=None,\n",
    "    img_size=args.input_size,\n",
    "    args = args\n",
    ")\n",
    "if args.dataset_name.lower() != \"imagenet\":\n",
    "    model.reset_classifier(args.num_classes)\n",
    "if args.num_clr:\n",
    "    model.add_clr(args.num_clr)\n",
    "print(model)\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if args.finetune:\n",
    "    checkpoint = torch.load(args.finetune, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90163f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLCATransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    支援 TopK Token Reduction 和 CLC 的 TransformerEncoder\n",
    "    保持與 nn.TransformerEncoder 相容的介面\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_layer: nn.TransformerEncoderLayer,\n",
    "        num_layers: int,\n",
    "        norm: Optional[nn.Module] = None,\n",
    "        # CLCA 特定參數\n",
    "        keep_rate: Optional[Union[float, List[float]]] = None,\n",
    "        reduction_loc: Optional[List[int]] = None,\n",
    "        use_clc: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 基本 Transformer 參數\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        \n",
    "        # CLCA 參數處理\n",
    "        self.reduction_loc = reduction_loc or []\n",
    "        self.use_clc = use_clc\n",
    "        \n",
    "        # 處理 keep_rate 格式\n",
    "        if keep_rate is not None:\n",
    "            if isinstance(keep_rate, float):\n",
    "                # 單一值，應用到所有 reduction 位置\n",
    "                self.keep_rates = [keep_rate] * len(self.reduction_loc)\n",
    "            elif isinstance(keep_rate, list):\n",
    "                if len(keep_rate) == 1:\n",
    "                    # [0.5] -> 應用到所有 reduction 位置\n",
    "                    self.keep_rates = keep_rate * len(self.reduction_loc)\n",
    "                else:\n",
    "                    # [0.5, 0.5, 0.5] -> 直接使用\n",
    "                    assert len(keep_rate) == len(self.reduction_loc), \\\n",
    "                        f\"keep_rate 長度 {len(keep_rate)} 必須與 reduction_loc 長度 {len(self.reduction_loc)} 相同\"\n",
    "                    self.keep_rates = keep_rate\n",
    "        else:\n",
    "            self.keep_rates = []\n",
    "        \n",
    "        # 建立層列表（複製 encoder_layer）\n",
    "        self.layers = nn.ModuleList([\n",
    "            self._copy_encoder_layer(encoder_layer) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # 標記哪些層需要 TopK（注意：reduction_loc 是 1-indexed）\n",
    "        self.layer_configs = []\n",
    "        keep_rate_idx = 0\n",
    "        for i in range(num_layers):\n",
    "            layer_config = {\n",
    "                'index': i,\n",
    "                'use_topk': False,\n",
    "                'keep_rate': 1.0,\n",
    "                'use_clc': use_clc\n",
    "            }\n",
    "            \n",
    "            # 檢查是否是 reduction 層（轉換為 0-indexed）\n",
    "            if (i + 1) in self.reduction_loc:\n",
    "                layer_config['use_topk'] = True\n",
    "                layer_config['keep_rate'] = self.keep_rates[keep_rate_idx]\n",
    "                keep_rate_idx += 1\n",
    "            \n",
    "            self.layer_configs.append(layer_config)\n",
    "        \n",
    "        # 計算 CLC groups（如果啟用）\n",
    "        if use_clc:\n",
    "            self.clc_groups = self._compute_clc_groups()\n",
    "        else:\n",
    "            self.clc_groups = []\n",
    "    \n",
    "    def _copy_encoder_layer(self, layer: nn.TransformerEncoderLayer):\n",
    "        \"\"\"複製一個 encoder layer\"\"\"\n",
    "        # 這裡需要深度複製層的參數\n",
    "        import copy\n",
    "        return copy.deepcopy(layer)\n",
    "    \n",
    "    def _compute_clc_groups(self):\n",
    "        \"\"\"\n",
    "        根據 reduction_loc 計算 CLC groups\n",
    "        reduction_loc=[3, 6, 9] -> groups=[[0,1,2,3], [4,5,6], [7,8,9], [10,11]]\n",
    "        \"\"\"\n",
    "        groups = []\n",
    "        start = 0\n",
    "        \n",
    "        for end in self.reduction_loc:\n",
    "            # end 是 0-indexed，包含該層\n",
    "            groups.append(list(range(start, end + 1)))\n",
    "            start = end + 1\n",
    "        \n",
    "        # 最後一組（如果還有剩餘的層）\n",
    "        if start < self.num_layers:\n",
    "            groups.append(list(range(start, self.num_layers)))\n",
    "        \n",
    "        return groups\n",
    "    \n",
    "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        在 PyTorch 端保持標準行為，TopK 和 CLC 邏輯會在 HLS 轉換時處理\n",
    "        \"\"\"\n",
    "        output = src\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"返回 CLCA 配置，供 hls4ml 使用\"\"\"\n",
    "        return {\n",
    "            'num_layers': self.num_layers,\n",
    "            'reduction_loc': self.reduction_loc,\n",
    "            'keep_rates': self.keep_rates,\n",
    "            'use_clc': self.use_clc,\n",
    "            'clc_groups': self.clc_groups,\n",
    "            'layer_configs': self.layer_configs\n",
    "        }\n",
    "\n",
    "class Transformer4HLS_CLCA(nn.Module):\n",
    "    \"\"\"\n",
    "    包裝 CLCA TransformerEncoder 的完整模型\n",
    "    相容原本的 Transformer4HLS 介面\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        num_encoder_layers: int,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"relu\",\n",
    "        norm_first: bool = False,\n",
    "        device: Optional[str] = None,\n",
    "        # CLCA 特定參數\n",
    "        keep_rate: Optional[Union[float, List[float]]] = None,\n",
    "        reduction_loc: Optional[List[int]] = None,\n",
    "        use_clc: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 儲存所有參數\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.norm_first = norm_first\n",
    "        self.device = device\n",
    "        \n",
    "        # CLCA 參數\n",
    "        self.keep_rate = keep_rate\n",
    "        self.reduction_loc = reduction_loc\n",
    "        self.use_clc = use_clc\n",
    "        \n",
    "        # 初始化 transformer\n",
    "        self._init_transformer()\n",
    "    \n",
    "    def _init_transformer(self):\n",
    "        # 創建基本的 encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.nhead,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "            activation=self.activation,\n",
    "            norm_first=self.norm_first,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # 創建 norm layer\n",
    "        norm = nn.LayerNorm(self.d_model)\n",
    "        \n",
    "        # 根據是否使用 CLCA 功能決定使用哪種 encoder\n",
    "        if self.keep_rate is not None or self.use_clc:\n",
    "            # 使用 CLCA encoder\n",
    "            self.transformer_encoder = CLCATransformerEncoder(\n",
    "                encoder_layer=encoder_layer,\n",
    "                num_layers=self.num_encoder_layers,\n",
    "                norm=norm,\n",
    "                keep_rate=self.keep_rate,\n",
    "                reduction_loc=self.reduction_loc,\n",
    "                use_clc=self.use_clc\n",
    "            )\n",
    "            self.is_clca = True\n",
    "        else:\n",
    "            # 使用標準 encoder（向後相容）\n",
    "            self.transformer_encoder = nn.TransformerEncoder(\n",
    "                encoder_layer=encoder_layer,\n",
    "                num_layers=self.num_encoder_layers,\n",
    "                norm=norm\n",
    "            )\n",
    "            self.is_clca = False\n",
    "    \n",
    "    def forward(self, src):\n",
    "        return self.transformer_encoder(src)\n",
    "    \n",
    "    def get_clca_config(self):\n",
    "        \"\"\"獲取 CLCA 配置（如果有的話）\"\"\"\n",
    "        if self.is_clca:\n",
    "            return self.transformer_encoder.get_config()\n",
    "        return None\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# 1. 原本的使用方式（不啟用 CLCA）\n",
    "# model4hls = Transformer4HLS_CLCA(\n",
    "#     d_model=192,\n",
    "#     nhead=3,\n",
    "#     num_encoder_layers=12,\n",
    "#     dim_feedforward=768,\n",
    "#     dropout=0,\n",
    "#     activation='gelu',\n",
    "#     norm_first=True,\n",
    "#     device='cpu'\n",
    "#     # 沒有指定 keep_rate, reduction_loc, use_clc\n",
    "# )\n",
    "\n",
    "# 2. 啟用 TopK，使用單一 keep_rate\n",
    "# model4hls = Transformer4HLS_CLCA(\n",
    "#     d_model=192,\n",
    "#     nhead=3,\n",
    "#     num_encoder_layers=12,\n",
    "#     dim_feedforward=768,\n",
    "#     dropout=0,\n",
    "#     activation='gelu',\n",
    "#     norm_first=True,\n",
    "#     device='cpu',\n",
    "#     keep_rate=0.5,  # 單一值，應用到所有 reduction 位置\n",
    "#     reduction_loc=[3, 6, 9]  # 在第 4, 7, 10 層後 reduction\n",
    "# )\n",
    "\n",
    "# 4. 啟用 TopK + CLC\n",
    "model4hls = Transformer4HLS_CLCA(\n",
    "    d_model=192,\n",
    "    nhead=3,\n",
    "    num_encoder_layers=12,\n",
    "    dim_feedforward=768,\n",
    "    dropout=0,\n",
    "    activation='gelu',\n",
    "    norm_first=True,\n",
    "    device='cpu',\n",
    "    keep_rate=[0.5],  # 或 0.5\n",
    "    reduction_loc=[3, 6, 9],\n",
    "    use_clc=True  # 啟用 CLC\n",
    ")\n",
    "\n",
    "model4hls.eval()\n",
    "\n",
    "for i in range(args.model_depth):\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.in_proj_weight    = model.blocks[i].attn.qkv.weight\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.in_proj_bias      = model.blocks[i].attn.qkv.bias\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.out_proj.weight   = model.blocks[i].attn.proj.weight\n",
    "    model4hls.transformer_encoder.layers[i].self_attn.out_proj.bias     = model.blocks[i].attn.proj.bias\n",
    "    model4hls.transformer_encoder.layers[i].linear1.weight              = model.blocks[i].mlp.fc1.weight\n",
    "    model4hls.transformer_encoder.layers[i].linear1.bias                = model.blocks[i].mlp.fc1.bias\n",
    "    model4hls.transformer_encoder.layers[i].linear2.weight              = model.blocks[i].mlp.fc2.weight\n",
    "    model4hls.transformer_encoder.layers[i].linear2.bias                = model.blocks[i].mlp.fc2.bias\n",
    "    model4hls.transformer_encoder.layers[i].norm1.weight                = model.blocks[i].norm1.weight\n",
    "    model4hls.transformer_encoder.layers[i].norm1.bias                  = model.blocks[i].norm1.bias\n",
    "    model4hls.transformer_encoder.layers[i].norm2.weight                = model.blocks[i].norm2.weight\n",
    "    model4hls.transformer_encoder.layers[i].norm2.bias                  = model.blocks[i].norm2.bias\n",
    "model4hls.transformer_encoder.norm.weight   = model.norm.weight\n",
    "model4hls.transformer_encoder.norm.bias     = model.norm.bias\n",
    "\n",
    "# torch.save(model4hls, './model4hls_{}.pth'.format(args.input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41c9935e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src\n",
      "transformer_encoder_layers_0\n",
      "transformer_encoder_layers_1\n",
      "transformer_encoder_layers_2\n",
      "transformer_encoder_layers_3\n",
      "transformer_encoder_layers_4\n",
      "transformer_encoder_layers_5\n",
      "transformer_encoder_layers_6\n",
      "transformer_encoder_layers_7\n",
      "transformer_encoder_layers_8\n",
      "transformer_encoder_layers_9\n",
      "transformer_encoder_layers_10\n",
      "transformer_encoder_layers_11\n",
      "transformer_encoder_norm\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "from torch.fx import symbolic_trace\n",
    "\n",
    "traced_model = symbolic_trace(model4hls)\n",
    "\n",
    "for node in traced_model.graph.nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4a0d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape of encoders = torch.Size([1, 197, 192])\n",
      "torch.Size([1, 197, 192])\n",
      "torch.Size([1, 197, 192])\n",
      "tensor([[[ 0.6368, -0.9250, -2.5562,  ...,  0.0086, -1.8008, -0.8929],\n",
      "         [-0.7014,  1.5679,  1.9164,  ...,  2.0516,  1.2177,  0.7111],\n",
      "         [-1.0806,  1.3600,  1.1683,  ...,  2.0420,  0.7527,  0.5741],\n",
      "         ...,\n",
      "         [-0.0988,  1.1558,  3.5726,  ...,  3.5264,  0.6754, -0.0927],\n",
      "         [-0.7743,  0.0852,  2.0394,  ...,  1.2179,  3.1379,  0.9270],\n",
      "         [-0.2831,  0.5343,  2.6471,  ...,  3.0796,  1.0631,  0.4195]]])\n",
      "tensor([[[ 0.6368, -0.9249, -2.5562,  ...,  0.0086, -1.8008, -0.8929],\n",
      "         [-0.7015,  1.5679,  1.9164,  ...,  2.0516,  1.2178,  0.7111],\n",
      "         [-1.0807,  1.3600,  1.1682,  ...,  2.0419,  0.7527,  0.5741],\n",
      "         ...,\n",
      "         [-0.0989,  1.1559,  3.5725,  ...,  3.5264,  0.6754, -0.0927],\n",
      "         [-0.7743,  0.0852,  2.0395,  ...,  1.2180,  3.1378,  0.9270],\n",
      "         [-0.2833,  0.5343,  2.6471,  ...,  3.0796,  1.0632,  0.4194]]])\n",
      "Difference between pytorch model and model4hls = 0.00032685697078704834\n"
     ]
    }
   ],
   "source": [
    "for idx in range(1):\n",
    "    random_tensor = torch.randn(1, 3, args.input_size, args.input_size)\n",
    "    # 將 random_tensor 移動到與模型相同的設備\n",
    "    random_tensor = random_tensor.to(args.device)\n",
    "    model4hls.to(args.device)\n",
    "    # print(random_tensor)\n",
    "    with torch.no_grad():\n",
    "        x = model.patch_embed(random_tensor)\n",
    "        x = model._pos_embed(x)\n",
    "        x = model.patch_drop(x)\n",
    "        x = model.norm_pre(x)\n",
    "        print('Input shape of encoders = {}'.format(x.shape))\n",
    "        out = x\n",
    "        out2 = x\n",
    "        # out, left_token, sample_idx, compl = model.blocks[0](x)\n",
    "        # out2 = model4hls.transformer_encoder.layers[0](x.permute(1, 0, 2))\n",
    "        for i, blk in enumerate(model.blocks):\n",
    "            # print('Processing block {}'.format(i))\n",
    "            # out, left_token, sample_idx, compl = blk(out) # for evit\n",
    "            out, left_token, sample_idx = blk(out) # for topk \n",
    "        out = model.norm(out)\n",
    "        out2 = model4hls(out2.permute(1, 0, 2))\n",
    "        out2 = out2.permute(1, 0, 2)\n",
    "        print(out.shape)\n",
    "        print(out2.shape)\n",
    "        print(out)\n",
    "        print(out2)\n",
    "        difference = (out - out2).max()\n",
    "        \n",
    "        print('Difference between pytorch model and model4hls = {}'.format(difference))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_hls4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
